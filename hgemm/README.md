# HGEMM 

## HGEMM/SGEMM Supported Matrix

|CUDA Cores|Sliced K(Loop over K)|Tile Block|Tile Thread|
|:---:|:---:|:---:|:---:|
|âœ”ï¸|âœ”ï¸|âœ”ï¸|âœ”ï¸|
|**WMMA(m16n16k16)**|**MMA(m16n8k16)**|**Pack LDST(128 bits)**|**SMEM Padding**|
|âœ”ï¸|âœ”ï¸|âœ”ï¸|âœ”ï¸|
|**Copy Async**|**Tile MMA(More Threads)**|**Tile Warp(More Values)**|**Multi Stages**|  
|âœ”ï¸|âœ”ï¸|âœ”ï¸|âœ”ï¸|
|**Reg Double Buffers**|**Block Swizzle**|**Warp Swizzle**|**Collective Store(Reg Reuse&Warp Shfl)**|
|âœ”ï¸|âœ”ï¸|âœ”ï¸|âœ”ï¸|
|**Row Major(NN)**|**Col Major(TN)**|**SGEMM TF32**|**SMEM Swizzle/Permuted**|
|âœ”ï¸|âœ”ï¸|âœ”ï¸|â”|

<details>
<summary> ğŸ”‘ï¸ ç‚¹å‡»æŸ¥çœ‹æ‰€æœ‰æ”¯æŒçš„HGEMM Kernels! </summary>  
  
- [X] hgemm_sliced_k_f16_kernel 
- [X] hgemm_t_8x8_sliced_k_f16x4_kernel(unpack)
- [X] hgemm_t_8x8_sliced_k_f16x4_pack_kernel(pack 16x4)
- [X] hgemm_t_8x8_sliced_k_f16x4_bcf_kernel(bank conflicts reduce)
- [X] hgemm_t_8x8_sliced_k_f16x4_pack_bcf_kernel(bank conflicts reduce, pack)
- [X] hgemm_t_8x8_sliced_k_f16x8_pack_bcf_kernel(bank conflicts reduce, pack)
- [X] hgemm_t_8x8_sliced_k_f16x8_pack_bcf_dbuf_kernel(bank conflicts reduce, pack, double buffers)
- [X] hgemm_t_8x8_sliced_k16/32_f16x8_pack_bcf_dbuf_kernel(pack, double buffers)
- [X] hgemm_t_8x8_sliced_k16/32_f16x8_pack_bcf_dbuf_async_kernel(pack, double buffers, copy async)
- [X] hgemm_wmma_m16n16k16_naive(WMMA) 
- [X] hgemm_wmma_m16n16k16_mma4x2(WMMA, Tile MMA) 
- [X] hgemm_wmma_m16n16k16_mma4x2_warp2x4(TWMMA, Tile MMA/Warp, pack) 
- [X] hgemm_wmma_m16n16k16_mma4x2_warp2x4_async(WMMA, Tile MMA/Warp, Copy Async) 
- [X] hgemm_wmma_m16n16k16_mma4x2_warp2x4_async_offset(WMMA, Tile MMA/Warp, Copy Async, Pad)
- [X] hgemm_wmma_m16n16k16_mma4x2_warp2x4_dbuf_async(WMMA, Tile MMA/Warp, Copy Async, Double Buffers, Pad)  
- [X] hgemm_wmma_m16n16k16_mma4x2_warp2x4_stages(WMMA, Tile MMA/Warp, Copy Async, Stages, Pad, Block swizzle) 
- [X] hgemm_wmma_m16n16k16_mma4x2_warp4x4_stages(WMMA, Tile MMA/Warp, Copy Async, Stages, Pad, Block swizzle)
- [X] hgemm_wmma_m16n16k16_mma4x4_warp4x4_stages(WMMA, Tile MMA/Warp, Copy Async, Stages, Pad, Block swizzle) 
- [X] hgemm_wmma_m32n8k16_mma2x4_warp2x4_dbuf_async(WMMA, Tile MMA/Warp, Copy Async, Double Buffers, Pad)
- [X] hgemm_mma_m16n8k16_naive(MMA)
- [X] hgemm_mma_m16n8k16_mma2x4_warp4x4(MMA, Tile MMA/Warp, pack)
- [X] hgemm_mma_m16n8k16_mma2x4_warp4x4_stages(MMA, Tile MMA/Warp, Copy Async, Stages, Pad, Block swizzle)
- [X] hgemm_mma_m16n8k16_mma2x4_warp4x4x2_stages(MMA, Tile MMA/Warp, Copy Async, Stages, Pad, Block swizzle, Warp swizzle, Reg Double Buffers, Collective Store with Reg Reuse & Warp Shuffle) 
- [X] PyTorch bindings

</details>

## æµ‹è¯•å‘½ä»¤

```bash
# åªæµ‹è¯•Adaæ¶æ„ ä¸æŒ‡å®šé»˜è®¤ç¼–è¯‘æ‰€æœ‰æ¶æ„ è€—æ—¶è¾ƒé•¿: Volta, Ampere, Ada, Hopper, ...
export TORCH_CUDA_ARCH_LIST=Ada 
python3 hgemm.py --wmma # test defalut wmma kernels for all MNK
python3 hgemm.py --mma  # test defalut mma kernels for all MNK
python3 hgemm.py --M 16384 --N 16384 --K 8192 --i 10 --wmma # test default wmma kernels for specific MNK
python3 hgemm.py --M 16384 --N 16384 --K 8192 --i 10 --mma # test default mma kernels for specific MNK
python3 hgemm.py --wmma-all # test all wmma kernels for all MNK
python3 hgemm.py --mma-all # test all mma kernels for all MNK
python3 hgemm.py --cuda-all --wmma-all --mma-all # test all kernels for all MNK
```

## ç›®å‰æ€§èƒ½  

### NVIDIA L20  

ç›®å‰æœ€ä¼˜çš„å®ç°ï¼Œåœ¨L20ä¸Šï¼ˆç†è®ºTensor Cores FP16ç®—åŠ›ä¸º 119.5 TFLOPSï¼‰ï¼Œä½¿ç”¨WMMA APIèƒ½è¾¾åˆ°cuBLASå¤§æ¦‚95%~98%å·¦å³çš„æ€§èƒ½(105-113 TFLOPS vs 105-115 TFLOPS)ï¼Œä½¿ç”¨MMA APIèƒ½è¾¾åˆ°115 TFLOPSï¼Œéƒ¨åˆ†caseä¼šè¶…è¶ŠcuBLASã€‚å·²çŸ¥é—®é¢˜ä¸ºbank conflictsæ²¡æœ‰å®Œå…¨æ¶ˆé™¤ï¼Œç›®å‰é€šè¿‡paddingçš„æ–¹å¼ç¼“è§£bank conflictsä¼šå¯¼è‡´shared memoryæµªè´¹ï¼Œä¹Ÿä¼šå½±å“SM occupancyã€‚å¹¶ä¸”å°šæœªæ‰‹å·¥å®ç°smem swizzle/permute(å—é™äºWMMA APIçš„çµæ´»æ€§ä»¥åŠrow majorçš„layout)ï¼Œåç»­å°†ä¼šå°è¯•é€šè¿‡MMA PTXå®ç°smem swizzle/permuteã€‚

<div id="NV-L20"></div>

- WMMA: Up to 113.76 TFLOPS, 113.83/119.5=95.25% TFLOPS utilization, 113.83/116.25=97.91% cuBLAS performance.
- MMA: Up to 115.12 TFLOPS, 115.12/119.5=96.33% TFLOPS utilization, 115.12/116.25=99.03% cuBLAS performance.

```bash
python3 hgemm.py --M 16384 --N 16384 --K 8192 --mma-all --wmma-all --cuda-all
----------------------------------------------------------------------------------------------------------------------------------
                                        M=16384, N=16384, K=8192, Warmup=2, Iters=10, 1/1
----------------------------------------------------------------------------------------------------------------------------------
                                   (naive): ['-236.75   ', '176.0     '], time:1835.537ms, swizzle: NOOP, TFLOPS: 2.40  (+0.00%)
                      (f16x8pack+t8x8+bcf): ['-236.75   ', '176.0     '], time:99.63080ms, swizzle: NOOP, TFLOPS: 44.14 (+1742.34%)
                 (f16x8pack+t8x8+k16+dbuf): ['-236.75   ', '176.0     '], time:98.20067ms, swizzle: NOOP, TFLOPS: 44.79 (+1.46%)
--------------------------------------------------------------------WMMA----------------------------------------------------------
                         (wmma4x2+warp2x4): ['-234.0    ', '181.0     '], time:55.99505ms, swizzle: NOOP, TFLOPS: 78.54 (+75.37%)
                  (wmma4x2+warp2x4+stage3): ['-234.0    ', '181.0     '], time:49.62856ms, swizzle: NOOP, TFLOPS: 88.62 (+12.83%)
            (wmma4x2+warp2x4+stage3+dsmem): ['-234.0    ', '181.0     '], time:49.62389ms, swizzle: NOOP, TFLOPS: 88.63 (+0.01%)
          (wmma4x2+warp2x4+stage3+swizzle): ['-234.0    ', '181.0     '], time:39.11254ms, swizzle: 4096, TFLOPS: 112.45(+26.87%)
          (wmma4x2+warp2x4+stage2+swizzle): ['-234.0    ', '181.0     '], time:38.63754ms, swizzle: 4096, TFLOPS: 113.83(+1.23%)
--------------------------------------------------------------------MMA-----------------------------------------------------------
           (mma2x4+warp4x4+stage2+swizzle): ['-234.0    ', '181.0     '], time:38.40544ms, swizzle: 4096, TFLOPS: 114.52(+0.60%)
     (mma2x4+warp4x4+stage2+dsmem+swizzle): ['-234.0    ', '181.0     '], time:38.20540ms, swizzle: 4096, TFLOPS: 115.12(+0.52%)
                                  (cublas): ['-234.0    ', '181.0     '], time:37.83144ms, swizzle: NOOP, TFLOPS: 116.25(+0.99%)
----------------------------------------------------------------------------------------------------------------------------------
```
å…¨é‡MNKæµ‹è¯•å‘½ä»¤ï¼ˆæç¤º: æ¯ä¸ªMNKå•ç‹¬æµ‹è¯•çš„æ€§èƒ½æ•°æ®æ›´å‡†ç¡®ï¼‰
```bash
python3 hgemm.py --mma-all --wmma-all --cuda-all
```

### NVIDIA GeForce RTX 4090
åœ¨NVIDIA RTX 4090ä¸Š(FP16 Tensor Coresç®—åŠ›ä¸º330 TFLOPS)ï¼ŒWMMA(m16n16k16)æ€§èƒ½è¡¨ç°æ¯”MMA(m16n8k16)è¦æ›´å¥½ï¼Œå¤§åˆ†éƒ¨MNKä¸‹ï¼Œæœ¬ä»“åº“çš„å®ç°èƒ½è¾¾åˆ°cuBLAS 95%~99%çš„æ€§èƒ½ï¼ŒæŸäº›caseèƒ½è¶…è¿‡cuBLASã€‚å°±æœ¬ä»“åº“çš„å®ç°è€Œè¨€ï¼Œåœ¨RTX 4090ä¸Šï¼Œå¤§è§„æ¨¡çŸ©é˜µä¹˜(MNK>=8192)ï¼ŒWMMAè¡¨ç°æ›´ä¼˜ï¼Œå°è§„æ¨¡çŸ©é˜µä¹˜ï¼ŒMMAè¡¨ç°æ›´ä¼˜ã€‚
```bash
----------------------------------------------------------------------------------------------------------------------------------
                                        M=16384, N=16384, K=8192, Warmup=2, Iters=10, 1/1
----------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------WMMA----------------------------------------------------------
                                 (wmma4x2): ['-137.375  ', '53.65625  '], time:90.05668ms, swizzle: NOOP, TFLOPS: 48.84 (+0.00%)
                         (wmma4x2+warp2x4): ['-137.375  ', '53.65625  '], time:37.53635ms, swizzle: NOOP, TFLOPS: 117.17(+139.92%)
                  (wmma4x2+warp2x4+stage3): ['-137.375  ', '53.65625  '], time:25.96564ms, swizzle: NOOP, TFLOPS: 169.38(+44.56%)
                  (wmma4x2+warp2x4+stage2): ['-137.375  ', '53.65625  '], time:25.21226ms, swizzle: NOOP, TFLOPS: 174.44(+2.99%)
          (wmma4x2+warp2x4+stage3+swizzle): ['-137.375  ', '53.65625  '], time:22.99013ms, swizzle: 4096, TFLOPS: 191.30(+9.67%)
          (wmma4x2+warp2x4+stage2+swizzle): ['-137.375  ', '53.65625  '], time:22.91676ms, swizzle: 4096, TFLOPS: 191.91(+0.32%)
    (wmma4x2+warp2x4+stage2+dsmem+swizzle): ['-137.375  ', '53.65625  '], time:22.78118ms, swizzle: 4096, TFLOPS: 193.06(+0.60%)
            (wmma4x4+warp4x4+stage3+dsmem): ['-137.375  ', '53.65625  '], time:18.66145ms, swizzle: NOOP, TFLOPS: 235.68(+22.08%)
    (wmma4x4+warp4x4+stage3+dsmem+swizzle): ['-137.375  ', '53.65625  '], time:18.16847ms, swizzle: 4096, TFLOPS: 242.07(+2.71%)
    (wmma4x4+warp4x4+stage2+dsmem+swizzle): ['-137.375  ', '53.65625  '], time:18.11864ms, swizzle: 4096, TFLOPS: 242.74(+0.28%)
                                  (cublas): ['-137.375  ', '53.65625  '], time:18.07777ms, swizzle: NOOP, TFLOPS: 243.28(+0.23%)
----------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------
                                        M=8192, N=8192, K=8192, Warmup=2, Iters=10, 1/1
----------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------WMMA----------------------------------------------------------
                                 (wmma4x2): ['11.453125 ', '-1.0664062'], time:18.48518ms, swizzle: NOOP, TFLOPS: 59.48 (+0.00%)
                         (wmma4x2+warp2x4): ['11.453125 ', '-1.0664062'], time:9.354352ms, swizzle: NOOP, TFLOPS: 117.54(+97.61%)
                  (wmma4x2+warp2x4+stage3): ['11.453125 ', '-1.0664062'], time:5.835342ms, swizzle: NOOP, TFLOPS: 188.42(+60.31%)
                  (wmma4x2+warp2x4+stage2): ['11.453125 ', '-1.0664062'], time:5.795311ms, swizzle: NOOP, TFLOPS: 189.72(+0.69%)
            (wmma4x2+warp2x4+stage3+dsmem): ['11.453125 ', '-1.0664062'], time:5.795168ms, swizzle: NOOP, TFLOPS: 189.73(+0.00%)
          (wmma4x2+warp2x4+stage3+swizzle): ['11.453125 ', '-1.0664062'], time:5.384325ms, swizzle: 2048, TFLOPS: 204.21(+7.63%)
            (wmma4x4+warp4x4+stage3+dsmem): ['11.453125 ', '-1.0664062'], time:4.254937ms, swizzle: NOOP, TFLOPS: 258.41(+26.54%)
                                  (cublas): ['11.421875 ', '-1.3203125'], time:4.288864ms, swizzle: NOOP, TFLOPS: 256.36
----------------------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------------------
                                        M=4096, N=4096, K=4096, Warmup=2, Iters=10, 1/1
----------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------WMMA----------------------------------------------------------
                                 (wmma4x2): ['-9.0      ', '-144.875  '], time:2.341437ms, swizzle: NOOP, TFLOPS: 58.70 (+0.00%)
                         (wmma4x2+warp2x4): ['-9.0      ', '-144.875  '], time:1.237440ms, swizzle: NOOP, TFLOPS: 111.07(+89.22%)
                  (wmma4x2+warp2x4+stage3): ['-9.0      ', '-144.875  '], time:0.725293ms, swizzle: NOOP, TFLOPS: 189.49(+70.61%)
            (wmma4x2+warp2x4+stage3+dsmem): ['-9.0      ', '-144.875  '], time:0.723266ms, swizzle: NOOP, TFLOPS: 190.03(+0.28%)
          (wmma4x2+warp2x4+stage3+swizzle): ['-9.0      ', '-144.875  '], time:0.702548ms, swizzle: 2048, TFLOPS: 195.63(+2.95%)
    (wmma4x2+warp2x4+stage3+dsmem+swizzle): ['-9.0      ', '-144.875  '], time:0.702190ms, swizzle: 2048, TFLOPS: 195.73(+0.05%)
            (wmma4x4+warp4x4+stage3+dsmem): ['-9.0      ', '-144.875  '], time:0.556564ms, swizzle: NOOP, TFLOPS: 246.94(+26.17%)
                                  (cublas): ['-9.0      ', '-144.875  '], time:0.539851ms, swizzle: NOOP, TFLOPS: 254.59(+3.10%)
----------------------------------------------------------------------------------------------------------------------------------
```

### NVIDIA GeForce RTX 3080 Laptop   

åœ¨NVIDIA GeForce RTX 3080 Laptopä¸Šæµ‹è¯•ï¼Œä½¿ç”¨mma4x4_warp4x4ï¼ˆ16 WMMA m16n16k16 ops, warp tile 64x64ï¼‰ä»¥åŠThread block swizzleï¼Œå¤§éƒ¨åˆ†caseèƒ½æŒå¹³ç”šè‡³è¶…è¿‡cuBLASï¼Œä¸è¿‡Laptopæµ‹è¯•çš„æ€§èƒ½æ•°æ®ä¸ç¨³å®šï¼Œè¿™éƒ¨åˆ†çœ‹çœ‹å°±å¥½ï¼Œåˆ«å¤ªå½“çœŸã€‚

```bash
python3 hgemm.py --wmma-all
----------------------------------------------------------------------------------------------------------------------------------
                              M=16384, N=16384, K=8192, Warmup=5, Iters=20, 27/27
----------------------------------------------------------------------------------------------------------------------------------
           (wmma4x4+warp4x4+stage3+dsmem): ['68.375    ', '-2.234375 '], time:96.91984ms, swizzle: NOOP, TFLOPS: 45.38 (+0.00%)
           (wmma4x4+warp4x4+stage2+dsmem): ['68.375    ', '-2.234375 '], time:102.8722ms, swizzle: NOOP, TFLOPS: 42.75
   (wmma4x4+warp4x4+stage3+dsmem+swizzle): ['68.375    ', '-2.234375 '], time:85.65800ms, swizzle: 4096, TFLOPS: 51.34 (+13.15%)
   (wmma4x4+warp4x4+stage2+dsmem+swizzle): ['68.375    ', '-2.234375 '], time:95.70884ms, swizzle: 4096, TFLOPS: 45.95
                                 (cublas): ['68.375    ', '-2.234375 '], time:104.2092ms, swizzle: NOOP, TFLOPS: 42.20
----------------------------------------------------------------------------------------------------------------------------------
```

## æ€§èƒ½ä¼˜åŒ–ç¬”è®°

### PyTorch HGEMM Profile

åœ¨Adaæ¶æ„ä¸‹ï¼ŒPyTorch 2.4å¯¹FP16ä½¿ç”¨matmulæ—¶ï¼Œä¼šè°ƒç”¨:
```C++
ampere_fp16_s1688gemm_fp16_128x128_ldg8_f2f_stages_32x1_nn_kernel
```
å†…éƒ¨å®é™…ä½¿ç”¨HMMA(Tensor Cores)è¿›è¡Œè®¡ç®—ï¼Œåœ¨3080ä¸Šprofileå‘ç°ä½¿ç”¨:
```C++
sm80_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize96x64x32_stage3_warpsize2x2x1_tensor16x8x16_kernel
```
å› æ­¤ï¼Œåªæœ‰å®ç°ä½¿ç”¨Tensor Coresçš„HGEMMï¼Œæ‰æœ‰å¯èƒ½æ¥è¿‘PyTorch/cuBLASçš„æ€§èƒ½ã€‚
```bash
ncu -o hgemm.prof -f python3 prof.py
nsys profile --stats=true -t cuda,osrt,nvtx -o hgemm.prof --force-overwrite true python3 prof.py
```
- SASS (L20)

```C
// ampere_fp16_s1688gemm_fp16_128x128_ldg8_f2f_stages_32x1_nn_kernel
310	00007f41 37d5b850	      LDSM.16.M88.4 R192, [R169+UR8+0x2000] 
311	00007f41 37d5b860	      LDSM.16.M88.4 R196, [R169+UR8+0x2800]
336	00007f41 37d5b9f0	      HMMA.1688.F32 R112, R182, R196, R112
...
```

### SMEM Padding  

#### Bank Conflictsçš„äº§ç”Ÿ
  
å«ä¹‰ï¼šåœ¨è®¿é—®shared memoryæ—¶ï¼Œå› å¤šä¸ªçº¿ç¨‹è¯»å†™åŒä¸€ä¸ªBankä¸­çš„ä¸åŒæ•°æ®åœ°å€æ—¶ï¼Œå¯¼è‡´shared memory å¹¶å‘è¯»å†™ é€€åŒ– æˆé¡ºåºè¯»å†™çš„ç°è±¡å«åšBank Conflictï¼›

![](https://github.com/PaddleJitLab/CUDATutorial/blob/develop/docs/09_optimize_reduce/02_bank_conflict/images/ef322be7c3e5b6b9be69d2b90e88083f50569a58a97129f348e483b946ab4edf.png)

SMè°ƒåº¦å•ä½ä¸ºä¸€ä¸ªwarpï¼ˆä¸€ä¸ªwarpå†…32ä¸ªThreadï¼‰ï¼Œshared_memory å¯ä»¥ è¢«ä¸€ä¸ªwarpä¸­çš„æ‰€æœ‰ï¼ˆ32ä¸ªï¼‰çº¿ç¨‹è¿›è¡Œè®¿é—®ï¼Œshared_memory æ˜ å°„åˆ°å¤§å°ç›¸ç­‰çš„32ä¸ªBankä¸Šï¼ŒBankçš„æ•°æ®è¯»å–å¸¦å®½ä¸º32bit / cycle (4 bytes)ï¼Œå› æ­¤ï¼Œä¸»è¦éœ€è¦è€ƒè™‘ä¸€ä¸ªWarpå†…32çº¿ç¨‹çš„è®¿é—®å…±äº«å†…å­˜æ—¶çš„bankå†²çªã€‚
å¯¹äºå¤šä¸ªçº¿ç¨‹è¯»å–åŒä¸€ä¸ªBankæ•°æ®æ—¶ï¼ˆä¸åŒåœ°å€ï¼‰ï¼Œç¡¬ä»¶æŠŠå†…å­˜è¯»å†™è¯·æ±‚ï¼Œæ‹†åˆ†æˆ conflict-free requestsï¼Œè¿›è¡Œé¡ºåºè¯»å†™ï¼Œæ­¤æ—¶å°†ä¼šè§¦å‘å¤šæ¬¡å†…å­˜äº‹åŠ¡ã€‚ç‰¹åˆ«åœ°ï¼Œå½“ä¸€ä¸ªwarpä¸­çš„æ‰€æœ‰çº¿ç¨‹è¯»å†™åŒä¸€ä¸ªåœ°å€æ—¶ï¼Œä¼šè§¦å‘broadcastæœºåˆ¶ï¼Œæ­¤æ—¶ä¸ä¼šé€€åŒ–æˆé¡ºåºè¯»å†™ã€‚ä¸Šé¢æåˆ°è§¦å‘broadcastæœºåˆ¶çš„æ¡ä»¶æ˜¯all threads acess same addressï¼Œä½†åœ¨ç¿»é˜…cuda-c-programming-guideä»¥åŠæœ€æ–°ç‰ˆæœ¬çš„[NVProfGuide](https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html) æ—¶ï¼Œå‘ç°åªè¦æ˜¯å¤šä¸ªthread è¯»å†™å°±ä¼šè§¦å‘broadcastï¼ˆä¸éœ€è¦Allï¼‰ã€‚
  
- å¤šä¸ªçº¿ç¨‹è¯»åŒä¸€ä¸ªæ•°æ®æ—¶ï¼Œä»…æœ‰ä¸€ä¸ªçº¿ç¨‹è¯»ï¼Œç„¶åbroadcaståˆ°å…¶ä»–çº¿ç¨‹
- å¤šä¸ªçº¿ç¨‹å†™åŒä¸€ä¸ªæ•°æ®æ—¶ï¼Œä»…ä¼šæœ‰ä¸€ä¸ªçº¿ç¨‹å†™æˆåŠŸ

NVIDIAçš„[æ–‡ç« ](https://developer.nvidia.com/blog/using-shared-memory-cuda-cc/)ä¸­æŒ‡å‡ºï¼Œæˆ‘ä»¬è¿˜å¯ä»¥é€šè¿‡ `cudaDeviceSetSharedMemConfig()` å‡½æ•°è®¾ç½®é»˜è®¤Bank Sizeï¼ˆé»˜è®¤ä¸º4 bytesï¼‰æ¥é¿å…bank conflictsï¼Œå¯è®¾ç½®ä¸ºcudaSharedMemBankSizeFourByteæˆ–è€…cudaSharedMemBankSizeEightByteã€‚å¯¹äºæŸäº›åœºæ™¯æ¥è¯´ï¼Œè®¾ç½®cudaSharedMemBankSizeEightByteæˆ–è®¸æ›´åŠ åˆé€‚ï¼Œæ¯”å¦‚ä½¿ç”¨doubleæ•°æ®ç±»å‹æ—¶ã€‚ 

```C
cudaDeviceSetSharedMemConfig(cudaSharedMemBankSizeEightByte);
```
æœ¬é¡¹ç›®ç›®å‰é€šè¿‡paddingçš„æ–¹å¼ç¼“è§£bank conflictsä¼šå¯¼è‡´shared memoryæµªè´¹ï¼Œä¹Ÿä¼šå½±å“SM occupancyã€‚å¹¶ä¸”å°šæœªæ‰‹å·¥å®ç°smem swizzle/permute(å—é™äºWMMA APIçš„çµæ´»æ€§ä»¥åŠrow majorçš„layout)ï¼Œåç»­å°†ä¼šå°è¯•é€šè¿‡MMA PTXå®ç°smem swizzle/permuteã€‚

### åŒç¼“å†² Double Buffers

æœ¬ä»“åº“å®ç°çš„HGEMM Double Buffersç­–ç•¥å¦‚ä¸‹ï¼š1ï¼‰ä¸»å¾ªç¯ä»bk = 1 å¼€å§‹ï¼Œç¬¬ä¸€æ¬¡æ•°æ®åŠ è½½åœ¨ä¸»å¾ªç¯ä¹‹å‰ï¼Œæœ€åä¸€æ¬¡è®¡ç®—åœ¨ä¸»å¾ªç¯ä¹‹åï¼Œè¿™æ˜¯pipeline çš„ç‰¹ç‚¹å†³å®šçš„ï¼›2ï¼‰ç”±äºè®¡ç®—å’Œä¸‹ä¸€æ¬¡è®¿å­˜ä½¿ç”¨çš„Shared Memoryä¸åŒï¼Œå› æ­¤ä¸»å¾ªç¯ä¸­æ¯æ¬¡å¾ªç¯åªéœ€è¦ä¸€æ¬¡__syncthreads()å³å¯ï¼Œå¯¹æ¯”édouble buffersç‰ˆæœ¬ï¼Œæ€»å…±èŠ‚çœäº† ((K + BK - 1) / BK) - 1 æ¬¡blockå†…çš„åŒæ­¥æ“ä½œã€‚æ¯”å¦‚ï¼Œbk=1æ—¶ï¼ŒHFMAè®¡ç®—ä½¿ç”¨çš„æ˜¯s_a[0]å’Œs_b[0]ï¼Œå› æ­¤ï¼Œå’Œs_a[1]å’Œs_b[1]çš„åŠ è½½æ˜¯æ²¡æœ‰ä¾èµ–å…³ç³»çš„ã€‚HFMAè®¡ç®—ï¼Œä»globalå†…å­˜åˆ°s_a[1]å’Œs_b[1]å’ŒHFMAè®¡ç®—å¯ä»¥å¹¶è¡Œã€‚s_a[1]å’Œs_b[1]ç”¨äºåŠ è½½ä¸‹ä¸€å—BKéœ€è¦çš„æ•°æ®åˆ°å…±äº«å†…å­˜ï¼›3ï¼‰ç”±äºGPUä¸èƒ½å‘CPUé‚£æ ·æ”¯æŒä¹±åºæ‰§è¡Œï¼Œä¸»å¾ªç¯ä¸­éœ€è¦å…ˆå°†ä¸‹ä¸€æ¬¡å¾ªç¯è®¡ç®—éœ€è¦çš„Gloabal Memoryä¸­çš„æ•°æ®load åˆ°å¯„å­˜å™¨ï¼Œç„¶åè¿›è¡Œæœ¬æ¬¡è®¡ç®—ï¼Œä¹‹åå†å°†loadåˆ°å¯„å­˜å™¨ä¸­çš„æ•°æ®å†™åˆ°Shared Memoryï¼Œè¿™æ ·åœ¨LDGæŒ‡ä»¤å‘Global Memoryåšloadæ—¶ï¼Œä¸ä¼šå½±å“åç»­HFMAåŠå…¶å®ƒè¿ç®—æŒ‡ä»¤çš„ launch æ‰§è¡Œï¼Œä¹Ÿå°±è¾¾åˆ°äº†Double Buffersçš„ç›®çš„ï¼Œå…·ä½“ä»£ç è§[hgemm.cu](./hgemm.cu)ã€‚

<details>
<summary> ğŸ”‘ï¸ æ›´å¤šæ€§èƒ½ä¼˜åŒ–ç¬”è®°(TODO) ï¼Click here! </summary>    

### Tile Block

TODO

### Tile Thread

TODO

### Pack LDST 128 bits

TODO

### Async Copy

TODO

### Multi Stages

TODO

### Tensor Cores(WMMA/MMA)

TODO

### Tile MMA/Warp

TODO 

### Thread Block Swizze 

TODO

### Warp Swizzle

TODO

### Reg Double Buffers

TODO

### Collective Store(Reg Reuse&Warp Shuffle)

TODO

### SMEM Swizzle/Permuted

TODO

</details>

## å‚è€ƒæ–‡çŒ® 

- [CUDAç¼–ç¨‹æ¦‚å¿µã€‘ä¸€ã€ä»€ä¹ˆæ˜¯bank conflictï¼Ÿ](https://zhuanlan.zhihu.com/p/659142274)
- [è§£å†³ bank conflict](https://github.com/PaddleJitLab/CUDATutorial/blob/develop/docs/09_optimize_reduce/02_bank_conflict/README.md)
- [Bank Conflict free çš„å‡ ç§æ–¹å¼](https://zhuanlan.zhihu.com/p/722286440)
- [Using Shared Memory in CUDA C/C++](https://developer.nvidia.com/blog/using-shared-memory-cuda-cc/)
- [CUDAï¼ˆä¸‰ï¼‰ï¼šé€šç”¨çŸ©é˜µä¹˜æ³•ï¼šä»å…¥é—¨åˆ°ç†Ÿç»ƒ](https://zhuanlan.zhihu.com/p/657632577)

