
## âš¡ï¸âš¡ï¸Toy-HGEMM: Achieve the 98%~100% TFLOPS of cuBLAS ğŸ‰ğŸ‰

![toy-hgemm-library](https://github.com/user-attachments/assets/962bda14-b494-4423-b8eb-775da9f5503d)

[ğŸ“–Toy-HGEMM Libraryâš¡ï¸âš¡ï¸](./kernels/hgemm) is a library that write many HGEMM kernels from scratch using Tensor Cores with WMMA, MMA PTX and CuTe API, thus, can achieve `98%~100%` performance of **cuBLAS**. The codes here are source from ğŸ“–[CUDA-Learn-Notes](https://github.com/DefTruth/CUDA-Learn-Notes)  ![](https://img.shields.io/github/stars/DefTruth/CUDA-Learn-Notes.svg?style=social) and exported as a standalone library, please checkout [CUDA-Learn-Notes](https://github.com/DefTruth/CUDA-Learn-Notes) for latest updates. Welcome to ğŸŒŸğŸ‘†ğŸ»star this repo to support me, many thanks ~ ğŸ‰ğŸ‰

<div id="hgemm-sgemm"></div>  

<div align='center'>
  <img src='https://github.com/user-attachments/assets/71927ac9-72b3-4ce9-b0e2-788b5885bc99' height="170px" width="270px">
  <img src='https://github.com/user-attachments/assets/05ef4f5e-d999-48ea-b58e-782cffb24e85' height="170px" width="270px">
  <img src='https://github.com/user-attachments/assets/9472e970-c083-4b31-9252-3eeecc761078' height="170px" width="270px">
</div> 


Currently, on NVIDIA L20, RTX 4090 and RTX 3080 Laptop, compared with cuBLAS's default Tensor Cores math algorithm `CUBLAS_GEMM_DEFAULT_TENSOR_OP`, the `HGEMM (WMMA/MMA/CuTe)` implemented in this repo (`blue`ğŸ”µ) can achieve `98%~100%` of its (`orange`ğŸŸ ) performance. Please check [toy-hgemm libraryâš¡ï¸âš¡ï¸](./kernels/hgemm) for more details.

|ğŸ“šFeature |ğŸ“šFeature |ğŸ“šFeature |ğŸ“šFeature|
|:---:|:---:|:---:|:---:|
|âœ”ï¸CUDA/**Tensor Cores**|âœ”ï¸Loop over K|âœ”ï¸Tile Block(BMxBK)|âœ”ï¸Tile Threads(T 8x8)|
|âœ”ï¸WMMA(m16n16k16)|âœ”ï¸MMA(m16n8k16)|âœ”ï¸Pack LDST(128 bits)|âœ”ï¸SMEM Padding|
|âœ”ï¸Copy Async|âœ”ï¸Tile MMAs|âœ”ï¸Tile Warps|âœ”ï¸**Multi Stages(2~4)**|  
|âœ”ï¸Register Double Buffers|âœ”ï¸**Block Swizzle**|âœ”ï¸**Warp Swizzle**|âœ”ï¸**SMEM Swizzle**(CuTe/MMA)|
|âœ”ï¸Collective Store(Shfl)|âœ”ï¸Layout NN|âœ”ï¸Layout TN|âœ”ï¸SGEMM FP32/TF32|

## Â©ï¸CitationsğŸ‰ğŸ‰

```BibTeX
@misc{hgemm-tensorcores-mma@2024,
  title={hgemm-tensorcores-mma: Write HGEMM from scratch using Tensor Cores with WMMA, MMA PTX and CuTe API.},
  url={https://github.com/DefTruth/hgemm-tensorcores-mma},
  note={Open-source software available at https://github.com/DefTruth/hgemm-tensorcores-mma},
  author={DefTruth etc},
  year={2024}
}
```

## ğŸ“– HGEMM CUDA Kernels in Toy-HGEMM Library ğŸ‰ğŸ‰ 

<div id="kernels"></div>  

```C++  
void hgemm_naive_f16(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_sliced_k_f16(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_t_8x8_sliced_k_f16x4(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_t_8x8_sliced_k_f16x4_pack(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_t_8x8_sliced_k_f16x4_bcf(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_t_8x8_sliced_k_f16x4_pack_bcf(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_t_8x8_sliced_k_f16x8_pack_bcf(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_t_8x8_sliced_k_f16x8_pack_bcf_dbuf(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_t_8x8_sliced_k16_f16x8_pack_dbuf(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_t_8x8_sliced_k16_f16x8_pack_dbuf_async(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_t_8x8_sliced_k32_f16x8_pack_dbuf(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_t_8x8_sliced_k32_f16x8_pack_dbuf_async(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_t_16x8_sliced_k32_f16x8_pack_dbuf(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_t_16x8_sliced_k32_f16x8_pack_dbuf_async(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_cublas_tensor_op_nn(torch::Tensor a, torch::Tensor b, torch::Tensor c); 
void hgemm_cublas_tensor_op_tn(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_wmma_m16n16k16_naive(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_wmma_m16n16k16_mma4x2(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_wmma_m16n16k16_mma4x2_warp2x4(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_wmma_m16n16k16_mma4x2_warp2x4_dbuf_async(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_wmma_m32n8k16_mma2x4_warp2x4_dbuf_async(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_wmma_m16n16k16_mma4x2_warp2x4_stages(torch::Tensor a, torch::Tensor b, torch::Tensor c, int stages, bool swizzle, int swizzle_stride);
void hgemm_wmma_m16n16k16_mma4x2_warp2x4_stages_dsmem(torch::Tensor a, torch::Tensor b, torch::Tensor c, int stages, bool swizzle, int swizzle_stride);
void hgemm_wmma_m16n16k16_mma4x2_warp4x4_stages_dsmem(torch::Tensor a, torch::Tensor b, torch::Tensor c, int stages, bool swizzle, int swizzle_stride);                                                        
void hgemm_wmma_m16n16k16_mma4x4_warp4x4_stages_dsmem(torch::Tensor a, torch::Tensor b, torch::Tensor c, int stages, bool swizzle, int swizzle_stride);
void hgemm_mma_m16n8k16_naive(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_mma_m16n8k16_mma2x4_warp4x4(torch::Tensor a, torch::Tensor b, torch::Tensor c);
void hgemm_mma_m16n8k16_mma2x4_warp4x4_stages(torch::Tensor a, torch::Tensor b, torch::Tensor c, int stages, bool swizzle, int swizzle_stride);
void hgemm_mma_m16n8k16_mma2x4_warp4x4_stages_dsmem(torch::Tensor a, torch::Tensor b, torch::Tensor c, int stages, bool swizzle, int swizzle_stride);
void hgemm_mma_m16n8k16_mma2x4_warp4x4x2_stages_dsmem(torch::Tensor a, torch::Tensor b, torch::Tensor c, int stages, bool swizzle, int swizzle_stride);
void hgemm_mma_m16n8k16_mma2x4_warp4x4x2_stages_dsmem_x4(torch::Tensor a, torch::Tensor b, torch::Tensor c, int stages, bool swizzle, int swizzle_stride);
void hgemm_mma_m16n8k16_mma2x4_warp4x4x2_stages_dsmem_rr(torch::Tensor a, torch::Tensor b, torch::Tensor c, int stages, bool swizzle, int swizzle_stride);
void hgemm_mma_m16n8k16_mma2x4_warp4x4_stages_dsmem_tn(torch::Tensor a, torch::Tensor b, torch::Tensor c, int stages, bool swizzle, int swizzle_stride);
void hgemm_mma_stages_block_swizzle_tn_cute(torch::Tensor a, torch::Tensor b, torch::Tensor c, int stages, bool swizzle, int swizzle_stride);
void hgemm_mma_m16n8k16_mma2x4_warp4x4x2_stages_dsmem_swizzle(torch::Tensor a, torch::Tensor b, torch::Tensor c, int stages, bool swizzle, int swizzle_stride);
void hgemm_mma_m16n8k16_mma2x4_warp4x4x2_stages_dsmem_tn_swizzle_x4(torch::Tensor a, torch::Tensor b, torch::Tensor c, int stages, bool swizzle, int swizzle_stride);
```

## ğŸ“– Contents

- [ğŸ“– Prerequisites](#prerequisites)
- [ğŸ“– Installation](#install)
- [ğŸ“– Python Testing](#test)
- [ğŸ“– C++ Testing](#test-cpp)
- [ğŸ“– NVIDIA L20 bench](#perf-l20)
- [ğŸ“– NVIDIA RTX 4090 bench](#perf-4090)
- [ğŸ“– NVIDIA RTX 3080 Laptop bench](#perf-3080)
- [ğŸ“– Docs](#opt-docs)
- [ğŸ“– References](#ref)

## ğŸ“– Prerequisites
<div id="prerequisites"></div>  

- PyTorch >= 2.0, CUDA >= 12.0
- Recommended: PyTorch 2.5.1, CUDA 12.5

## ğŸ“– Installation  

<div id="install"></div>  

The HGEMM implemented in this repo can be install as a python library, namely, `toy-hgemm` library (optional). 
```bash
cd kernels/hgemm
git submodule update --init --recursive --force # Fetch `CUTLASS` submoduleï¼Œ needed
python3 setup.py bdist_wheel && cd dist && python3 -m pip install *.whl # pip uninstall toy-hgemm -y 
```

## ğŸ“– Python Testing

<div id="test"></div>  

**CUTLASS**: Fetch `CUTLASS` submodule. Currently, I use `v3.5.1` for HGEMM CuTe kernel.
```bash
git submodule update --init --recursive --force
```

You can test many custom HGEMM kernel via Python script and figure out the difference in their performance.

```bash
# You can test Ada or Ampere only, also, Volta, Ampere, Ada, Hopper, ...
export TORCH_CUDA_ARCH_LIST=Ada # for Ada only
export TORCH_CUDA_ARCH_LIST=Ampere # for Ampere only
python3 hgemm.py --wmma # test defalut wmma kernels for all MNK
python3 hgemm.py --mma  # test defalut mma kernels for all MNK
python3 hgemm.py --M 16384 --N 16384 --K 8192 --i 10 --wmma # test default wmma kernels for specific MNK
python3 hgemm.py --M 16384 --N 16384 --K 8192 --i 10 --mma # test default mma kernels for specific MNK
python3 hgemm.py --wmma-all # test all wmma kernels for all MNK
python3 hgemm.py --mma-all # test all mma kernels for all MNK
python3 hgemm.py --cuda-all --wmma-all --mma-all # test all kernels for all MNK
python3 hgemm.py --cute-tn --no-default # test cute hgemm kernels with smem swizzle for all MNK
```
If you want to draw a TFLOPS curve, you need to install `matplotlib` first and set the --plot-flops (or --plot) option.
```bash
python3 -m pip install matplotlib
# Specify topk to plot only the top k kernels with the best performance.
python3 hgemm.py --mma-all --plot --topk 8
# test default mma kernels & cute hgemm kernels with smem swizzle for all MNK
python3 hgemm.py --cute-tn --mma --plot 
```

## ğŸ“– C++ Testing

<div id="test-cpp"></div>  

The HGEMM benchmark also supports C++ testing. Currently, it supports comparisons between the following implementations:

- MMA HGEMM NN implemented in this repository
- CuTe HGEMM TN implemented in this repository
- cuBLAS HGEMM TN use default Tensor Cores math algorithm

Performance data obtained from C++ binary tests tend to be slightly better than those from Python tests. This difference may be attributed to additional overhead introduced by the PyTorch Python bindings.
```bash
make
./hgemm_mma_stage.bin
# NVIDIA L20
ALGO = MMA16816 HGEMM NN MMA=2x4 WARP=4x4x2 STAGES=2 BLOCK SWIZZLE=2048
M N K =  12544  12544  12544, Time =   0.03445555   0.03446098   0.03447399 s, AVG Performance =   114.5541 Tflops
M N K =  15360  15360  15360, Time =   0.06307226   0.06307789   0.06308864 s, AVG Performance =   114.9017 Tflops
M N K =  15616  15616  15616, Time =   0.06612480   0.06612798   0.06613094 s, AVG Performance =   115.1739 Tflops
M N K =  15872  15872  15872, Time =   0.06969549   0.06970215   0.06971290 s, AVG Performance =   114.7305 Tflops
M N K =  16128  16128  16128, Time =   0.07295078   0.07295406   0.07295693 s, AVG Performance =   115.0064 Tflops
M N K =  16384  16384  16384, Time =   0.07663001   0.07663534   0.07664947 s, AVG Performance =   114.7785 Tflops

./hgemm_cute.bin
# NVIDIA L20
ALGO = CuTe HGEMM, TN, STAGES=2, SMEM SWIZZLE=<3, 3, 3>, BLOCK SWIZZLE=2048
M N K =  12544  12544  12544, Time =   0.03413504   0.03414354   0.03415450 s, AVG Performance =   115.6191 Tflops
M N K =  15360  15360  15360, Time =   0.06227354   0.06228111   0.06228992 s, AVG Performance =   116.3717 Tflops
M N K =  15616  15616  15616, Time =   0.06492467   0.06493727   0.06496666 s, AVG Performance =   117.2858 Tflops
M N K =  15872  15872  15872, Time =   0.06843085   0.06843873   0.06844723 s, AVG Performance =   116.8485 Tflops
M N K =  16128  16128  16128, Time =   0.07200256   0.07200881   0.07201792 s, AVG Performance =   116.5161 Tflops
M N K =  16384  16384  16384, Time =   0.07564493   0.07565752   0.07567462 s, AVG Performance =   116.2620 Tflops

./hgemm_cublas.bin
# NVIDIA L20
ALGO = cuBLAS CUBLAS_GEMM_DEFAULT_TENSOR_OP TN
M N K =  12544  12544  12544, Time =   0.03472691   0.03472968   0.03473408 s, AVG Performance =   113.6678 Tflops
M N K =  15360  15360  15360, Time =   0.06332416   0.06333143   0.06334157 s, AVG Performance =   114.4417 Tflops
M N K =  15616  15616  15616, Time =   0.06649446   0.06650184   0.06651699 s, AVG Performance =   114.5264 Tflops
M N K =  15872  15872  15872, Time =   0.06977024   0.06977659   0.06978355 s, AVG Performance =   114.6081 Tflops
M N K =  16128  16128  16128, Time =   0.07319142   0.07320709   0.07326925 s, AVG Performance =   114.6089 Tflops
M N K =  16384  16384  16384, Time =   0.07668429   0.07669371   0.07670784 s, AVG Performance =   114.6912 Tflops
```

## ğŸ“– Benchmark  

<div id="perf-l20"></div>  

### ğŸ“– NVIDIA L20  
<!--
ç›®å‰æœ€ä¼˜çš„å®ç°ï¼Œåœ¨L20ä¸Šï¼ˆç†è®ºTensor Cores FP16ç®—åŠ›ä¸º 119.5 TFLOPSï¼‰ï¼Œæ•´ä½“ä¸Šèƒ½è¾¾åˆ°cuBLASå¤§æ¦‚`99~100+%`å·¦å³çš„æ€§èƒ½ã€‚ä½¿ç”¨WMMA APIèƒ½è¾¾åˆ°cuBLASå¤§æ¦‚`95%~98%`å·¦å³çš„æ€§èƒ½(105-113 TFLOPS vs 105-115 TFLOPS)ï¼Œä½¿ç”¨MMA APIèƒ½è¾¾åˆ°115 TFLOPSï¼Œéƒ¨åˆ† case ä¼šè¶…è¶Š cuBLASã€‚CuTe ç‰ˆæœ¬çš„ HGEMM å®ç°äº† Block Swizzleï¼ˆL2 Cache friendlyï¼‰å’Œ SMEM Swizzleï¼ˆbank conflicts freeï¼‰ï¼Œæ€§èƒ½æœ€ä¼˜ï¼Œå¤§è§„æ¨¡çŸ©é˜µä¹˜èƒ½è¾¾åˆ° 116-117 TFLOPSï¼Œæ˜¯ cuBLAS å¤§æ¦‚`98%~100%+`å·¦å³çš„æ€§èƒ½ï¼Œå¾ˆå¤šcaseä¼šè¶…è¶ŠcuBLASã€‚ç›®å‰é€šè¿‡ SMEM Padding å’Œ SMEM Swizzle çš„æ–¹å¼ç¼“è§£ bank conflictsã€‚å¯¹äº NN layoutï¼Œä½¿ç”¨ SMEM Padding ç¼“è§£ bank conflictsï¼›å¯¹äº TN layoutï¼Œé€šè¿‡ CUTLASS/CuTe çš„ SMEM Swizzle æ¶ˆé™¤ bank conflictsã€‚
-->
The current best implementation, on the L20 (with a theoretical Tensor Cores FP16 performance of 119.5 TFLOPS), achieves performance that is approximately 99~100+% of cuBLAS.

- Using the WMMA API, it can achieve around 95%~98% of cuBLAS performance (105-113 TFLOPS vs 105-115 TFLOPS).
- Using the MMA API, it can reach 115 TFLOPS, surpassing cuBLAS in some cases.
- The CuTe version of HGEMM implements Block Swizzle (L2 Cache friendly) and SMEM Swizzle (bank conflicts free), achieving the best performance. For large-scale matrix multiplication, it can reach 116-117 TFLOPS, which is approximately 98%~100%+ of cuBLAS performance, and it outperforms cuBLAS in many cases.

Currently, SMEM Padding and SMEM Swizzle are used to mitigate bank conflicts:

- For the NN layout, SMEM Padding is used to alleviate bank conflicts.
- For the TN layout, CUTLASS/CuTe's SMEM Swizzle is used to eliminate bank conflicts.

<div id="NV-L20"></div>


![NVIDIA_L20_NN+TN+v2](https://github.com/user-attachments/assets/71927ac9-72b3-4ce9-b0e2-788b5885bc99)

  
The command for testing all MNK setups (Tip: Performance data for each MNK tested individually is more accurate.)
```bash
python3 hgemm.py --cute-tn --mma --plot
```

### ğŸ“– NVIDIA GeForce RTX 4090

<div id="perf-4090"></div>  

<!--
åœ¨NVIDIA RTX 4090ä¸Š(FP16 Tensor Coresç®—åŠ›ä¸º330 TFLOPS)ï¼ŒWMMA(m16n16k16)æ€§èƒ½è¡¨ç°æ¯”MMA(m16n8k16)è¦æ›´å¥½ï¼Œå¤§åˆ†éƒ¨MNKä¸‹ï¼Œæœ¬ä»“åº“çš„å®ç°èƒ½è¾¾åˆ°cuBLAS 95%~99%çš„æ€§èƒ½ï¼ŒæŸäº›caseèƒ½è¶…è¿‡cuBLASã€‚å°±æœ¬ä»“åº“çš„å®ç°è€Œè¨€ï¼Œåœ¨RTX 4090ä¸Šï¼Œå¤§è§„æ¨¡çŸ©é˜µä¹˜(MNK>=8192)ï¼ŒWMMAè¡¨ç°æ›´ä¼˜ï¼Œå°è§„æ¨¡çŸ©é˜µä¹˜ï¼ŒMMAè¡¨ç°æ›´ä¼˜ã€‚
-->

On the NVIDIA RTX 4090 (with an FP16 Tensor Cores performance of 330 TFLOPS), the WMMA (m16n16k16) implementation shows better performance compared to MMA (m16n8k16). For most MNK configurations, this repository's implementation achieves 95%~99% of cuBLAS performance, and in certain cases, it can surpass cuBLAS. Specifically:

- For large-scale matrix multiplications (MNK >= 8192), the WMMA implementation performs better.
- For small-scale matrix multiplications, the MMA implementation is more efficient.


![NVIDIA_GeForce_RTX_4090_NN+TN+v4](https://github.com/user-attachments/assets/05ef4f5e-d999-48ea-b58e-782cffb24e85)

```bash
python3 hgemm.py --cute-tn --mma --wmma-all --plot
```

### ğŸ“– NVIDIA GeForce RTX 3080 Laptop   

<div id="perf-3080"></div>  

<!--
åœ¨NVIDIA GeForce RTX 3080 Laptopä¸Šæµ‹è¯•ï¼Œä½¿ç”¨mma4x4_warp4x4ï¼ˆ16 WMMA m16n16k16 ops, warp tile 64x64ï¼‰ä»¥åŠThread block swizzleï¼Œå¤§éƒ¨åˆ†caseèƒ½æŒå¹³ç”šè‡³è¶…è¿‡cuBLASï¼Œä½¿ç”¨Windows WSL2 + RTX 3080 Laptopè¿›è¡Œæµ‹è¯•ã€‚
-->
Testing was conducted on a NVIDIA GeForce RTX 3080 Laptop using the mma4x4_warp4x4 configuration (which includes 16 WMMA m16n16k16 operations with a warp tile size of 64x64) along with Thread block swizzle. In most cases, this setup matches or even exceeds cuBLAS performance. The tests were performed using Windows WSL2 + RTX 3080 Laptop.

![image](https://github.com/user-attachments/assets/9472e970-c083-4b31-9252-3eeecc761078)

```bash
python3 hgemm.py --wmma-all --plot
```

<details>
<summary> ğŸ”‘ï¸ Performance Optimization Notes(TODO)</summary>    

## ğŸ“– Performance Optimization Notes

<div id="opt-docs"></div>  

### PyTorch HGEMM Profile

åœ¨Adaæ¶æ„ä¸‹ï¼ŒPyTorch 2.4å¯¹FP16ä½¿ç”¨matmulæ—¶ï¼Œä¼šè°ƒç”¨:
```C++
ampere_fp16_s1688gemm_fp16_128x128_ldg8_f2f_stages_32x1_nn_kernel
```
å†…éƒ¨å®é™…ä½¿ç”¨HMMA(Tensor Cores)è¿›è¡Œè®¡ç®—ï¼Œåœ¨3080ä¸Šprofileå‘ç°ä½¿ç”¨:
```C++
sm80_xmma_gemm_f16f16_f16f32_f32_nn_n_tilesize96x64x32_stage3_warpsize2x2x1_tensor16x8x16_kernel
```
å› æ­¤ï¼Œåªæœ‰å®ç°ä½¿ç”¨Tensor Coresçš„HGEMMï¼Œæ‰æœ‰å¯èƒ½æ¥è¿‘PyTorch/cuBLASçš„æ€§èƒ½ã€‚
```bash
ncu -o hgemm.prof -f python3 bench/prof.py
nsys profile --stats=true -t cuda,osrt,nvtx -o hgemm.prof --force-overwrite true python3 prof.py
```
- SASS (L20)

```C
// ampere_fp16_s1688gemm_fp16_128x128_ldg8_f2f_stages_32x1_nn_kernel
310	00007f41 37d5b850	      LDSM.16.M88.4 R192, [R169+UR8+0x2000] 
311	00007f41 37d5b860	      LDSM.16.M88.4 R196, [R169+UR8+0x2800]
336	00007f41 37d5b9f0	      HMMA.1688.F32 R112, R182, R196, R112
...
```

### SMEM Padding  

#### Bank Conflictsçš„äº§ç”Ÿ
  
å«ä¹‰ï¼šåœ¨è®¿é—®shared memoryæ—¶ï¼Œå› å¤šä¸ªçº¿ç¨‹è¯»å†™åŒä¸€ä¸ªBankä¸­çš„ä¸åŒæ•°æ®åœ°å€æ—¶ï¼Œå¯¼è‡´shared memory å¹¶å‘è¯»å†™ é€€åŒ– æˆé¡ºåºè¯»å†™çš„ç°è±¡å«åšBank Conflictï¼›

![](https://github.com/PaddleJitLab/CUDATutorial/blob/develop/docs/09_optimize_reduce/02_bank_conflict/images/ef322be7c3e5b6b9be69d2b90e88083f50569a58a97129f348e483b946ab4edf.png)

SMè°ƒåº¦å•ä½ä¸ºä¸€ä¸ªwarpï¼ˆä¸€ä¸ªwarpå†…32ä¸ªThreadï¼‰ï¼Œshared_memory å¯ä»¥ è¢«ä¸€ä¸ªwarpä¸­çš„æ‰€æœ‰ï¼ˆ32ä¸ªï¼‰çº¿ç¨‹è¿›è¡Œè®¿é—®ï¼Œshared_memory æ˜ å°„åˆ°å¤§å°ç›¸ç­‰çš„32ä¸ªBankä¸Šï¼ŒBankçš„æ•°æ®è¯»å–å¸¦å®½ä¸º32bit / cycle (4 bytes)ï¼Œå› æ­¤ï¼Œä¸»è¦éœ€è¦è€ƒè™‘ä¸€ä¸ªWarpå†…32çº¿ç¨‹çš„è®¿é—®å…±äº«å†…å­˜æ—¶çš„bankå†²çªã€‚
å¯¹äºå¤šä¸ªçº¿ç¨‹è¯»å–åŒä¸€ä¸ªBankæ•°æ®æ—¶ï¼ˆä¸åŒåœ°å€ï¼‰ï¼Œç¡¬ä»¶æŠŠå†…å­˜è¯»å†™è¯·æ±‚ï¼Œæ‹†åˆ†æˆ conflict-free requestsï¼Œè¿›è¡Œé¡ºåºè¯»å†™ï¼Œæ­¤æ—¶å°†ä¼šè§¦å‘å¤šæ¬¡å†…å­˜äº‹åŠ¡ã€‚ç‰¹åˆ«åœ°ï¼Œå½“ä¸€ä¸ªwarpä¸­çš„æ‰€æœ‰çº¿ç¨‹è¯»å†™åŒä¸€ä¸ªåœ°å€æ—¶ï¼Œä¼šè§¦å‘broadcastæœºåˆ¶ï¼Œæ­¤æ—¶ä¸ä¼šé€€åŒ–æˆé¡ºåºè¯»å†™ã€‚ä¸Šé¢æåˆ°è§¦å‘broadcastæœºåˆ¶çš„æ¡ä»¶æ˜¯all threads acess same addressï¼Œä½†åœ¨ç¿»é˜…cuda-c-programming-guideä»¥åŠæœ€æ–°ç‰ˆæœ¬çš„[NVProfGuide](https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html) æ—¶ï¼Œå‘ç°åªè¦æ˜¯å¤šä¸ªthread è¯»å†™å°±ä¼šè§¦å‘broadcastï¼ˆä¸éœ€è¦Allï¼‰ã€‚
  
- å¤šä¸ªçº¿ç¨‹è¯»åŒä¸€ä¸ªæ•°æ®æ—¶ï¼Œä»…æœ‰ä¸€ä¸ªçº¿ç¨‹è¯»ï¼Œç„¶åbroadcaståˆ°å…¶ä»–çº¿ç¨‹
- å¤šä¸ªçº¿ç¨‹å†™åŒä¸€ä¸ªæ•°æ®æ—¶ï¼Œä»…ä¼šæœ‰ä¸€ä¸ªçº¿ç¨‹å†™æˆåŠŸ

NVIDIAçš„[æ–‡ç« ](https://developer.nvidia.com/blog/using-shared-memory-cuda-cc/)ä¸­æŒ‡å‡ºï¼Œæˆ‘ä»¬è¿˜å¯ä»¥é€šè¿‡ `cudaDeviceSetSharedMemConfig()` å‡½æ•°è®¾ç½®é»˜è®¤Bank Sizeï¼ˆé»˜è®¤ä¸º4 bytesï¼‰æ¥é¿å…bank conflictsï¼Œå¯è®¾ç½®ä¸ºcudaSharedMemBankSizeFourByteæˆ–è€…cudaSharedMemBankSizeEightByteã€‚å¯¹äºæŸäº›åœºæ™¯æ¥è¯´ï¼Œè®¾ç½®cudaSharedMemBankSizeEightByteæˆ–è®¸æ›´åŠ åˆé€‚ï¼Œæ¯”å¦‚ä½¿ç”¨doubleæ•°æ®ç±»å‹æ—¶ã€‚ 

```C
cudaDeviceSetSharedMemConfig(cudaSharedMemBankSizeEightByte);
```
ç›®å‰é€šè¿‡ SMEM Padding å’Œ SMEM swizzleçš„æ–¹å¼ç¼“è§£bank conflictsã€‚å¯¹äº NN layoutï¼Œä½¿ç”¨ SMEM Padding ç¼“è§£ bank conflictsï¼›å¯¹äº TN layoutï¼Œé€šè¿‡cutlass cuteçš„ SMEM Swizzle æ¶ˆé™¤ bank conflictsã€‚

### åŒç¼“å†² Double Buffers

æœ¬ä»“åº“å®ç°çš„HGEMM Double Buffersç­–ç•¥å¦‚ä¸‹ï¼š1ï¼‰ä¸»å¾ªç¯ä»bk = 1 å¼€å§‹ï¼Œç¬¬ä¸€æ¬¡æ•°æ®åŠ è½½åœ¨ä¸»å¾ªç¯ä¹‹å‰ï¼Œæœ€åä¸€æ¬¡è®¡ç®—åœ¨ä¸»å¾ªç¯ä¹‹åï¼Œè¿™æ˜¯pipeline çš„ç‰¹ç‚¹å†³å®šçš„ï¼›2ï¼‰ç”±äºè®¡ç®—å’Œä¸‹ä¸€æ¬¡è®¿å­˜ä½¿ç”¨çš„Shared Memoryä¸åŒï¼Œå› æ­¤ä¸»å¾ªç¯ä¸­æ¯æ¬¡å¾ªç¯åªéœ€è¦ä¸€æ¬¡__syncthreads()å³å¯ï¼Œå¯¹æ¯”édouble buffersç‰ˆæœ¬ï¼Œæ€»å…±èŠ‚çœäº† ((K + BK - 1) / BK) - 1 æ¬¡blockå†…çš„åŒæ­¥æ“ä½œã€‚æ¯”å¦‚ï¼Œbk=1æ—¶ï¼ŒHFMAè®¡ç®—ä½¿ç”¨çš„æ˜¯s_a[0]å’Œs_b[0]ï¼Œå› æ­¤ï¼Œå’Œs_a[1]å’Œs_b[1]çš„åŠ è½½æ˜¯æ²¡æœ‰ä¾èµ–å…³ç³»çš„ã€‚HFMAè®¡ç®—ï¼Œä»globalå†…å­˜åˆ°s_a[1]å’Œs_b[1]å’ŒHFMAè®¡ç®—å¯ä»¥å¹¶è¡Œã€‚s_a[1]å’Œs_b[1]ç”¨äºåŠ è½½ä¸‹ä¸€å—BKéœ€è¦çš„æ•°æ®åˆ°å…±äº«å†…å­˜ï¼›3ï¼‰ç”±äºGPUä¸èƒ½å‘CPUé‚£æ ·æ”¯æŒä¹±åºæ‰§è¡Œï¼Œä¸»å¾ªç¯ä¸­éœ€è¦å…ˆå°†ä¸‹ä¸€æ¬¡å¾ªç¯è®¡ç®—éœ€è¦çš„Gloabal Memoryä¸­çš„æ•°æ®load åˆ°å¯„å­˜å™¨ï¼Œç„¶åè¿›è¡Œæœ¬æ¬¡è®¡ç®—ï¼Œä¹‹åå†å°†loadåˆ°å¯„å­˜å™¨ä¸­çš„æ•°æ®å†™åˆ°Shared Memoryï¼Œè¿™æ ·åœ¨LDGæŒ‡ä»¤å‘Global Memoryåšloadæ—¶ï¼Œä¸ä¼šå½±å“åç»­HFMAåŠå…¶å®ƒè¿ç®—æŒ‡ä»¤çš„ launch æ‰§è¡Œï¼Œä¹Ÿå°±è¾¾åˆ°äº†Double Buffersçš„ç›®çš„ï¼Œå…·ä½“ä»£ç è§[hgemm.cu](./hgemm.cu)ã€‚


### Tile Block

TODO

### Tile Thread

TODO

### Pack LDST 128 bits

TODO

### Async Copy

TODO

### Multi Stages

TODO

### Tensor Cores(WMMA/MMA)

TODO

### Tile MMA/Warp

TODO 

### Thread Block Swizze 

TODO

### Warp Swizzle

TODO

### Reg Double Buffers

TODO

### Collective Store(Reg Reuse&Warp Shuffle)

TODO

### SMEM Swizzle/Permuted

TODO

</details>

## ğŸ“– References 

<div id="ref"></div>  

- [flash-attention-minimal](https://github.com/tspeterkim/flash-attention-minimal)
- [tiny-flash-attention](https://github.com/66RING/tiny-flash-attention)
- [cute-gemm](https://github.com/reed-lau/cute-gemm)
- [cutlass_flash_atten_fp8](https://github.com/weishengying/cutlass_flash_atten_fp8)
- [cuda_learning](https://github.com/ifromeast/cuda_learning)
- [cuda_hgemm](https://github.com/Bruce-Lee-LY/cuda_hgemm)
- [cuda-tensorcore-hgemm](https://github.com/nicolaswilde/cuda-tensorcore-hgemm)
- [How_to_optimize_in_GPU](https://github.com/Liu-xiandong/How_to_optimize_in_GPU/tree/master/sgemv)
- [cute_gemm](https://github.com/weishengying/cute_gemm)
- [cutlass](https://github.com/NVIDIA/cutlass)
