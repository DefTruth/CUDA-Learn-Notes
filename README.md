![cuda-learn-note](https://github.com/DefTruth/CUDA-Learn-Note/assets/31974251/882271fe-ab60-4b0e-9440-2e0fa3c0fb6f)   

<div align='center'>
  <img src=https://img.shields.io/badge/Language-CUDA-brightgreen.svg >
  <img src=https://img.shields.io/github/watchers/DefTruth/cuda-learn-note?color=9cc >
  <img src=https://img.shields.io/github/forks/DefTruth/cuda-learn-note.svg?style=social >
  <img src=https://img.shields.io/github/stars/DefTruth/cuda-learn-note.svg?style=social >
  <img src=https://img.shields.io/badge/Release-v0.3-brightgreen.svg >
  <img src=https://img.shields.io/badge/License-GPLv3.0-turquoise.svg >
 </div>   

ğŸ“’**CUDA-Learn-Notes**: CUDA ç¬”è®° / å¤§æ¨¡å‹æ‰‹æ’•CUDA / C++ç¬”è®°ï¼Œæ›´æ–°éšç¼˜: flash_attnã€sgemmã€sgemvã€warp reduceã€block reduceã€dotã€elementwiseã€softmaxã€layernormã€rmsnormã€histogramã€reluã€sigmoid etc.
<!--
<p align="center"> <a > ğŸŒŸå¦‚æœè§‰å¾—æœ‰ç”¨ï¼Œä¸å¦¨ç»™ä¸ªğŸŒŸğŸ‘†ğŸ»Staræ”¯æŒä¸€ä¸‹å§~ </a> </p>
-->

## å…¶ä»–é¡¹ç›® ğŸ”¥ğŸ”¥ 

|ğŸ› [lite.ai.toolkit](https://github.com/DefTruth/lite.ai.toolkit) | ğŸ’[torchlm](https://github.com/DefTruth/torchlm) | ğŸ“’[statistic-learning-R-note](https://github.com/DefTruth/statistic-learning-R-note) | ğŸ‰[cuda-learn-note](https://github.com/DefTruth/cuda-learn-note) | ğŸ“–[Awesome-LLM-Inference](https://github.com/DefTruth/Awesome-LLM-Inference) |   
|:---:|:---:|:---:|:---:|:---:|
|![](https://img.shields.io/github/stars/DefTruth/lite.ai.toolkit.svg?style=social) ![](https://img.shields.io/github/downloads/DefTruth/lite.ai.toolkit/total?color=ccf&label=downloads&logo=github&logoColor=lightgrey)| ![](https://img.shields.io/github/stars/DefTruth/torchlm.svg?style=social)   ![](https://static.pepy.tech/personalized-badge/torchlm?period=total&units=international_system&left_color=grey&right_color=blue&left_text=downloads)| ![](https://img.shields.io/github/stars/DefTruth/statistic-learning-R-note.svg?style=social) ![](https://img.shields.io/github/downloads/DefTruth/statistic-learning-R-note/total?color=ccf&label=downloads&logo=github&logoColor=lightgrey) |![](https://img.shields.io/github/stars/DefTruth/cuda-learn-note.svg?style=social) ![](https://img.shields.io/github/issues/DefTruth/cuda-learn-note?color=9cc)|  ![](https://img.shields.io/github/stars/DefTruth/Awesome-LLM-Inference.svg?style=social) ![](https://img.shields.io/github/downloads/DefTruth/Awesome-LLM-Inference/total?color=ccf&label=downloads&logo=github&logoColor=lightgrey)|

## 0x00 å‰è¨€
å‰æ®µæ—¶é—´å‚åŠ äº†ä¸€äº›**LLM AI Infra**é¢è¯•ï¼ŒåŸºæœ¬éƒ½è¦æ‰‹æ’•**CUDA**âš¡ï¸ï¼Œäºæ˜¯æ•´ä½“å¤ä¹ äº†ä¸€ä¸‹**CUDA**ä¼˜åŒ–çš„å†…å®¹ï¼Œä¹Ÿæ•´ç†äº†ä¸€äº›é«˜é¢‘é¢˜çš„å†™æ³•ã€‚ç¬”è®°åˆ†äº«åœ¨è¿™é‡Œï¼Œä¸å®šæœŸæ›´æ–°ã€‚å…³äº**LLM AI Infra**ï¼Œä¹Ÿæ¨èæˆ‘æ•´ç†çš„: ğŸ“–[Awesome-LLM-Inference](https://github.com/DefTruth/Awesome-LLM-Inference)  ![](https://img.shields.io/github/stars/DefTruth/Awesome-LLM-Inference.svg?style=social)

 
## 0x01 ğŸ“–ç›®å½•
<div id="kernellist"></div>  

- [x] ğŸ“– [sgemm_naive_f32_kernel](#sgemm)
- [x] ğŸ“– [sgemm_block_tile_k_tile_vec4_f32_kernel](#sgemm)
- [x] ğŸ“– [sgemv_k32_f32_kernel](#sgemv)
- [x] ğŸ“– [sgemv_k128_f32_kernel](#sgemv)
- [x] ğŸ“– [sgemv_k16_f32_kernel](#sgemv)
- [x] ğŸ“– [warp_reduce_sum/max_f32_kernel](#warpreduce)
- [x] ğŸ“– [block_reduce_sum/max_f32_kernel](#warpreduce)
- [x] ğŸ“– [block_all_reduce_f32_kernel](#blockallreduce)
- [x] ğŸ“– [block_all_reduce_vec4_f32_kernel](#blockallreduce)
- [x] ğŸ“– [dot_product_f32_kernel](#dot)
- [x] ğŸ“– [dot_product_vec4_f32_kernel](#dot)
- [x] ğŸ“– [elementwise_f32_kernel](#elementwise)
- [x] ğŸ“– [elementwise_vec4_f32_kernel](#elementwise)
- [x] ğŸ“– [histogram_i32_kernel](#histogram)
- [x] ğŸ“– [histogram_vec4_i32_kernel](#histogram)
- [x] ğŸ“– [softmax_f32_kernel (grid level memory fence)](#softmax)
- [x] ğŸ“– [softmax_vec4_f32_kernel (grid level memory fence)](#softmax)
- [ ] ğŸ“– [safe_softmax_f32_kernel (per token)](#softmax)
- [x] ğŸ“– [sigmoid_f32_kernel](#sigmoid)
- [x] ğŸ“– [sigmoid_vec4_f32_kernel](#sigmoid)
- [ ] ğŸ“– [safe_sigmoid_f32_kernel](#sigmoid)
- [x] ğŸ“– [relu_f32_kernel](#relu)
- [x] ğŸ“– [relu_vec4_f32_kernel](#relu)
- [x] ğŸ“– [layer_norm_f32_kernel (per token)](#layernorm)
- [x] ğŸ“– [layer_norm_vec4_f32_kernel (per token)](#layernorm)
- [ ] ğŸ“– [layer_norm_vec4_f16_kernel (per token)](#layernorm)
- [x] ğŸ“– [rms_norm_f32_kernel (per token)](#rmsnorm)
- [x] ğŸ“– [rms_norm_vec4_f32_kernel (per token)](#rmsnorm)
- [ ] ğŸ“– [rms_norm_vec4_f16_kernel (per token)](#rmsnorm)
- [x] ğŸ“– [flash_attn_1_fwd_f32_kernel](./flash_attn_1_fwd_f32.cu)
- [ ] ğŸ“– flash_attn_2_fwd_f32_kernel
- [ ] ğŸ“– flash_attn_2_fwd_f16_kernel
- [ ] ğŸ“– flash_attn_2_fwd_b16_kernel
- [ ] ğŸ“– flash_attn_2_fwd_f8_kernel
- [ ] ğŸ“– flash_attn_2_split_kv_f16_kernel
- [ ] ğŸ“– flash_attn_2_split_kv_b16_kernel
- [ ] ğŸ“– flash_attn_2_split_kv_f8_kernel
- [ ] ğŸ“– online_softmax_f32_kernel
- [ ] ğŸ“– online_softmax_f16_kernel
- [ ] ğŸ“– online_softmax_b16_kernel
- [ ] ğŸ“– hgemm_f16_kernel
- [ ] ğŸ“– sgemm_dbuf_f32_kernel

## 0x02 sgemm naive, sgemm + block-tile + k-tile + vec4  ([Â©ï¸backğŸ‘†ğŸ»](#kernellist))  
<div id="sgemm"></div>  

```c++
#include <stdio.h>
#include <stdlib.h>
#include <float.h>
#include <vector>
#include <algorithm>
#include <cuda_runtime.h>

#define WARP_SIZE 32
#define INT4(value) (reinterpret_cast<int4*>(&(value))[0])
#define FLOAT4(value) (reinterpret_cast<float4*>(&(value))[0])

// SGEMM: Block Tile + K Tile, with smem
// Block Tile (BM, BN) + K Tile (BK=32)
// grid((N + BN - 1) / BN, (M + BM - 1) / BM), block(BN, BM)
// a: MxK, b: KxN, c: MxN, compute: c = a * b, all row major  
__global__ void sgemm(float* a, float* b, float* c, int M, int N, int K) {
  // [1] Block Tile: 32x32çš„blockå¤„ç†cä¸Šä¸€å—32x32çš„å…ƒç´ è®¡ç®—
  // [2]     K Tile: ä½¿ç”¨å…±äº«å†…å­˜ï¼Œå¹¶å°†Kåˆ†å—ä¸ºBKå¤§å°çš„å—
  constexpr int BM = 32;
  constexpr int BN = 32;
  constexpr int BK = 32;
  __shared__ float s_a[BM][BK], s_b[BK][BN]; 

  int bx = blockIdx.x;
  int by = blockIdx.y;
  int tx = threadIdx.x;
  int ty = threadIdx.y;
  int tid = threadIdx.y * blockDim.x + tx; // tid within the block
  // load values to shared memory, 32x32 threads working together 
  // to fetch data along the row direction of a and b both for s_a 
  // and s_b 32x32x4x2=8KB, we use 32x32 threads within block to 
  // load 32x32 elements from global memory to shared memory, namely, 
  // each thread will load 1 element.
  int load_smem_a_m = tid / 32; // 0~31, tid / 32, tid / BM, threadIdx.y
  int load_smem_a_k = tid % 32; // 0~31, tid % 32, tid % BK, threadIdx.x
  int load_smem_b_k = tid / 32; // 0~31, tid / 32, tid / BK, threadIdx.y
  int load_smem_b_n = tid % 32; // 0~31, tid % 32, tid % BN, threadIdx.x
  int load_gmem_a_m = by * BM + load_smem_a_m; // global row of a and c
  int load_gmem_b_n = bx * BN + load_smem_b_n; // global col of b and c
  // if (load_gmem_a_m >= M || load_gmem_b_n >= N) return;
  
  float sum = 0.f;
  for (int bk = 0; bk < (K + BK - 1) / BK; ++bk) {
    int load_gmem_a_k = bk * BK + load_smem_a_k;
    int load_gmem_a_addr = load_gmem_a_m * K + load_gmem_a_k;
    s_a[load_smem_a_m][load_smem_a_k] = a[load_gmem_a_addr];
    int load_gmem_b_k = bk * BK + load_smem_b_k;
    int load_gmem_b_addr = load_gmem_b_k * N + load_gmem_b_n;
    s_b[load_smem_b_k][load_smem_b_n] = b[load_gmem_b_addr];
    __syncthreads();
    #pragma unroll
    for (int k = 0; k < BK; ++k) {
      int comp_smem_a_m = load_smem_a_m;
      int comp_smem_b_n = load_smem_b_n;
      sum += s_a[comp_smem_a_m][k] * s_b[k][comp_smem_b_n];
    }
    __syncthreads();
  }
  int store_gmem_c_m = load_gmem_a_m;
  int store_gmem_c_n = load_gmem_b_n;
  int store_gmem_c_addr = store_gmem_c_m * N + store_gmem_c_n;
  c[store_gmem_c_addr] = sum;
}

// SGEMM: Block Tile + Thread Tile + K Tile + Vec4, with smem
// BK:TILE_K=8 BM=BN=128
// TM=TN=8 å¢åŠ è®¡ç®—å¯†åº¦ BM/TM=16 BN/TN=16
// dim3 blockDim(BN/TN, BM/TM);
// dim3 gridDim((N + BN - 1) / BN, (M + BM - 1) / BM)
__global__ void sgemm_thread_tile_vec4(
  float* a, float* b, float* c, int M, int N, int K) {
  // [1]  Block Tile: ä¸€ä¸ª16x16çš„blockå¤„ç†Cä¸Šå¤§å°ä¸º128X128çš„ä¸€ä¸ªç›®æ ‡å—
  // [2] Thread Tile: æ¯ä¸ªthreadè´Ÿè´£è®¡ç®—TM*TN(8*8)ä¸ªå…ƒç´ ï¼Œå¢åŠ è®¡ç®—å¯†åº¦
  // [3]      K Tile: å°†Kåˆ†å—ï¼Œæ¯å—BKå¤§å°ï¼Œè¿­ä»£(K+BK-1/BK)æ¬¡ï¼Œ
  //                  æ¯æ¬¡è®¡ç®—TM*TNä¸ªå…ƒç´ å„è‡ªçš„éƒ¨åˆ†ä¹˜ç´¯åŠ 
  // [4]   Vectorize: å‡å°‘loadå’ŒstoreæŒ‡ä»¤ï¼Œä½¿ç”¨float4
  constexpr int BM = 128;
  constexpr int BN = 128;
  constexpr int BK = 8; 
  constexpr int TM = 8;
  constexpr int TN = 8;

  int bx = blockIdx.x;
  int by = blockIdx.y;
  int tx = threadIdx.x;
  int ty = threadIdx.y;
  int tid = threadIdx.y * blockDim.x + tx; // tid within the block
  __shared__ float s_a[BM][BK], s_b[BK][BN]; // 2*128*8*4=8KB
  
  // 0. å…ˆè®¡ç®—shared memoryä¸­çš„ç´¢å¼•
  // tidå’Œéœ€è¦åŠ è½½çš„smem s_a[BM][BK] ä¹‹é—´çš„ç´¢å¼•å…³ç³» BM=128 BK=8 æŒ‰è¡Œè¯»å– Aè¡Œä¸»åº
  // å¯¹äºs_aæ¯è¡Œ8ä¸ªæ•°æ®ï¼Œæ¯ä¸ªçº¿ç¨‹è¯»å–4ä¸ªï¼Œéœ€è¦2ä¸ªçº¿ç¨‹ï¼›æ€»å…±128è¡Œï¼Œéœ€è¦128x2åˆšå¥½256çº¿ç¨‹
  int load_smem_a_m = tid / 2; // tid/2 (128/8)*(128/8)=256 threads per block, tid/2->[0,128), BM=128 0~127
  int load_smem_a_k = (tid % 2 == 0) ? 0 : 4;  // (tid%2 == 0) ? 0 : 4, col of s_a 0,4
  // tidå’Œéœ€è¦åŠ è½½çš„smem s_b[BK][BN] ä¹‹é—´çš„ç´¢å¼•å…³ç³» BK=8 BN=128 æŒ‰è¡Œè¯»å– Bè¡Œä¸»åº
  // å¯¹äºs_bæ¯è¡Œ128ä¸ªæ•°æ®ï¼Œæ¯ä¸ªçº¿ç¨‹è¯»4ä¸ªæ•°æ®ï¼Œéœ€è¦32ä¸ªçº¿ç¨‹ï¼›æ€»å…±8è¡Œï¼Œéœ€è¦32x8=256ä¸ªçº¿ç¨‹
  int load_smem_b_k = tid / 32; // tid/32, row of s_b 256/32=8 è¡Œ 0~7
  int load_smem_b_n = (tid % 32) * 4;  // (tid % 32) * 4, col of s_b 0,4,...,124
  // 1. å†è®¡ç®—å…¨å±€å†…å­˜ä¸­çš„ç´¢å¼•
  // è¦åŠ è½½åˆ°s_aä¸­çš„å…ƒç´ å¯¹åº”åˆ°Aå…¨å±€å†…å­˜ä¸­çš„è¡Œæ•° æ¯ä¸ªblockè´Ÿè´£å‡ºCä¸­å¤§å°ä¸ºBM*BNçš„å—
  int load_gmem_a_m = by * BM + load_smem_a_m; // global row of a and c
  int load_gmem_b_n = bx * BN + load_smem_b_n; // global col of b and c
  
  float r_c[TM][TN] = {0.0}; // 8x8
  // 2. å…ˆå¯¹Kè¿›è¡Œåˆ†å—ï¼Œæ¯å—BKå¤§å°
  for (int bk = 0; bk < (K + BK - 1) / BK; ++bk) {
    // åŠ è½½æ•°æ®åˆ°å…±äº«å†…å­˜smem s_a BM*BK 128*8 vectorize float4
    int load_gmem_a_k = bk * BK + load_smem_a_k; // global col of a
    int load_gmem_a_addr = load_gmem_a_m * K + load_gmem_a_k;
    FLOAT4(s_a[load_smem_a_m][load_smem_a_k]) = FLOAT4(a[load_gmem_a_addr]);
    // åŠ è½½æ•°æ®åˆ°å…±äº«å†…å­˜smem s_b BK*BN 8*128 vectorize float4
    int load_gmem_b_k = bk * BK + load_smem_b_k; // global row of b
    int load_gmem_b_addr = load_gmem_b_k * N + load_gmem_b_n; 
    FLOAT4(s_b[load_smem_b_k][load_smem_b_n]) = FLOAT4(b[load_gmem_b_addr]); 
    __syncthreads();
    #pragma unroll
    for (int k = 0; k < BK; k++) {
      // 3. æ¯ä¸ªçº¿ç¨‹è´Ÿè´£è®¡ç®—BM*BN(12x128)ä¸­çš„TM*TN(8x8)ä¸ªå…ƒç´ 
      #pragma unroll
      for (int m = 0; m < TM; m++) {
        #pragma unroll
        for (int n = 0; n < TN; n++) {
          // k from 0~7ï¼Œ0 ~ BK, ty and tx range from 0 to 15, 16x8=128
          int comp_smem_a_m = ty * TM + m;  // 128*8 128/TM(8)=16 Mæ–¹å‘ 16çº¿ç¨‹
          int comp_smem_b_n = tx * TN + n;  // 8*128 128/TN(8)=16 Næ–¹å‘ 16çº¿ç¨‹
          r_c[m][n] += s_a[comp_smem_a_m][k] * s_b[k][comp_smem_b_n];
        }
      }
    }
    __syncthreads();
  }

  #pragma unroll
  for (int m = 0; m < TM; ++m) {
    int store_gmem_c_m = by * BM + ty * TM + m;
    #pragma unroll
    for (int n = 0; n < TN; n += 4) {
      int store_gmem_c_n = bx * BN + tx * TN + n;
      int store_gmem_c_addr = store_gmem_c_m * N + store_gmem_c_n;
      FLOAT4(c[store_gmem_c_addr]) = FLOAT4(r_c[m][n]);
    }
  }
}
```
è¿™é‡Œgemmçš„å®ç°æ¯”è¾ƒç®€å•ï¼Œåªä½¿ç”¨äº†CUDA Coresï¼Œå¹¶ä¸”åªå®ç°Block Tile + K Tileä»¥åŠBlock Tile + K Tile+Thread Tile+å‘é‡åŒ–çš„ç‰ˆæœ¬ã€‚ä¸»è¦åœ¨äºå¦‚ä½•åŠ è½½gmemä¸­çš„æ•°æ®åˆ°smemï¼Œä¹Ÿå°±æ˜¯æŠŠå…¨å±€å†…å­˜ä¸­çš„æ•°æ®ç´¢å¼•mappingåˆ°å…±äº«å†…å­˜ä¸­çš„ã€‚æ ¸å¿ƒæ€ç»´ï¼šæŠŠä¸€ä¸ªblockä¸­çš„çº¿ç¨‹idæŒ‰ç…§çº¿æ€§æ¥ç†è§£ï¼Œç„¶åæŠŠè¿™ä¸ªçº¿æ€§çš„idå’Œå…¨å±€å†…å­˜ç´¢å¼•ä»¥åŠå…±äº«å†…å­˜ç´¢å¼•è¿›è¡ŒåŒ¹é…ã€‚æ¯”å¦‚Block Tile + K Tileçš„å®ç°ï¼Œblockå†…ä¸€å…±32x32ä¸ªThreadsï¼Œéœ€è¦åŠ è½½åˆ°smemçš„æ•°æ®ä¹Ÿæ˜¯32x32ï¼Œé‚£ä¹ˆï¼Œæœ€ç®€å•çš„åšæ³•ï¼Œåªéœ€è¦æ¯ä¸ªçº¿ç¨‹åŠ è½½ä¸€ä¸ªäº’ä¸é‡å¤æ•°æ®å³å¯ã€‚NOTEï¼Œæœ¬æ–‡çš„gemm kernelä¿®æ”¹è‡ªï¼š[ç´«æ°”ä¸œæ¥ï¼šCUDAï¼ˆä¸‰ï¼‰ï¼šé€šç”¨çŸ©é˜µä¹˜æ³•ï¼šä»å…¥é—¨åˆ°ç†Ÿç»ƒ](https://zhuanlan.zhihu.com/p/657632577)


## 0x03 warp/block reduce sum/max  ([Â©ï¸backğŸ‘†ğŸ»](#kernellist))
<div id="warpreduce"></div>  

```C++
// Warp Reduce Sum
template<const int kWarpSize = WARP_SIZE>
__device__ __forceinline__ float warp_reduce_sum(float val) {
  #pragma unroll
  for (int mask = kWarpSize >> 1; mask >= 1; mask >>= 1) {
    val += __shfl_xor_sync(0xffffffff, val, mask);
  }
  return val;
}

// Warp Reduce Max
template<const int kWarpSize = WARP_SIZE>
__device__ __forceinline__ float warp_reduce_max(float val) {
  #pragma unroll
  for (int mask = kWarpSize >> 1; mask >= 1; mask >>= 1) {
    val = fmaxf(val, __shfl_xor_sync(0xffffffff, val, mask));
  }
  return val;
}

// Block reduce sum/max/min device helper for Layer/RMS Norm/Softmax etc.
// grid 1D block 1D, grid(N/128), block(128)
template<const int NUM_THREADS=128>
__device__ __forceinline__ float block_reduce_sum(float val) {
  // always <= 32 warps per block (limited by 1024 threads per block)
  constexpr int NUM_WARPS = (NUM_THREADS + WARP_SIZE - 1) / WARP_SIZE;
  int warp = threadIdx.x / WARP_SIZE;
  int lane = threadIdx.x % WARP_SIZE;
  static __shared__ float shared[NUM_WARPS];
  
  val = warp_reduce_sum<WARP_SIZE>(val);
  if (lane == 0) shared[warp] = val;
  __syncthreads();
  val = (lane < NUM_WARPS) ? shared[lane] : 0.0f;
  val = warp_reduce_sum<NUM_WARPS>(val);
  return val;
}

template<const int NUM_THREADS=128>
__device__ __forceinline__ float block_reduce_max(float val) {
  // always <= 32 warps per block (limited by 1024 threads per block)
  constexpr int NUM_WARPS = (NUM_THREADS + WARP_SIZE - 1) / WARP_SIZE;
  int warp = threadIdx.x / WARP_SIZE;
  int lane = threadIdx.x % WARP_SIZE;
  static __shared__ float shared[NUM_WARPS];
  
  val = warp_reduce_max<WARP_SIZE>(val);
  if (lane == 0) shared[warp] = val;
  __syncthreads();
  val = (lane < NUM_WARPS) ? shared[lane] : -FLT_MAX;
  val = warp_reduce_max<NUM_WARPS>(val);
  return val;
}
```
warp reduceå‡ ä¹å·²ç»æˆä¸ºå¤§éƒ¨åˆ†reduce kernelçš„æ ‡å‡†å†™æ³•äº†ï¼Œæ¯”å¦‚vLLMä¸­ï¼Œå°±æ˜¯è¿™ç§ç»å…¸çš„å†™æ³•ã€‚æ‰€ä»¥ï¼Œå…ˆææ‡‚warp reduceï¼ˆä¹Ÿå°±æ˜¯ææ‡‚å„ç§warp functionsçš„ç”¨æ³•ï¼‰ï¼Œå†å»å†™å…¶ä»–kernelï¼Œæ€è·¯å°±ä¼šå®¹æ˜“å¾ˆå¤šã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œwarpå‡½æ•°å¤„ç†çš„æ˜¯å¯„å­˜å™¨ä¸Šçš„æ•°æ®ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæ­¤æ—¶ï¼Œæ²¡å¿…è¦å…ˆåŠ è½½æ•°æ®åˆ°smemï¼Œå†è¿›è¡Œreduceï¼Œç›´æ¥åŠ è½½åˆ°å¯„å­˜å™¨å³å¯ï¼ˆä»¥å‰çŠ¯è¿‡è¿™ä¸ªå°é”™è¯¯...ï¼‰ã€‚Warp Functionså»ºè®®å‚è€ƒï¼š[jhangï¼šCUDAç¼–ç¨‹å…¥é—¨ä¹‹Warp-Level Primitives](https://zhuanlan.zhihu.com/p/572820783)

## 0x04 block all reduce + vec4  ([Â©ï¸backğŸ‘†ğŸ»](#kernellist))
<div id="blockallreduce"></div>  

```c++
// Block All Reduce Sum
// grid(N/128), block(128)
// a: Nx1, y=sum(a)
template<const int NUM_THREADS = 128>
__global__ void block_all_reduce_sum(float* a, float* y, int N) {
  int tid = threadIdx.x;
  int idx = blockIdx.x * NUM_THREADS + tid;
  constexpr int NUM_WARPS = (NUM_THREADS + WARP_SIZE - 1) / WARP_SIZE;
  __shared__ float reduce_smem[NUM_WARPS];
  // keep the data in register is enougth for warp operaion.
  float sum = (idx < N) ? a[idx] : 0.0f;
  int warp = tid / WARP_SIZE;
  int lane = tid % WARP_SIZE;
  // perform warp sync reduce.
  sum = warp_reduce_sum<WARP_SIZE>(sum);
  // warp leaders store the data to shared memory.
  if (lane == 0) reduce_smem[warp] = sum;
  __syncthreads(); // make sure the data is in shared memory.
  // the first warp compute the final sum.
  sum = (lane < NUM_WARPS) ? reduce_smem[lane] : 0.0f;
  if (warp == 0) sum = warp_reduce_sum<NUM_WARPS>(sum);
  if (tid == 0) atomicAdd(y, sum);
}

// Block All Reduce Sum + float4
// grid(N/128), block(128/4)
// a: Nx1, y=sum(a)
template<const int NUM_THREADS = 128/4>
__global__ void block_all_reduce_sum_vec4(float* a, float* y, int N) {
  int tid = threadIdx.x;
  int idx = (blockIdx.x * NUM_THREADS + tid) * 4;
  constexpr int NUM_WARPS = (NUM_THREADS + WARP_SIZE - 1) / WARP_SIZE;
  __shared__ float reduce_smem[NUM_WARPS];

  float4 reg_a = FLOAT4(a[idx]);
  // keep the data in register is enougth for warp operaion.
  float sum = (idx < N) ? (reg_a.x + reg_a.y + reg_a.z + reg_a.w) : 0.0f;
  int warp = tid / WARP_SIZE;
  int lane = tid % WARP_SIZE;
  // perform warp sync reduce.
  sum = warp_reduce_sum<WARP_SIZE>(sum);
  // warp leaders store the data to shared memory.
  if (lane == 0) reduce_smem[warp] = sum;
  __syncthreads(); // make sure the data is in shared memory.
  // the first warp compute the final sum.
  sum = (lane < NUM_WARPS) ? reduce_smem[lane] : 0.0f;
  if (warp == 0) sum = warp_reduce_sum<NUM_WARPS>(sum);
  if (tid == 0) atomicAdd(y, sum);
}
```
block all reduceæ˜¯åœ¨warp reduceçš„åŸºç¡€ä¸Šè¿›è¡Œçš„ï¼Œreduce_smemè¿™éƒ¨åˆ†çš„å…±äº«å†…å­˜ç”³è¯·æ— æ³•é¿å…ï¼Œè¿™æ˜¯ç”¨æ¥åŒæ­¥æ¯ä¸ªwarpä¹‹é—´å¾—åˆ°å±€éƒ¨ç»“æœã€‚æ³¨æ„ï¼Œæœ€åï¼Œè¿˜éœ€è¦atomicAddåšä¸€ä¸ªblockçº§åˆ«çš„åŸå­æ“ä½œï¼Œä»¥å¾—åˆ°å…¨å±€çš„å’Œã€‚float4å‘é‡åŒ–ä¼˜åŒ–è®¿å­˜ï¼Œå¯ä»¥å‡ç¼“WarpSchedulerå‘é€æŒ‡ä»¤çš„å‹åŠ›ã€‚

## 0x05 sgemv k32/k128/k16 kernel   ([Â©ï¸backğŸ‘†ğŸ»](#kernellist))
<div id="sgemv"></div>  

```C++
// SGEMV: Warp SGEMV K32
// å‡è®¾Kä¸º32çš„å€æ•°ï¼Œæ¯ä¸ªwarpè´Ÿè´£ä¸€è¡Œ
// grid(M/4), block(32,4) blockDim.x=32=K, blockDim.y=4
// a: MxK, x: Kx1, y: Mx1, compute: y = a * x
__global__ void sgemv_k32(float* a, float* x, float* y, int M, int K) {
  int tx = threadIdx.x;         // 0~31
  int ty = threadIdx.y;         // 0~4
  int bx = blockIdx.x;          // 0~M/4
  int lane = tx % WARP_SIZE;    // 0~31
  int m = bx * blockDim.y + ty; // (0~M/4) * 4 + (0~3)
  if (m < M) {
    float sum = 0.0f;
    int NUM_WARPS = (K + WARP_SIZE - 1) / WARP_SIZE;
    #pragma unroll
    for (int w = 0; w < NUM_WARPS; ++w) {
      // è‹¥NUM_WARPS>=2ï¼Œå…ˆå°†å½“å‰è¡Œçš„æ•°æ®ç´¯åŠ åˆ°ç¬¬ä¸€ä¸ªwarpä¸­
      int k = w * WARP_SIZE + lane;
      sum += a[m * K + k] * x[k];
    }
    sum = warp_reduce_sum<WARP_SIZE>(sum);
    if (lane == 0) y[m] = sum;
  }
}

// SGEMV: Warp SGEMV K128 + Vec4
// å‡è®¾Kä¸º128çš„å€æ•° float4
// grid(M/4), block(32,4) blockDim.x=32=K, blockDim.y=4
// a: MxK, x: Kx1, y: Mx1, compute: y = a * x
__global__ void sgemv_k128(float* a, float* x, float* y, int M, int K) {
  // æ¯ä¸ªçº¿ç¨‹è´Ÿè´£4ä¸ªå…ƒç´ ï¼Œä¸€ä¸ªwarpè¦†ç›–128ä¸ªå…ƒç´ 
  int tx = threadIdx.x;         // 0~31
  int ty = threadIdx.y;         // 0~3
  int bx = blockIdx.x;          // 0~M/4
  int lane = tx % WARP_SIZE;    // 0~31
  int m = blockDim.y * bx + ty; // (0~M/4) * 4 + (0~3)
  
  if (m < M) {
    float sum = 0.0f;
    // process 4*WARP_SIZE elements per warp.
    int NUM_WARPS = (((K + WARP_SIZE - 1) / WARP_SIZE) + 4 - 1) / 4;
    #pragma unroll
    for (int w = 0; w < NUM_WARPS; ++w) {
      int k = (w * WARP_SIZE + lane) * 4;
      float4 reg_x = FLOAT4(x[k]);
      float4 reg_a = FLOAT4(a[m * K + k]);
      sum += (reg_a.x * reg_x.x + reg_a.y * reg_x.y 
            + reg_a.z * reg_x.z + reg_a.w * reg_x.w);
    }
    sum = warp_reduce_sum<WARP_SIZE>(sum);
    if(lane == 0) y[m] = sum;
  }
}

// SGEMV: Warp SGEMV K16
// å‡è®¾Kä¸º16 < 32,æ¯ä¸ªwarpè´Ÿè´£2è¡Œï¼Œæ¯è¡Œæœ‰16ä¸ªå…ƒç´ 
// NUM_THREADS=128, NUM_WARPS=NUM_THREADS/WARP_SIZE;
// NUM_ROWS=NUM_WARPS * ROW_PER_WARP, grid(M/NUM_ROWS), block(32,NUM_WARPS)
// a: MxK, x: Kx1, y: Mx1, compute: y = a * x
template<const int ROW_PER_WARP = 2> 
__global__ void sgemv_k16(float* A, float* x, float* y, int M, int K) {
  constexpr int K_WARP_SIZE = (WARP_SIZE + ROW_PER_WARP - 1) / ROW_PER_WARP;
  int tx = threadIdx.x;       // 0~31
  int ty = threadIdx.y;       // 0~NUM_WARPS
  int bx = blockIdx.x;        // 0~M/NUM_ROWS (NUM_ROWS=NUM_WARPS * ROW_PER_WARP)
  int lane = tx % WARP_SIZE;  // 0~31
  int k = lane % K_WARP_SIZE; // 0~15
  // gloabl row of a: MxK and y:Mx1, blockDim.y=NUM_WARPS
  int m = (blockDim.y * bx + ty) * ROW_PER_WARP + lane / K_WARP_SIZE;
  if (m < M) {
    float sum = A[m * K + k] * x[k];
    sum = warp_reduce_sum<K_WARP_SIZE>(sum);
    // æ³¨æ„æ˜¯k == 0ï¼Œè€Œä¸æ˜¯lane == 0
    if(k == 0) y[m] = sum; 
  }
}
```
ä¼°è®¡æœ‰äº›å¤§ä½¬å€’ç«‹éƒ½èƒ½å†™sgemvçš„å„ç§ä¼˜åŒ–ç‰ˆäº†ï¼Œæ ¸å¿ƒæ€è·¯å…¶å®ä¹Ÿæ˜¯åŸºäºwarp reduceï¼Œè€ƒè™‘Kçš„ä¸åŒæƒ…å†µè¿›è¡Œä¼˜åŒ–ã€‚æœ¬æ–‡çš„sgemv kernelä¿®æ”¹è‡ªï¼š[æœ‰äº†ç¦ç¦çš„æ£å­ï¼šæ·±å…¥æµ…å‡ºGPUä¼˜åŒ–ç³»åˆ—ï¼šgemvä¼˜åŒ–](https://zhuanlan.zhihu.com/p/494144694)

## 0x06 dot product, dot product + vec4  ([Â©ï¸backğŸ‘†ğŸ»](#kernellist))
<div id="dot"></div>  

```c++
// Dot Product
// grid(N/128), block(128)
// a: Nx1, b: Nx1, y=sum(elementwise_mul(a,b))
template<const int NUM_THREADS = 128>
__global__ void dot(float* a, float* b, float* y, int N) {
  int tid = threadIdx.x;
  int idx = blockIdx.x * NUM_THREADS + tid;
  constexpr int NUM_WARPS = (NUM_THREADS + WARP_SIZE - 1) / WARP_SIZE;
  __shared__ float reduce_smem[NUM_WARPS];

  // keep the data in register is enougth for warp operaion.
  float prod = (idx < N) ? a[idx] * b[idx] : 0.0f;
  int warp = tid / WARP_SIZE;
  int lane = tid % WARP_SIZE;
  // perform warp sync reduce.
  prod = warp_reduce_sum<WARP_SIZE>(prod);
  // warp leaders store the data to shared memory.
  if (lane == 0) reduce_smem[warp] = prod;
  __syncthreads(); // make sure the data is in shared memory.
  // the first warp compute the final sum.
  prod = (lane < NUM_WARPS) ? reduce_smem[lane] : 0.0f;
  if (warp == 0) prod = warp_reduce_sum<NUM_WARPS>(prod);
  if (tid == 0) atomicAdd(y, prod);
}

// Dot Product + Vec4
// grid(N/128), block(128/4)
// a: Nx1, b: Nx1, y=sum(elementwise_mul(a,b))
template<const int NUM_THREADS = 128/4>
__global__ void dot_vec4(float* a, float* b, float* y, int N) {
  int tid = threadIdx.x;
  int idx = (blockIdx.x * NUM_THREADS + tid) * 4;
  constexpr int NUM_WARPS = (NUM_THREADS + WARP_SIZE - 1) / WARP_SIZE;
  __shared__ float reduce_smem[NUM_WARPS];

  float4 reg_a = FLOAT4(a[idx]);
  float4 reg_b = FLOAT4(b[idx]);
  float prod = (idx < N) ? (reg_a.x * reg_b.x + reg_a.y * reg_b.y 
                          + reg_a.z * reg_b.z + reg_a.w * reg_b.w) : 0.0f;
  int warp = tid / WARP_SIZE;
  int lane = tid % WARP_SIZE;
  // perform warp sync reduce.
  prod = warp_reduce_sum<WARP_SIZE>(prod);
  // warp leaders store the data to shared memory.
  if (lane == 0) reduce_smem[warp] = prod;
  __syncthreads(); // make sure the data is in shared memory.
  // the first warp compute the final sum.
  prod = (lane < NUM_WARPS) ? reduce_smem[lane] : 0.0f;
  if (warp == 0) prod = warp_reduce_sum<NUM_WARPS>(prod);
  if (tid == 0) atomicAdd(y, prod);
}
```
dot product kernelçš„æ ¸å¿ƒå°±æ˜¯block reduceï¼Œä¸å¤šè¯´äº†ã€‚

## 0x07 elementwise, elementwise + vec4  ([Â©ï¸backğŸ‘†ğŸ»](#kernellist))
<div id="elementwise"></div>  

```c++
// ElementWise Add  
// grid(N/128), block(128)
// a: Nx1, b: Nx1, c: Nx1, c = elementwise_add(a, b)
__global__ void elementwise_add(float* a, float* b, float* c, int N) {
  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  if (idx < N) c[idx] = a[idx] + b[idx];
}

// ElementWise Add + Vec4
// grid(N/128), block(128/4)
// a: Nx1, b: Nx1, c: Nx1, c = elementwise_add(a, b)
__global__ void elementwise_add_vec4(float* a, float* b, float* c, int N) {
  int idx = 4 * (blockIdx.x * blockDim.x + threadIdx.x);
  if (idx < N) {
    float4 reg_a = FLOAT4(a[idx]);
    float4 reg_b = FLOAT4(b[idx]);
    float4 reg_c;
    reg_c.x = reg_a.x + reg_b.x;
    reg_c.y = reg_a.y + reg_b.y;
    reg_c.z = reg_a.z + reg_b.z;
    reg_c.w = reg_a.w + reg_b.w;
    FLOAT4(c[idx]) = reg_c;
  }
}
```
elementwiseå¯ä»¥è€ƒè™‘åŠ ç‚¹å‘é‡åŒ–è¿›è¡Œè®¿å­˜ä¼˜åŒ–ã€‚

## 0x08 histogram, histogram + vec4  
<div id="histogram"></div>  

```c++
// Histogram
// grid(N/128), block(128)
// a: Nx1, y: count histogram
__global__ void histogram(int* a, int* y, int N) {
  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  if (idx < N) atomicAdd(&(y[a[idx]]), 1);
}

// Histogram + Vec4
// grid(N/128), block(128/4)
// a: Nx1, y: count histogram
__global__ void histogram_vec4(int* a, int* y, int N) {
  int idx = 4 * (blockIdx.x * blockDim.x + threadIdx.x);
  if (idx < N) {
    int4 reg_a = INT4(a[idx]);
    atomicAdd(&(y[reg_a.x]), 1);
    atomicAdd(&(y[reg_a.y]), 1);
    atomicAdd(&(y[reg_a.z]), 1);
    atomicAdd(&(y[reg_a.w]), 1);
  }
}
```
ç»Ÿè®¡é¢‘æ•°ç›´æ–¹å›¾ï¼Œå¾ˆç®€å•ï¼Œä¸¤è¡Œä»£ç æå®šã€‚

## 0x09 softmax, softmax + vec4 (grid level memory fence)   ([Â©ï¸backğŸ‘†ğŸ»](#kernellist))
<div id="softmax"></div>  

```c++
// Softmax x: N, y: N
// grid(N/128), block(K=128)
template<const int NUM_THREADS = 128>
__global__ void softmax(float* x, float* y, float* total, int N) {
  const int tid = threadIdx.x;
  const int idx = blockIdx.x * blockDim.x + tid; 
  constexpr int NUM_WARPS = (NUM_THREADS + WARP_SIZE - 1) / WARP_SIZE;
  __shared__ float reduce_smem[NUM_WARPS];
  
  float sum = (idx < N) ? expf(x[idx]) : 0.0f;
  int warp = tid / WARP_SIZE;
  int lane = tid % WARP_SIZE;
  sum = warp_reduce_sum<WARP_SIZE>(sum);
  if (lane == 0) reduce_smem[warp] = sum;
  __syncthreads();
  // compute the final sum in each warp
  sum = (lane < NUM_WARPS) ? reduce_smem[lane] : 0.0f;
  sum = warp_reduce_sum<NUM_WARPS>(sum); // sum(e^x_0,...,e^x_n-1)
  // get the total sum of all blocks.
  if (tid == 0) atomicAdd(total, sum);
  __threadfence(); // grid level memory fence æ³¨æ„è¿™é‡Œéœ€è¦ç½‘æ ¼çº§åˆ«çš„å†…å­˜åŒæ­¥
  // e^x_i/sum(e^x_0,...,e^x_n-1) 
  if (idx < N) y[idx] = block_smem[tid] / (*total); 
}

// Softmax x: N, y: N
// grid(N/128), block(K=128)
template<const int NUM_THREADS = 128>
__global__ void softmax_v2(float* x, float* y, float* total, int N) {
  const int tid = threadIdx.x;
  const int idx = blockIdx.x * blockDim.x + tid; 
  
  float exp_val = (idx < N) ? expf(x[idx]) : 0.0f;
  float sum = block_reduce_sum<NUM_THREADS>(exp_val);
  // get the total sum of all blocks.
  if (tid == 0) atomicAdd(total, sum);
  __threadfence(); // grid level memory fence  æ³¨æ„è¿™é‡Œéœ€è¦ç½‘æ ¼çº§åˆ«çš„å†…å­˜åŒæ­¥
  // e^x_i/sum(e^x_0,...,e^x_n-1) 
  if (idx < N) y[idx] = exp_val / (*total); 
}

// Softmax Vec4 x: N, y: N
// grid(N/128), block(128/4)
template<const int NUM_THREADS = 128/4>
__global__ void softmax_v2_vec4(float* x, float* y, float* total, int N) {
  const int tid = threadIdx.x;
  const int idx = (blockIdx.x * blockDim.x + tid) * 4; 
  
  float4 reg_x = FLOAT4(x[idx]);
  float4 reg_exp;
  reg_exp.x = (idx < N) ? expf(reg_x.x) : 0.0f;
  reg_exp.y = (idx < N) ? expf(reg_x.y) : 0.0f;
  reg_exp.z = (idx < N) ? expf(reg_x.z) : 0.0f;
  reg_exp.w = (idx < N) ? expf(reg_x.w) : 0.0f;
  float exp_val = (reg_exp.x + reg_exp.y + reg_exp.z + reg_exp.w);
  float sum = block_reduce_sum<NUM_THREADS>(exp_val);
  // get the total sum of all blocks.
  if (tid == 0) atomicAdd(total, sum);
  __threadfence(); // grid level memory fence  æ³¨æ„è¿™é‡Œéœ€è¦ç½‘æ ¼çº§åˆ«çš„å†…å­˜åŒæ­¥
  // e^x_i/sum(e^x_0,...,e^x_n-1) 
  if (idx < N) {
    float4 reg_y;
    reg_y.x = reg_exp.x / (*total);
    reg_y.y = reg_exp.y / (*total);
    reg_y.z = reg_exp.z / (*total);
    reg_y.w = reg_exp.w / (*total);
    FLOAT4(y[idx]) = reg_y; 
  }
}
```
softmaxç¨å¾®è¦æ³¨æ„çš„å°±æ˜¯å†…å­˜åŒæ­¥çš„é—®é¢˜ï¼Œè¿™é‡Œï¼Œä½ éœ€è¦åšä¸€ä¸ªç½‘æ ¼çº§åˆ«çš„åŒæ­¥ï¼Œè€Œä¸èƒ½ä»…ä»…æ˜¯blockçº§åˆ«ï¼Œå¦åˆ™æ‹¿ä¸åˆ°å…¨å±€çš„exp sumä½œä¸ºåˆ†æ¯é¡¹ã€‚å› æ­¤ä½¿ç”¨ __threadfence è¿™ä¸ªç½‘æ ¼åŠå†…å­˜åŒæ­¥æ“ä½œã€‚ä¸è¿‡æ•ˆç‡æˆ‘è¿˜æ²¡æµ‹è¿‡ï¼Œå®åœ¨è¦é«˜æ•ˆçš„è¯ï¼Œå¯èƒ½å¾—æ•´æˆFA2é‚£æ ·çš„ 1-pass + online softmaxçš„å®ç°ã€‚ä¸è¿‡ï¼Œå¦‚æœæ˜¯é¢è¯•çš„è¯ï¼Œå°±ä¸è¦å¤ªä¸ºéš¾è‡ªå·±äº†...ï¼Œä½†æ˜¯FA1/FA2çš„è®ºæ–‡å¾ˆç»å…¸ï¼Œå¼ºçƒˆå»ºè®®å¤šè¯»å‡ éã€‚

## 0x0a sigmoid, sigmoid + vec4   ([Â©ï¸backğŸ‘†ğŸ»](#kernellist))
<div id="sigmoid"></div>  

```c++
// Sigmoid x: N, y: N y=1/(1+exp(-x))
// grid(N/128), block(K=128) 
__global__ void sigmoid(float* x, float* y, int N) {
  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  if (idx < N) y[idx] = 1.0f / (1.0f + expf(-x[idx]));
}

// Sigmoid x: N, y: N y=1/(1+exp(-x)) Vec4
// grid(N/128), block(128/4)
__global__ void sigmoid_vec4(float* x, float* y, int N) {
  int idx = (blockIdx.x * blockDim.x + threadIdx.x) * 4;
  if (idx < N) {
    float4 reg_x = FLOAT4(x[idx]);
    float4 reg_y;
    reg_y.x = 1.0f / (1.0f + expf(-reg_x.x));
    reg_y.y = 1.0f / (1.0f + expf(-reg_x.y));
    reg_y.z = 1.0f / (1.0f + expf(-reg_x.z));
    reg_y.w = 1.0f / (1.0f + expf(-reg_x.w));
    FLOAT4(y[idx]) = reg_y;
  }
}
```

## 0x0b relu, relu + vec4   ([Â©ï¸backğŸ‘†ğŸ»](#kernellist))
<div id="relu"></div>  

```c++
// Relu x: N, y: N y=max(0,x)
// grid(N/128), block(K=128) 
__global__ void relu(float* x, float* y, int N) {
  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  if (idx < N) y[idx] = fmaxf(0.0f, x[idx]);
}

// Relu x: N, y: N y=max(0,x) Vec4
// grid(N/128/4), block(128/4) 
__global__ void relu_vec4(float* x, float* y, int N) {
  int idx = (blockIdx.x * blockDim.x + threadIdx.x) * 4;
  if (idx < N) {
    float4 reg_x = FLOAT4(x[idx]);
    float4 reg_y;
    reg_y.x = fmaxf(0.0f, reg_x.x);
    reg_y.y = fmaxf(0.0f, reg_x.y);
    reg_y.z = fmaxf(0.0f, reg_x.z);
    reg_y.w = fmaxf(0.0f, reg_x.w);
    FLOAT4(y[idx]) = reg_y;
  }
}
```

## 0x0c layer_norm, layer_norm + vec4   ([Â©ï¸backğŸ‘†ğŸ»](#kernellist))
<div id="layernorm"></div>  

```c++
// Layer Norm: x: NxK(K=128<1024), y': NxK, y'=x-mean(x)/std(x) each row
// mean(x) = sum(x)/K, 1/std(x) = rsqrtf( sum( (x-mean(x))^2 )/K ) each row
// grid(N*K/K), block(K<1024) N=batch_size*seq_len, K=hidden_size
// y=y'*g + b (g: scale, b: bias)
template<const int NUM_THREADS=128>
__global__ void layer_norm(float* x, float* y, float g, float b, int N, int K) {
  int tid = threadIdx.x; // 0..K-1
  int bid = blockIdx.x; // 0..N-1
  int idx = bid * blockDim.x + threadIdx.x;
  const float epsilon = 1e-5f;

  __shared__ float s_mean; // shared within block
  __shared__ float s_variance; // shared within block
  float value = (idx < N * K) ? x[idx] : 0.0f; // load once only
  float sum = block_reduce_sum<NUM_THREADS>(value);
  if (tid == 0) s_mean = sum / (float) K;
  // wait for s_mean in shared memory to be ready for all threads
  __syncthreads();
  float variance = (value - s_mean) * (value - s_mean);
  variance = block_reduce_sum<NUM_THREADS>(variance);
  if (tid == 0) s_variance = rsqrtf(variance / (float) K + epsilon);
  // wait for s_variance in shared memory to be ready for all threads
  __syncthreads();
  if (idx < N * K) y[idx] = ((value - s_mean) * s_variance) * g + b;
}

// Layer Norm Vec4: x: NxK(K=128<1024), y': NxK, y'=x-mean(x)/std(x) each row
// mean(x) = sum(x)/K, 1/std(x) = rsqrtf( sum( (x-mean(x))^2 )/K ) each row
// grid(N*K/K), block(K/4<1024) N=batch_size*seq_len, K=hidden_size
// y=y'*g + b (g: scale, b: bias)
template<const int NUM_THREADS=128/4>
__global__ void layer_norm_vec4(float* x, float* y, float g, float b, int N, int K) {
  int tid = threadIdx.x; // 0..K-1
  int bid = blockIdx.x; // 0..N-1
  int idx = (bid * blockDim.x + threadIdx.x) * 4;
  const float epsilon = 1e-5f;

  __shared__ float s_mean; // shared within block
  __shared__ float s_variance; // shared within block
  float4 reg_x = FLOAT4(x[idx])
  float value = (idx < N * K) ? (reg_x.x + reg_x.y 
                               + reg_x.z + reg_x.w) : 0.0f;
  float sum = block_reduce_sum<NUM_THREADS>(value);
  if (tid == 0) s_mean = sum / (float) K;
  // wait for s_mean in shared memory to be ready for all threads
  __syncthreads();
  float4 reg_x_hat;
  reg_x_hat.x = reg_x.x - s_mean;
  reg_x_hat.y = reg_x.y - s_mean;
  reg_x_hat.z = reg_x.z - s_mean;
  reg_x_hat.w = reg_x.w - s_mean;
  float variance = reg_x_hat.x * reg_x_hat.x + reg_x_hat.y * reg_x_hat.y 
                 + reg_x_hat.z * reg_x_hat.z + reg_x_hat.w * reg_x_hat.w;
  variance = block_reduce_sum<NUM_THREADS>(variance);
  if (tid == 0) s_variance = rsqrtf(variance / (float) K + epsilon);
  // wait for s_variance in shared memory to be ready for all threads
  __syncthreads();
  float4 reg_y;
  reg_y.x = reg_x_hat.x * s_variance * g + b;
  reg_y.y = reg_x_hat.y * s_variance * g + b;
  reg_y.z = reg_x_hat.z * s_variance * g + b;
  reg_y.w = reg_x_hat.w * s_variance * g + b;
  if (idx < N * K) FLOAT4(y[idx]) = reg_y;
}
```
layer normå®ç°çš„æ ¸å¿ƒåŒæ ·ä¹Ÿæ˜¯block reduceå’Œwarp reduceï¼Œç„¶åå†æ•´ç‚¹å‘é‡åŒ–...

## 0x0d rms_norm, rms_norm + vec4   ([Â©ï¸backğŸ‘†ğŸ»](#kernellist))
<div id="rmsnorm"></div>  

```c++
// RMS Norm: x: NxK(K=128<1024), y': NxK, y'=x/rms(x) each row
// 1/rms(x) = rsqrtf( sum(x^2)/K ) each row
// grid(N*K/K), block(K<1024) N=batch_size*seq_len, K=hidden_size
// y=y'*g (g: scale)
template<const int NUM_THREADS=128>
__global__ void rms_norm(float* x, float* y, float g, int N, int K) {
  int tid = threadIdx.x; // 0..K-1
  int bid = blockIdx.x; // 0..N-1
  int idx = bid * blockDim.x + threadIdx.x;
  const float epsilon = 1e-5f;

  __shared__ float s_variance; // shared within block
  float value = (idx < N * K) ? x[idx] : 0.0f; // load once only
  float variance = value * value;
  variance = block_reduce_sum<NUM_THREADS>(variance);
  if (tid == 0) s_variance = rsqrtf(variance / (float) K + epsilon);
  // wait for s_variance in shared memory to be ready for all threads
  __syncthreads(); 
  if (idx < N * K) y[idx] = (value * s_variance) * g;
}

// RMS Norm Vec4: x: NxK(K=128<1024), y': NxK, y'=x/rms(x) each row
// 1/rms(x) = rsqrtf( sum(x^2)/K ) each row
// grid(N*K/K), block(K/4<1024) N=batch_size*seq_len, K=hidden_size
// y=y'*g (g: scale)
template<const int NUM_THREADS=128/4>
__global__ void rms_norm_vec4(float* x, float* y, float g, int N, int K) {
  int tid = threadIdx.x; // 0..K-1
  int bid = blockIdx.x; // 0..N-1
  int idx = (bid * blockDim.x + threadIdx.x) * 4;
  const float epsilon = 1e-5f;

  __shared__ float s_variance; // shared within block
  float4 reg_x = FLOAT4(x[idx]);
  float variance = (idx < N * K) ? (reg_x.x * reg_x.x + reg_x.y * reg_x.y 
                                  + reg_x.z * reg_x.z + reg_x.w * reg_x.w) : 0.0f;
  variance = block_reduce_sum<NUM_THREADS>(variance);
  if (tid == 0) s_variance = rsqrtf(variance / (float) K + epsilon);
  // wait for s_variance in shared memory to be ready for all threads
  __syncthreads(); 
  float4 reg_y;
  reg_y.x = reg_x.x * s_variance * g;
  reg_y.y = reg_x.y * s_variance * g;
  reg_y.z = reg_x.z * s_variance * g;
  reg_y.w = reg_x.w * s_variance * g;
  if (idx < N * K) FLOAT4(y[idx]) = reg_y;
}
```
rms normå®ç°çš„æ ¸å¿ƒåŒæ ·ä¹Ÿæ˜¯block reduceå’Œwarp reduce...ï¼Œç„¶åå†åŠ ç‚¹float4å‘é‡åŒ–ä»€ä¹ˆçš„ã€‚

## 0x0e NMS  ([Â©ï¸backğŸ‘†ğŸ»](#kernellist))
<div id="NMS"></div>  

```c++
struct Box {
  float x1, y1, x2, y2, score;
  float area() const {return (std::abs(x2 - x1 + 1)) * std::abs(y2 - y1 + 1); }
  float iou_of(const Box& other) const{
    float inner_x1 = x1 > other.x1 ? x1 : other.x1;
    float inner_y1 = y1 > other.y1 ? y1 : other.y1;
    float inner_x2 = x2 < other.x2 ? x2 : other.x2;
    float inner_y2 = y2 < other.y2 ? y2 : other.y2;
    float inner_h = inner_y2 - inner_y1 + 1.0f;
    float inner_w = inner_x2 - inner_x1 + 1.0f;
    float inner_area = inner_h * inner_w;
    return (inner_area / (area() + tbox.area() - inner_area));
  }
}
void hard_nms(std::vector<Box> &input, std::vector<Box> &output, float iou_threshold){
  if (input.empty()) return;
  std::sort(input.begin(), input.end(),[](Box& a, Box& b) { return a.score > b.score; });
  int box_num = input.size();
  std::vector<int> merged(box_num, 0);
  for (int i = 0; i < box_num; ++i) {
    if (merged[i]) continue;
    merged[i] = 1;
    for (int j = i + 1; j < box_num; ++j) {
      if (merged[j]) continue;
      float iou = input[i].iou_of(input[j]);
      if (iou > iou_threshold) merged[j] = 1;
    }
    output.push_back(input[i]);
  }
}
```
CVç›¸å…³çš„ç»å¸¸ä¼šè¦æ‰‹æ’•NMSï¼Œä¹Ÿè®°å½•ä¸‹ã€‚

## 0x0f æ€»ç»“  ([Â©ï¸backğŸ‘†ğŸ»](#kernellist))
å¯ä»¥å‘ç°ï¼Œå¤§éƒ¨åˆ†kernelçš„åŸºæœ¬å†™æ³•éƒ½æ˜¯ä¾èµ–warp reduceå’Œblock reduceçš„ï¼ŒåŸºæœ¬ä¸Šåªè¦ç†Ÿç»ƒåº”ç”¨warp functionså„ç§åœºæ™¯çš„å†™æ³•ï¼Œåº”è¯¥é—®é¢˜ä¸å¤§ï¼›softmaxéœ€è¦è€ƒè™‘ç½‘æ ¼çº§åŒæ­¥çš„é—®é¢˜ï¼Œæˆ–è€…online softmaxä»¥åŠFlashAttentionï¼›sgemmçš„ä¼˜åŒ–æ˜¯ä¸ªå¾ˆå¤§çš„è¯¾é¢˜ï¼Œä¸æ˜¯æ¡ˆä¾‹ä¸­å†™çš„è¿™ä¹ˆç®€å•ï¼Œä½†æ˜¯å…¥é—¨çš„è¯ï¼ŒåŸºæœ¬å°±æ˜¯tilingçš„æ€æƒ³ä»¥åŠå¦‚ä½•åšç´¢å¼•ä¹‹é—´çš„mappingï¼›sgemvçš„ä¼˜åŒ–åˆ™ä¸»è¦è€ƒè™‘Kä¸åŒçš„å€¼ï¼ˆå› ä¸ºMä¸º1äº†ï¼‰ï¼Œæ¯”å¦‚K=16,64,128ç­‰æƒ…å†µä¸‹ï¼Œå¦‚ä½•æŒ‰ç…§warpæ¥å¤„ç†ï¼›reluã€sigmoidç­‰éƒ½æ˜¯elementwiseçš„æ“ä½œï¼Œå¾ˆå¥½å®ç°ï¼Œå¯ä»¥å†è€ƒè™‘åŠ ç‚¹å‘é‡åŒ–ä¼˜åŒ–è®¿å­˜ï¼›layer normå’Œrms normåœ¨æ•°å­¦ä¸Šå…¶å®ä¹Ÿæ˜¯æŒºæ¸…æ™°ç®€å•çš„ï¼Œè½å®åˆ°cuda kernelæ—¶ï¼Œåªè¦æŒ‰ç…§é€ä¸ªtokenæ¥å¤„ç†ï¼Œheaddimæ²¡æœ‰è¶…è¿‡1024çš„æƒ…å†µä¸‹ï¼ˆä¸€ä¸ªblockæœ€å¤šå¯ä»¥æ”¾1024ä¸ªthreadsï¼‰ï¼Œå¯ä»¥æ”¾åˆ°ä¸€ä¸ªblockå¤„ç†ï¼Œè¿™æ ·å¹¶è¡ŒåŒ–å°±å¾ˆå¥½å†™ã€‚å½“ç„¶ï¼Œæ ¸å¿ƒè¿˜æ˜¯warp reduceå’Œblock reduceï¼›NMSæ˜¯ä¹±å…¥çš„ï¼Œæ²¡æœ‰CUDAç‰ˆæœ¬ï¼Œåˆ«é—®äº†...

## Â©ï¸License
GNU General Public License v3.0

## References  
- [flash-attention-minimal](https://github.com/tspeterkim/flash-attention-minimal): Flash Attention in ~100 lines of CUDA (forward pass only)

## ğŸ‰Contribute
ğŸŒŸå¦‚æœè§‰å¾—æœ‰ç”¨ï¼Œä¸å¦¨ç»™ä¸ªğŸŒŸğŸ‘†ğŸ»Staræ”¯æŒä¸€ä¸‹å§~

<div align='center'>
<a href="https://star-history.com/#DefTruth/Awesome-LLM-Inference&Date">
  <picture align='center'>
    <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=DefTruth/cuda-learn-note&type=Date&theme=dark" />
    <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=DefTruth/cuda-learn-note&type=Date" />
    <img width=450 height=300 alt="Star History Chart" src="https://api.star-history.com/svg?repos=DefTruth/cuda-learn-note&type=Date" />
  </picture>
</a>  
</div>

