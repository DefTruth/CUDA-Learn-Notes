# Notes ğŸ‘‡ğŸ‘‡

This project has been moved to [xlite-dev/LeetCUDA](https://github.com/xlite-dev/LeetCUDA). Please check [xlite-dev/LeetCUDA](https://github.com/xlite-dev/LeetCUDA) for latest updates! ğŸ‘ğŸ‘‹

---
<!--
<div align="center">
  <p align="center">
    <h2>ğŸ“š Modern CUDA Learn Notes with PyTorch for Beginners ğŸ‘</h2>
    <a href="#cuda-kernel">ğŸ“š200+ CUDA Kernels</a> | <a href="#my-blogs-part-1"> ğŸ“š100+ Blogs</a> | <a href="#hgemm-mma-bench"> âš¡ï¸HGEMM MMA</a> | <a href="#fa-mma-bench"> âš¡ï¸FA-2 MMA </a> <p>
  </p>
  <img src='https://github.com/user-attachments/assets/9306862b-2a30-4a87-bb33-0fde9e9d7cea' width=250 >
  <div align='center'>
      <img src=https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg >
      <img src=https://img.shields.io/badge/Language-CUDA-brightgreen.svg >
      <img src=https://img.shields.io/github/watchers/xlite-dev/CUDA-Learn-Notes?color=9cc >
      <img src=https://img.shields.io/github/forks/xlite-dev/CUDA-Learn-Notes.svg?style=social >
      <img src=https://img.shields.io/github/stars/xlite-dev/CUDA-Learn-Notes.svg?style=social >
      <img src=https://img.shields.io/badge/Release-v3.0.0-brightgreen.svg >
      <img src=https://img.shields.io/badge/License-GPLv3.0-turquoise.svg >
 </div>
</div>

ğŸ“š **Modern CUDA Learn Notes with PyTorch** for Beginners: It includes **Tensor/CUDA Cores, TF32/F16/BF16/F8**, [ğŸ“–200+ CUDA KernelsğŸ”¥ğŸ”¥(Easy -> Hard++)](#cuda-kernel) with PyTorch bindings, [ğŸ“–100+ LLM/VLM/CV/CUDA/CuTeğŸ”¥](#my-blogs-part-1) blogs, [ğŸ“–toy-hgemmâš¡ï¸âš¡ï¸](./kernels/hgemm) which can achieve `98%~100%` performance of **cuBLAS**, and [ğŸ“–flash-attention-mmaâš¡ï¸âš¡ï¸](./kernels/flash-attn) using Tensor Cores with pure MMA PTX. Welcome to ğŸŒŸğŸ‘†ğŸ»star this repo to support me, many thanks ~ ğŸ‰ğŸ‰

<div id="contents"></div>    

## ğŸ“– News ğŸ”¥ğŸ”¥
<div id="news"></div>  

- [2025-01-08]: [ğŸ“šSplit Q + Fully QKV Fine-grained Tiling](#mma-tiling-qkv) has been refactored into ğŸ¤–[ffpa-attn-mma](https://github.com/xlite-dev/ffpa-attn-mma.git): ğŸ“šFFPA - Yet another Faster Flash Prefill Attention with O(1)ğŸ‰SRAM complexity for headdim > 256, **1.8x~3x**ğŸ‰faster than SDPA EA: [ğŸ“ˆL20 ~1.9xâ†‘ğŸ‰](https://github.com/xlite-dev/ffpa-attn-mma?tab=readme-ov-file#L1-bench-l20), [ğŸ“ˆ A30 ~1.8xâ†‘ğŸ‰](https://github.com/xlite-dev/ffpa-attn-mma?tab=readme-ov-file#L1-bench-a30), [ğŸ“ˆ3080 ~2.9xâ†‘ğŸ‰](https://github.com/xlite-dev/ffpa-attn-mma?tab=readme-ov-file#L1-bench-3080), [ğŸ“ˆ4090 ~2.1xâ†‘ğŸ‰](https://github.com/xlite-dev/ffpa-attn-mma?tab=readme-ov-file#L1-bench-4090).  

<div align='center'>
  <img src='https://github.com/user-attachments/assets/cba2edce-ac0d-412e-823c-7eea2cc63f83' height="170px" width="270px">
  <img src='https://github.com/user-attachments/assets/447e2937-f7c8-47c8-8550-8c0c71b910e6' height="170px" width="270px">
  <img src='https://github.com/user-attachments/assets/65a8d564-8fa7-4d66-86b9-e238feb86143' height="170px" width="270px">
</div> 

- [2024-12-02]: HGEMM MMA kernels has been refactored into ğŸ¤–[hgemm-mma](https://github.com/xlite-dev/hgemm-mma.git): âš¡ï¸Write HGEMM from scratch using Tensor Cores with WMMA, MMA and CuTe API, achieve peakâš¡ï¸ performance.

<div align='center'>
  <img src='https://github.com/user-attachments/assets/71927ac9-72b3-4ce9-b0e2-788b5885bc99' height="170px" width="270px">
  <img src='https://github.com/user-attachments/assets/05ef4f5e-d999-48ea-b58e-782cffb24e85' height="170px" width="270px">
  <img src='https://github.com/user-attachments/assets/9472e970-c083-4b31-9252-3eeecc761078' height="170px" width="270px">
</div> 


## ğŸ“– HGEMM Benchmark ğŸ‰ğŸ‰

<div id="hgemm-mma-bench"></div>  

Currently, on NVIDIA L20, RTX 4090 and RTX 3080 Laptop, compared with cuBLAS's default Tensor Cores algorithm, the `HGEMM (WMMA/MMA/CuTe)` in this repo (`blue`ğŸ”µ) can achieve `98%~100%` of its (`orange`ğŸŸ ) performance. Please check [toy-hgemm libraryâš¡ï¸âš¡ï¸](./kernels/hgemm) or [hgemm-mmaâš¡ï¸âš¡ï¸](https://github.com/xlite-dev/hgemm-mma) repo for more details.

![toy-hgemm-library](https://github.com/user-attachments/assets/962bda14-b494-4423-b8eb-775da9f5503d)

|ğŸ“šFeature |ğŸ“šFeature |ğŸ“šFeature |ğŸ“šFeature|
|:---:|:---:|:---:|:---:|
|âœ”ï¸CUDA/**Tensor Cores**|âœ”ï¸Loop over K|âœ”ï¸Tile Block(BMxBK)|âœ”ï¸Tile Threads(T 8x8)|
|âœ”ï¸WMMA(m16n16k16)|âœ”ï¸MMA(m16n8k16)|âœ”ï¸Pack LDST(128 bits)|âœ”ï¸SMEM Padding|
|âœ”ï¸Copy Async|âœ”ï¸Tile MMAs|âœ”ï¸Tile Warps|âœ”ï¸**Multi Stages(2~4)**|  
|âœ”ï¸Register Double Buffers|âœ”ï¸**Block Swizzle**|âœ”ï¸**Warp Swizzle**|âœ”ï¸**SMEM Swizzle**(CuTe/MMA)|
|âœ”ï¸Collective Store(Shfl)|âœ”ï¸Layout NN|âœ”ï¸Layout TN|âœ”ï¸SGEMM FP32/TF32|

## ğŸ“– FA2-MMA Benchmark ğŸ‰ğŸ‰ 

<div id="fa-mma-bench"></div>  

I have also implemented **FlashAttention-2** using pure MMA PTX instructions, which supports features such as Multi-Stages, Tile MMA, Tile Warp, Shared KV SMEM, **Fully Shared QKV SMEM**, **Prefetch Q s2r**, **Prefetch K/V g2s**, **QKV Fine-grained Tiling**, Collective Store, etc. Please refer to [flash-attention-mmaâš¡ï¸âš¡ï¸](./kernels/flash-attn) for more details.

![flash-attn-mma](https://github.com/user-attachments/assets/6f66796d-44d5-4ec1-b224-af997bd152b2)

|ğŸ“šFeature |ğŸ“šFeature |ğŸ“šFeature |ğŸ“šFeature|
|:---:|:---:|:---:|:---:|
|âœ”ï¸Tensor Cores|âœ”ï¸Loop over N/D |âœ”ï¸Tile Block(Br, Bc)|âœ”ï¸MMA(m16n8k16)|
|âœ”ï¸Pack LDST(128 bits)|âœ”ï¸SMEM **Swizzle**/Padding |âœ”ï¸Copy Async|âœ”ï¸Tile MMAs|
|âœ”ï¸Tile Warps|âœ”ï¸Multi Stages(1/2)|âœ”ï¸Collective Store(Shfl)|âœ”ï¸**Split KV/Q**|
|âœ”ï¸**Shared QKV** SMEM|âœ”ï¸**Prefetch Q** s2r|âœ”ï¸**Prefetch KV** g2s|âœ”ï¸**QKV Fine-grained Tiling**|

Currently, for small-scale attention `(B<=4, H <=48, SeqLen <= 8192, D <= 64)` it can run faster than FA2/SDPA on some Devices. For example, on NVIDIA RTX 3080 Laptop, [ğŸ“š Split Q + Fully Shared QKV SMEM](#mma-share-qkv) method can achieve **55 TFLOPS (D=64)** that almost **~1.5x** ğŸ‰ faster than FA2. On NVIDIA L20, ğŸ¤–[ffpa-attn-mma](https://github.com/xlite-dev/ffpa-attn-mma) method can achieve **104 TFLOPS (D=512)** that almost **~1.8x** ğŸ‰ faster than SDPA (EFFICIENT ATTENTION). However, for large-scale attention, there remains a performance gap. Stay tuned for updates ~ (MMA Acc F16/F32, softmax Acc F32 vs FA2 MMA/softmax Acc F32, ğŸ‘‡Benchmark)

|Algorithm| (B,H,N,D) | RTX 3080 Laptop | L20 | RTX 4090 |   
|:---:|:---:|:---:|:---:|:---:|  
|FlashAttention-2|(1,8,8192,64)|37 TFLOPS|100 TFLOPS|145 TFLOPS|  
|share-qkv+stage2|(1,8,8192,64)|**55 TFLOPS**|99 TFLOPS|**221 TFLOPS**|  
|FlashAttention-2|(1,48,8192,64)|37 TFLOPS|109 TFLOPS|163 TFLOPS|
|share-qkv+stage2|(1,48,8192,64)|**48 TFLOPS**|107 TFLOPS|**224 TFLOPS**|
|SDPA(EFFICIENT ATTENTION)|(1,48,8192,512)|16 TFLOPS|58 TFLOPS|85 TFLOPS|
|ğŸ¤–[ffpa-attn-mma](https://github.com/xlite-dev/ffpa-attn-mma)|(1,48,8192,512)|**39 TFLOPS**|**104 TFLOPS**|**200 TFLOPS**|
|Precision Errors vs FA2/SDPA| / | max: < ~1e-3 | min: ~0.0 | mean: < ~1e-5 |

The `Split KV` and `Split Q` implementations have been carried out in [flash-attention-mmaâš¡ï¸âš¡ï¸](./kernels/flash-attn) for performance comparison. The `Split KV` method, which involves splitting all QKV across MMA (Warps), is slower than `Split Q` method, which splitting Q across MMA(Warps) and keep access KV for all MMA(Warps). 

- ğŸ“š Split KV (Basic, FlashAttention-1)
<div id="mma-split-kv"></div>  

```C++
// Split QKV across MMA(Warps) using naive matmul MMA&Warp tiling policy.
// case: The layout of 8 MMA(2x4)  [after] kWarpTileSeqLenQxkWarpTileSeqLenK(2x2) -> 32x2,32x2=64x64: 
// |  [64,64]  |    warp_KV 0    |    warp_KV 1    |    warp_KV 2    |    warp_KV 3    |
// | warp_QP 0 |-- MMA 0,MMA 0 --|-- MMA 2,MMA 2 --|-- MMA 4,MMA 4 --|-- MMA 6,MMA 6 --|
// | warp_QP 0 |-- MMA 0,MMA 0 --|-- MMA 2,MMA 2 --|-- MMA 4,MMA 4 --|-- MMA 6,MMA 6 --|
// | warp_QP 1 |-- MMA 1,MMA 1 --|-- MMA 3,MMA 2 --|-- MMA 5,MMA 5 --|-- MMA 7,MMA 7 --|
// | warp_QP 1 |-- MMA 1,MMA 1 --|-- MMA 3,MMA 2 --|-- MMA 5,MMA 5 --|-- MMA 7,MMA 7 --|
__global__ void // Q, K, V, O -> [B, H, N, D]
flash_attn_mma_stages_split_kv_kernel(half* Q, half* K, half* V, half* O, ...);
```

- ğŸ“š Split Q (Faster, FlashAttention-2)
<div id="mma-split-q"></div>  

```C++
// Split Q across MMA(Warps) and keep access KV for all MMA(Warps),
// in order to reduce the comm between warps via smem and warp shuffle.
// case: MMA = m16n8k16, Br=16x4=64, Bc=8x8=64, layout: 4 warps
// |   64x64   |      warp_KV 0       |
// | warp_QP 0 | MMA 0 ... MMA 0 (x8) |
// | warp_QP 1 | MMA 1 ... MMA 1 (x8) |
// | warp_QP 2 | MMA 2 ... MMA 2 (x8) |
// | warp_QP 3 | MMA 3 ... MMA 3 (x8) |
__global__ void // Q, K, V, O -> [B, H, N, D]
flash_attn_mma_stages_split_q_kernel(half* Q, half* K, half* V, half* O, ...);
```

- ğŸ“š Split Q + Shared KV SMEM (**1/2 SRAM** vs FA2)
<div id="mma-share-kv"></div>  

```C++
// K, V shared the same shared memory, improve block occupancy.
__global__ void // Q, K, V, O -> [B, H, N, D]
flash_attn_mma_stages_split_q_shared_kv_kernel(half* Q, half* K, half* V, half* O, ...);
```
- ğŸ“š Split Q + Fully Shared QKV SMEM (**1/4 SRAM** vs FA2)

<div id="mma-share-qkv"></div>  

```C++
// Q, K, V fully shared the same shared memory and prefetch Q s2r, improve block occupancy
// and reduce Q SMEM IO-Access.
__global__ void // Q, K, V, O -> [B, H, N, D]
flash_attn_mma_stages_split_q_shared_qkv_kernel(half* Q, half* K, half* V, half* O, ...);
```
- ğŸ“š Split Q + QK Fine-grained Tiling (**O(16xd) SRAM** vs FA2 **O(4xBrxd) SRAM**, `Headdim -> 1024`)

<div id="mma-tiling-qk"></div>  

```C++
// Fine-grained tiling at the MMA level for Q@K^T results in a constant SRAM usage of
// 64 * kMmaAtomK for Q and K. For V, the SRAM complexity is O(kMmaAtomK * d), leading to
// an overall SRAM complexity of O(kMmaAtomK * d). Consequently, this approach allows us to
// extend D (head dimension) up to 1024.
__global__ void // Q, K, V, O -> [B, H, N, D]
flash_attn_mma_stages_split_q_tiling_qk_kernel(half* Q, half* K, half* V, half* O, ...);
```

- ğŸ“š Split Q + Fully QKV Fine-grained Tiling (**O(2xBrx16)~O(1) SRAM** vs FA2 **O(4xBrxd) SRAM**)

<div id="mma-tiling-qkv"></div>  

```C++
// Fine-grained tiling at the MMA level for all Q@K^T and P@V results in a constant SRAM usage of
// Br * 16 or Bc * 16 for Q, K, V, leading to an overall SRAM complexity of O(Br * 16). Consequently,
// this approach allows us to run faster than SDPA w or w/o MMA Acc F32. 
__global__ void // Q, K, V, O -> [B, H, N, D]
flash_attn_mma_stages_split_q_tiling_qkv_kernel(half* Q, half* K, half* V, half* O, ...);
```
ğŸ’¡NOTE: [ğŸ“šSplit Q + Fully QKV Fine-grained Tiling](#mma-tiling-qkv) has been refactored into ğŸ¤–[ffpa-attn-mma](https://github.com/xlite-dev/ffpa-attn-mma).
 
## Â©ï¸CitationsğŸ‰ğŸ‰

```BibTeX
@misc{CUDA-Learn-Notes@2024,
  title={CUDA-Learn-Notes: A Modern CUDA Learn Notes with PyTorch for Beginners},
  url={https://github.com/xlite-dev/CUDA-Learn-Notes},
  note={Open-source software available at https://github.com/xlite-dev/CUDA-Learn-Notes},
  author={xlite-dev etc},
  year={2024}
}
```

## ğŸ“– 200+ CUDA Kernels ğŸ”¥ğŸ”¥ (Easy -> Hard++) ([Â©ï¸backğŸ‘†ğŸ»](#contents))  

<div id="cuda-kernel"></div>    

The kernels listed here will guide you through a step-by-step progression, ranging from easy to very challenging topics. The **workflow** for each topic will be as follows: custom **CUDA kernel** implementation -> PyTorch **Python bindings** -> Run tests. ğŸ‘‰TIPS: `*` = Tensor Cores (WMMA, MMA, CuTe), otherwise, CUDA Cores; `/` = not supported; `âœ”ï¸` = supported; `â”` = TODO. Contents are listed as follows:  

- [ğŸ“š Easy â­ï¸](#cuda-kernel-easy-medium)
- [ğŸ“š Medium â­ï¸â­ï¸](#cuda-kernel-easy-medium)
- [ğŸ“š Hard â­ï¸â­ï¸â­ï¸](#cuda-kernel-hard)
- [ğŸ“š Hard+ â­ï¸â­ï¸â­ï¸â­ï¸](#cuda-kernel-hard-plus)
- [ğŸ“š Hard++ â­â­â­ï¸â­ï¸â­ï¸](#cuda-kernel-hard-plus)

[ğŸ“š Easy](#cuda-kernel-easy-medium) and [ğŸ“š Medium](#cuda-kernel-easy-medium) sections cover operations such as `element-wise, mat_trans, warp/block reduce, nms, relu, gelu, swish, layer-norm, rms-norm, online-softmax, dot-prod, embedding` and basic usage for `FP32`, `FP16`, `BF16` and `FP8` . [ğŸ“š Hard](#cuda-kernel-hard), [ğŸ“š Hard+](#cuda-kernel-hard-plus) and [ğŸ“š Hard++](#cuda-kernel-hard-plus) sections delve deeper into advanced topics, primarily focusing on operations like `sgemv, sgemm, hgemv, hgemm and flash-attention`. These sections also provide numerous kernels implemented using Tensor Cores with pure MMA PTX.

### ğŸ“š Easy â­ï¸ & Medium â­ï¸â­ï¸  ([Â©ï¸backğŸ‘†ğŸ»](#cuda-kernel))  
<div id="cuda-kernel-easy-medium"></div>  

|ğŸ“– CUDA Kernel| ğŸ“– Elem DType| ğŸ“– Acc DType| ğŸ“– Docs | ğŸ“– Level |
|:---|:---|:---|:---|:---|  
| âœ”ï¸ [elementwise_f32](./kernels/elementwise/elementwise.cu)|f32|/|[link](./kernels/elementwise/)|â­ï¸|
| âœ”ï¸ [elementwise_f32x4](./kernels/elementwise/elementwise.cu)|f32|/|[link](./kernels/elementwise/)|â­ï¸|
| âœ”ï¸ [elementwise_f16](./kernels/elementwise/elementwise.cu)|f16|/|[link](./kernels/elementwise/)|â­ï¸|
| âœ”ï¸ [elementwise_f16x2](./kernels/elementwise/elementwise.cu)|f16|/|[link](./kernels/elementwise/)|â­ï¸|
| âœ”ï¸ [elementwise_f16x8](./kernels/elementwise/elementwise.cu)|f16|/|[link](./kernels/elementwise/)|â­ï¸|
| âœ”ï¸ [elementwise_f16x8_pack](./kernels/elementwise/elementwise.cu)|f16|/|[link](./kernels/elementwise/)|â­ï¸â­ï¸|
| âœ”ï¸ [histogram_i32](./kernels/histogram/histogram.cu)|i32|/|[link](./kernels/histogram/)|â­ï¸|
| âœ”ï¸ [histogram_i32x4](./kernels/histogram/histogram.cu)|i32|/|[link](./kernels/histogram/)|â­ï¸|  
| âœ”ï¸ [sigmoid_f32](./kernels/sigmoid/sigmoid.cu)|f32|/|[link](./kernels/sigmoid/)|â­ï¸|  
| âœ”ï¸ [sigmoid_f32x4](./kernels/sigmoid/sigmoid.cu)|f32|/|[link](./kernels/sigmoid/)|â­ï¸|  
| âœ”ï¸ [sigmoid_f16](./kernels/sigmoid/sigmoid.cu)|16|/|[link](./kernels/sigmoid/)|â­ï¸|  
| âœ”ï¸ [sigmoid_f16x2](./kernels/sigmoid/sigmoid.cu)|f16|/|[link](./kernels/sigmoid/)|â­ï¸|  
| âœ”ï¸ [sigmoid_f16x8](./kernels/sigmoid/sigmoid.cu)|f16|/|[link](./kernels/sigmoid/)|â­ï¸|  
| âœ”ï¸ [sigmoid_f16x8_pack](./kernels/sigmoid/sigmoid.cu)|f16|/|[link](./kernels/sigmoid/)|â­ï¸â­ï¸|  
| âœ”ï¸ [relu_f32](./kernels/relu/relu.cu)|f32|/|[link](./kernels/relu/)|â­ï¸|  
| âœ”ï¸ [relu_f32x4](./kernels/relu/relu.cu)|f32|/|[link](./kernels/relu/)|â­ï¸|  
| âœ”ï¸ [relu_f16](./kernels/relu/relu.cu)|f16|/|[link](./kernels/relu/)|â­ï¸|  
| âœ”ï¸ [relu_f16x2](./kernels/relu/relu.cu)|f16|/|[link](./kernels/relu/)|â­ï¸|  
| âœ”ï¸ [relu_f16x8](./kernels/relu/relu.cu)|f16|/|[link](./kernels/relu/)|â­ï¸|  
| âœ”ï¸ [relu_f16x8_pack](./kernels/relu/relu.cu)|f16|/|[link](./kernels/relu/)|â­ï¸â­ï¸| 
| âœ”ï¸ [elu_f32](./kernels/elu/elu.cu)|f32|/|[link](./kernels/elu/)|â­ï¸|  
| âœ”ï¸ [elu_f32x4](./kernels/elu/elu.cu)|f32|/|[link](./kernels/elu/)|â­ï¸|  
| âœ”ï¸ [elu_f16](./kernels/elu/elu.cu)|f16|/|[link](./kernels/elu/)|â­ï¸|  
| âœ”ï¸ [elu_f16x2](./kernels/elu/elu.cu)|f16|/|[link](./kernels/elu/)|â­ï¸|  
| âœ”ï¸ [elu_f16x8](./kernels/elu/elu.cu)|f16|/|[link](./kernels/elu/)|â­ï¸|  
| âœ”ï¸ [elu_f16x8_pack](./kernels/elu/elu.cu)|f16|/|[link](./kernels/elu/)|â­ï¸â­ï¸| 
| âœ”ï¸ [gelu_f32](./kernels/gelu/gelu.cu)|f32|/|[link](./kernels/gelu/)|â­ï¸|  
| âœ”ï¸ [gelu_f32x4](./kernels/gelu/gelu.cu)|f32|/|[link](./kernels/gelu/)|â­ï¸|  
| âœ”ï¸ [gelu_f16](./kernels/gelu/gelu.cu)|f16|/|[link](./kernels/gelu/)|â­ï¸|  
| âœ”ï¸ [gelu_f16x2](./kernels/gelu/gelu.cu)|f16|/|[link](./kernels/gelu/)|â­ï¸|  
| âœ”ï¸ [gelu_f16x8](./kernels/gelu/gelu.cu)|f16|/|[link](./kernels/gelu/)|â­ï¸|  
| âœ”ï¸ [gelu_f16x8_pack](./kernels/gelu/gelu.cu)|f16|/|[link](./kernels/gelu/)|â­ï¸â­ï¸|  
| âœ”ï¸ [swish_f32](./kernels/swish/swish.cu)|f32|/|[link](./kernels/swish/)|â­ï¸|  
| âœ”ï¸ [swish_f32x4](./kernels/swish/swish.cu)|f32|/|[link](./kernels/swish/)|â­ï¸|  
| âœ”ï¸ [swish_f16](./kernels/swish/swish.cu)|f16|/|[link](./kernels/swish/)|â­ï¸|  
| âœ”ï¸ [swish_f16x2](./kernels/swish/swish.cu)|f16|/|[link](./kernels/swish/)|â­ï¸|  
| âœ”ï¸ [swish_f16x8](./kernels/swish/swish.cu)|f16|/|[link](./kernels/swish/)|â­ï¸|  
| âœ”ï¸ [swish_f16x8_pack](./kernels/swish/swish.cu)|f16|/|[link](./kernels/swish/)|â­ï¸â­ï¸|
| âœ”ï¸ [hardswish_f32](./kernels/hardswish/hardswish.cu)|f32|/|[link](./kernels/hardswish/)|â­ï¸|  
| âœ”ï¸ [hardswish_f32x4](./kernels/hardswish/hardswish.cu)|f32|/|[link](./kernels/hardswish/)|â­ï¸|  
| âœ”ï¸ [hardswish_f16](./kernels/hardswish/hardswish.cu)|f16|/|[link](./kernels/hardswish/)|â­ï¸|  
| âœ”ï¸ [hardswish_f16x2](./kernels/hardswish/hardswish.cu)|f16|/|[link](./kernels/hardswish/)|â­ï¸|  
| âœ”ï¸ [hardswish_f16x8](./kernels/hardswish/hardswish.cu)|f16|/|[link](./kernels/hardswish/)|â­ï¸|  
| âœ”ï¸ [hardswish_f16x8_pack](./kernels/hardswish/hardswish.cu)|f16|/|[link](./kernels/hardswish/)|â­ï¸â­ï¸|
| âœ”ï¸ [hardshrink_f32](./kernels/hardshrink/hardshrink.cu)|f32|/|[link](./kernels/hardshrink/)|â­ï¸|  
| âœ”ï¸ [hardshrink_f32x4](./kernels/hardshrink/hardshrink.cu)|f32|/|[link](./kernels/hardshrink/)|â­ï¸|  
| âœ”ï¸ [hardshrink_f16](./kernels/hardshrink/hardshrink.cu)|f16|/|[link](./kernels/hardshrink/)|â­ï¸|  
| âœ”ï¸ [hardshrink_f16x2](./kernels/hardshrink/hardshrink.cu)|f16|/|[link](./kernels/hardshrink/)|â­ï¸|  
| âœ”ï¸ [hardshrink_f16x8](./kernels/hardshrink/hardshrink.cu)|f16|/|[link](./kernels/hardshrink/)|â­ï¸|  
| âœ”ï¸ [hardshrink_f16x8_pack](./kernels/hardshrink/hardshrink.cu)|f16|/|[link](./kernels/hardshrink/)|â­ï¸â­ï¸|
| âœ”ï¸ [embedding_f32](./kernels/embedding/embedding.cu)|f32|/|[link](./kernels/embedding/)|â­ï¸|  
| âœ”ï¸ [embedding_f32x4](./kernels/embedding/embedding.cu)|f32|/|[link](./kernels/embedding/)|â­ï¸|  
| âœ”ï¸ [embedding_f32x4_pack](./kernels/embedding/embedding.cu)|f32|/|[link](./kernels/embedding/)|â­ï¸|  
| âœ”ï¸ [embedding_f16](./kernels/embedding/embedding.cu)|f16|/|[link](./kernels/embedding/)|â­ï¸|  
| âœ”ï¸ [embedding_f16x2](./kernels/embedding/embedding.cu)|f16|/|[link](./kernels/embedding/)|â­ï¸|  
| âœ”ï¸ [embedding_f16x8](./kernels/embedding/embedding.cu)|f16|/|[link](./kernels/embedding/)|â­ï¸|  
| âœ”ï¸ [embedding_f16x8_pack](./kernels/embedding/embedding.cu)|f16|/|[link](./kernels/embedding/)|â­ï¸â­ï¸| 
| âœ”ï¸ [mat_trans_f32_col2row{2d}](./kernels/mat-transpose/mat_transpose.cu)|f32|/|[link](./kernels/mat-transpose/)|â­ï¸|  
| âœ”ï¸ [mat_trans_f32_row2col{2d}](./kernels/mat-transpose/mat_transpose.cu)|f32|/|[link](./kernels/mat-transpose/)|â­ï¸|  
| âœ”ï¸ [mat_trans_f32_diagonal2d](./kernels/mat-transpose/mat_transpose.cu)|f32|/|[link](./kernels/mat-transpose/)|â­ï¸â­ï¸|  
| âœ”ï¸ [mat_trans_f32x4_col2row{2d}](./kernels/mat-transpose/mat_transpose.cu)|f32|/|[link](./kernels/mat-transpose/)|â­ï¸â­ï¸|  
| âœ”ï¸ [mat_trans_f32x4_row2col{2d}](./kernels/mat-transpose/mat_transpose.cu)|f32|/|[link](./kernels/mat-transpose/)|â­ï¸â­ï¸|  
| âœ”ï¸ [warp_reduce_{all}](./kernels/reduce/block_all_reduce.cu)|all|all|[link](./kernels/reduce/)|â­ï¸â­ï¸|  
| âœ”ï¸ [block_all_reduce_f32_f32](./kernels/reduce/block_all_reduce.cu)|f32|f32|[link](./kernels/reduce/)|â­ï¸â­ï¸|  
| âœ”ï¸ [block_all_reduce_f32x4_f32](./kernels/reduce/block_all_reduce.cu)|f32|f32|[link](./kernels/reduce/)|â­ï¸â­ï¸|  
| âœ”ï¸ [block_all_reduce_f16_f16](./kernels/reduce/block_all_reduce.cu)|f16|f16|[link](./kernels/reduce/)|â­ï¸â­ï¸|  
| âœ”ï¸ [block_all_reduce_f16_f32](./kernels/reduce/block_all_reduce.cu)|f16|f32|[link](./kernels/reduce/)|â­ï¸â­ï¸|  
| âœ”ï¸ [block_all_reduce_f16x2_f16](./kernels/reduce/block_all_reduce.cu)|f16|f16|[link](./kernels/reduce/)|â­ï¸â­ï¸|  
| âœ”ï¸ [block_all_reduce_f16x2_f32](./kernels/reduce/block_all_reduce.cu)|f16|f32|[link](./kernels/reduce/)|â­ï¸â­ï¸|  
| âœ”ï¸ [block_all_reduce_f16x8_pack_f16](./kernels/reduce/block_all_reduce.cu)|f16|f16|[link](./kernels/reduce/)|â­ï¸â­ï¸|  
| âœ”ï¸ [block_all_reduce_f16x8_pack_f32](./kernels/reduce/block_all_reduce.cu)|f16|f32|[link](./kernels/reduce/)|â­ï¸â­ï¸|  
| âœ”ï¸ [block_all_reduce_bf16_bf16](./kernels/reduce/block_all_reduce.cu)|bf16|bf16|[link](./kernels/reduce/)|â­ï¸â­ï¸|  
| âœ”ï¸ [block_all_reduce_bf16_f32](./kernels/reduce/block_all_reduce.cu)|bf16|f32|[link](./kernels/reduce/)|â­ï¸â­ï¸|  
| âœ”ï¸ [block_all_reduce_bf16x2_bf16](./kernels/reduce/block_all_reduce.cu)|bf16|bf16|[link](./kernels/reduce/)|â­ï¸â­ï¸|  
| âœ”ï¸ [block_all_reduce_bf16x2_f32](./kernels/reduce/block_all_reduce.cu)|bf16|f32|[link](./kernels/reduce/)|â­ï¸â­ï¸|  
| âœ”ï¸ [block_all_reduce_bf16x8_pack_bf16](./kernels/reduce/block_all_reduce.cu)|bf16|bf16|[link](./kernels/reduce/)|â­ï¸â­ï¸|  
| âœ”ï¸ [block_all_reduce_bf16x8_pack_f32](./kernels/reduce/block_all_reduce.cu)|bf16|f32|[link](./kernels/reduce/)|â­ï¸â­ï¸|  
| âœ”ï¸ [block_all_reduce_fp8_e4m3_f16](./kernels/reduce/block_all_reduce.cu)|fp8_e4m3|f16|[link](./kernels/reduce/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [block_all_reduce_fp8_e5m2_f16](./kernels/reduce/block_all_reduce.cu)|fp8_e5m2|f16|[link](./kernels/reduce/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [block_all_reduce_fp8_e4m3x16_pack_f16](./kernels/reduce/block_all_reduce.cu)|fp8_e4m3|f16|[link](./kernels/reduce/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [block_all_reduce_fp8_e5m2x16_pack_f16](./kernels/reduce/block_all_reduce.cu)|fp8_e5m2|f16|[link](./kernels/reduce/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [block_all_reduce_i8_i32](./kernels/reduce/block_all_reduce.cu)|i8|i32|[link](./kernels/reduce/)|â­ï¸â­ï¸|  
| âœ”ï¸ [block_all_reduce_i8x16_pack_i32](./kernels/reduce/block_all_reduce.cu)|i8|i32|[link](./kernels/reduce/)|â­ï¸â­ï¸|  
| âœ”ï¸ [dot_product_f32](./kernels/dot-product/dot_product.cu)|f32|f32|[link](./kernels/dot-product/)|â­ï¸â­ï¸|  
| âœ”ï¸ [dot_product_f32x4](./kernels/dot-product/dot_product.cu)|f32|f32|[link](./kernels/dot-product/)|â­ï¸â­ï¸|  
| âœ”ï¸ [dot_product_f16_f32](./kernels/dot-product/dot_product.cu)|f16|f32|[link](./kernels/dot-product/)|â­ï¸â­ï¸|  
| âœ”ï¸ [dot_product_f16x2_f32](./kernels/dot-product/dot_product.cu)|f16|f32|[link](./kernels/dot-product/)|â­ï¸â­ï¸|  
| âœ”ï¸ [dot_product_f16x8_pack_f32](./kernels/dot-product/dot_product.cu)|f16|f32|[link](./kernels/dot-product/)|â­ï¸â­ï¸|  
| âœ”ï¸ [softmax_f32(fence)](./kernels/softmax/softmax.cu)|f32|f32|[link](./kernels/softmax/)|â­ï¸â­ï¸|  
| âœ”ï¸ [softmax_f32x4(fence)](./kernels/softmax/softmax.cu)|f32|f32|[link](./kernels/softmax/)|â­ï¸â­ï¸|  
| âœ”ï¸ [softmax_f32](./kernels/softmax/softmax.cu)|f32|f32|[link](./kernels/softmax/)|â­ï¸â­ï¸|  
| âœ”ï¸ [softmax_f32x4](./kernels/softmax/softmax.cu)|f32|f32|[link](./kernels/softmax/)|â­ï¸â­ï¸|  
| âœ”ï¸ [safe_softmax_f32](./kernels/softmax/softmax.cu)|f32|f32|[link](./kernels/softmax/)|â­ï¸â­ï¸|  
| âœ”ï¸ [safe_softmax_f32x4](./kernels/softmax/softmax.cu)|f32|f32|[link](./kernels/softmax/)|â­ï¸â­ï¸|  
| âœ”ï¸ [safe_softmax_f16_f32](./kernels/softmax/softmax.cu)|f16|f32|[link](./kernels/softmax/)|â­ï¸â­ï¸|  
| âœ”ï¸ [safe_softmax_f16x2_f32](./kernels/softmax/softmax.cu)|f16|f32|[link](./kernels/softmax/)|â­ï¸â­ï¸|  
| âœ”ï¸ [safe_softmax_f16x8_pack_f32](./kernels/softmax/softmax.cu)|f16|f32|[link](./kernels/softmax/)|â­ï¸â­ï¸|  
| âœ”ï¸ [online_safe_softmax_f32](./kernels/softmax/softmax.cu)|f32|f32|[link](./kernels/softmax/)|â­ï¸â­ï¸|
| âœ”ï¸ [online_safe_softmax_f32x4_pack](./kernels/softmax/softmax.cu)|f32|f32|[link](./kernels/softmax/)|â­ï¸â­ï¸|
| âœ”ï¸ [rope_f32](./kernels/rope/rope.cu)|f32|f32|[link](./kernels/rope/)|â­ï¸â­ï¸|  
| âœ”ï¸ [rope_f32x4_pack](./kernels/rope/rope.cu)|f32|f32|[link](./kernels/rope/)|â­ï¸â­ï¸|  
| âœ”ï¸ [layer_norm_f32](./kernels/layer-norm/layer_norm.cu)|f32|f32|[link](./kernels/layer-norm/)|â­ï¸â­ï¸|  
| âœ”ï¸ [layer_norm_f32x4](./kernels/layer-norm/layer_norm.cu)|f32|f32|[link](./kernels/layer-norm/)|â­ï¸â­ï¸|  
| âœ”ï¸ [layer_norm_f16_f16](./kernels/layer-norm/layer_norm.cu)|f16|f16|[link](./kernels/layer-norm/)|â­ï¸â­ï¸|  
| âœ”ï¸ [layer_norm_f16x2_f16](./kernels/layer-norm/layer_norm.cu)|f16|f16|[link](./kernels/layer-norm/)|â­ï¸â­ï¸|  
| âœ”ï¸ [layer_norm_f16x8_f16](./kernels/layer-norm/layer_norm.cu)|f16|f16|[link](./kernels/layer-norm/)|â­ï¸â­ï¸|  
| âœ”ï¸ [layer_norm_f16x8_pack_f16](./kernels/layer-norm/layer_norm.cu)|f16|f16|[link](./kernels/layer-norm/)|â­ï¸â­ï¸|  
| âœ”ï¸ [layer_norm_f16x8_pack_f32](./kernels/layer-norm/layer_norm.cu)|f16|f32|[link](./kernels/layer-norm/)|â­ï¸â­ï¸|  
| âœ”ï¸ [layer_norm_f16_f32](./kernels/layer-norm/layer_norm.cu)|f16|f32|[link](./kernels/layer-norm/)|â­ï¸â­ï¸|  
| âœ”ï¸ [rms_norm_f32](./kernels/rms-norm/rms_norm.cu)|f32|f32|[link](./kernels/rms-norm/)|â­ï¸â­ï¸|  
| âœ”ï¸ [rms_norm_f32x4](./kernels/rms-norm/rms_norm.cu)|f32|f32|[link](./kernels/rms-norm/)|â­ï¸â­ï¸|  
| âœ”ï¸ [rms_norm_f16_f16](./kernels/rms-norm/rms_norm.cu)|f16|f16|[link](./kernels/rms-norm/)|â­ï¸â­ï¸|  
| âœ”ï¸ [rms_norm_f16x2_f16](./kernels/rms-norm/rms_norm.cu)|f16|f16|[link](./kernels/rms-norm/)|â­ï¸â­ï¸|  
| âœ”ï¸ [rms_norm_f16x8_f16](./kernels/rms-norm/rms_norm.cu)|f16|f16|[link](./kernels/rms-norm/)|â­ï¸â­ï¸|  
| âœ”ï¸ [rms_norm_f16x8_f32](./kernels/rms-norm/rms_norm.cu)|f16|f32|[link](./kernels/rms-norm/)|â­ï¸â­ï¸|  
| âœ”ï¸ [rms_norm_f16x8_pack_f16](./kernels/rms-norm/rms_norm.cu)|f16|f16|[link](./kernels/rms-norm/)|â­ï¸â­ï¸|  
| âœ”ï¸ [rms_norm_f16x8_pack_f32](./kernels/rms-norm/rms_norm.cu)|f16|f32|[link](./kernels/rms-norm/)|â­ï¸â­ï¸|  
| âœ”ï¸ [rms_norm_f16_f32](./kernels/rms-norm/rms_norm.cu)|f16|f32|[link](./kernels/rms-norm/)|â­ï¸â­ï¸| 
| âœ”ï¸ [nms_f32](./kernels/nms/nms.cu)|f32|/|[link](./kernels/nms)|â­ï¸â­ï¸|  
| âœ”ï¸ [notes v1(deprecated)](./kernels/notes-v1.cu)|f32|f32|/|â­ï¸â­ï¸|  
| âœ”ï¸ [How to use nsys/ncu(timeline/ptx/sass)](./kernels/nvidia-nsight/)|/|/|[link](./kernels/nvidia-nsight/)|â­ï¸â­ï¸| 

### ğŸ“š Hard â­â­â­ï¸ ([Â©ï¸backğŸ‘†ğŸ»](#cuda-kernel))  

<div id="cuda-kernel-hard"></div>  

|ğŸ“– CUDA Kernel| ğŸ“– Elem DType| ğŸ“– Acc DType| ğŸ“– Docs | ğŸ“– Level |
|:---|:---|:---|:---|:---|    
| âœ”ï¸ [sgemv_k32_f32](./kernels/sgemv/sgemv.cu)|f32|f32|[link](./kernels/sgemv/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [sgemv_k128_f32x4](./kernels/sgemv/sgemv.cu)|f32|f32|[link](./kernels/sgemv/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [sgemv_k16_f32](./kernels/sgemv/sgemv.cu)|f32|f32|[link](./kernels/sgemv/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [hgemv_k32_f16](./kernels/hgemv/hgemv.cu)|f16|f16|[link](./kernels/hgemv/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [hgemv_k128_f16x4](./kernels/hgemv/hgemv.cu)|f16|f16|[link](./kernels/hgemv/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [hgemv_k16_f16](./kernels/hgemv/hgemv.cu)|f16|f16|[link](./kernels/hgemv/)|â­ï¸â­ï¸â­ï¸| 
| âœ”ï¸ [sgemm_naive_f32](./kernels/sgemm/sgemm.cu)|f32|f32|[link](./kernels/sgemm/)|â­ï¸â­ï¸|  
| âœ”ï¸ [sgemm_sliced_k_f32](./kernels/sgemm/sgemm.cu)|f32|f32|[link](./kernels/sgemm/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [sgemm_t_8x8_sliced_k_f32x4](./kernels/sgemm/sgemm.cu)|f32|f32|[link](./kernels/sgemm/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [sgemm_t_8x8_sliced_k...bcf](./kernels/sgemm/sgemm.cu)|f32|f32|[link](./kernels/sgemm/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [sgemm_t_8x8_sliced_k...dbuf](./kernels/sgemm/sgemm.cu)|f32|f32|[link](./kernels/sgemm/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [sgemm_t_8x8_sliced_k16...dbuf](./kernels/sgemm/sgemm_async.cu)|f32|f32|[link](./kernels/sgemm/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [sgemm_t_8x8_sliced_k16...async](./kernels/sgemm/sgemm_async.cu)|f32|f32|[link](./kernels/sgemm/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [sgemm_wmma_m16n16k8...stages*](./kernels/sgemm/sgemm_wmma_tf32_stage.cu)|tf32|f32|[link](./kernels/sgemm/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [sgemm_wmma_m16n16k8...swizzle*](./kernels/sgemm/sgemm_wmma_tf32_stage.cu)|tf32|f32|[link](./kernels/sgemm/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [hgemm_naive_f16](./kernels/hgemm/naive/hgemm.cu)|f16|f16|[link](./kernels/hgemm/)|â­ï¸â­ï¸|  
| âœ”ï¸ [hgemm_sliced_k_f16](./kernels/hgemm/naive/hgemm.cu)|f16|f16|[link](./kernels/hgemm/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [hgemm_t_8x8_sliced_k_f16x4](./kernels/hgemm/hgemm.cu)|f16|f16|[link](./kernels/hgemm/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [hgemm_t_8x8_sliced_k_f16x4_pack](./kernels/hgemm/naive/hgemm.cu)|f16|f16|[link](./kernels/hgemm/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [hgemm_t_8x8_sliced_k_f16x8_pack](./kernels/hgemm/naive/hgemm.cu)|f16|f16|[link](./kernels/hgemm/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [hgemm_t_8x8_sliced_k...dbuf](./kernels/hgemm/naive/hgemm.cu)|f16|f16|[link](./kernels/hgemm/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [hgemm_t_8/16x8...k16/32...dbuf](./kernels/hgemm/naive/hgemm_async.cu)|f16|f16|[link](./kernels/hgemm/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [hgemm_t_8/16x8...k16/32...async](./kernels/hgemm/naive/hgemm_async.cu)|f16|f16|[link](./kernels/hgemm/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [hgemm_wmma_m16n16k16...naive*](./kernels/hgemm/wmma/hgemm_wmma.cu)|f16|f16|[link](./kernels/hgemm/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [hgemm_wmma_m16n16k16...mma4x2*](./kernels/hgemm/wmma/hgemm_wmma.cu)|f16|f16|[link](./kernels/hgemm/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [hgemm_wmma_m16n16k16...mma4x4*](./kernels/hgemm/wmma/hgemm_wmma.cu)|f16|f16|[link](./kernels/hgemm/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [hgemm_wmma_m16n16k16...dbuf*](./kernels/hgemm/wmma/hgemm_wmma.cu)|f16|f16|[link](./kernels/hgemm/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [hgemm_wmma_m32n8k16....dbuf*](./kernels/hgemm/wmma/hgemm_wmma.cu)|f16|f16|[link](./kernels/hgemm/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [hgemm_wmma_m16n16k16...stages*](./kernels/hgemm/wmma/hgemm_wmma_stage.cu)|f16|f16|[link](./kernels/hgemm/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [hgemm_wmma_m16n16k16...swizzle*](./kernels/hgemm/wmma/hgemm_wmma_stage.cu)|f16|f16|[link](./kernels/hgemm/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [hgemm_mma_m16n8k16...naive*](./kernels/hgemm/mma/basic/hgemm_mma.cu)|f16|f16|[link](./kernels/hgemm/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [hgemm_mma_m16n8k16...mma2x4*](./kernels/hgemm/mma/basic/hgemm_mma.cu)|f16|f16|[link](./kernels/hgemm/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [hgemm_mma_m16n8k16...stages*](./kernels/hgemm/mma/basic/hgemm_mma_stage.cu)|f16|f16|[link](./kernels/hgemm/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [hgemm_mma_m16n8k16...swizzle*](./kernels/hgemm/mma/basic/hgemm_mma_stage.cu)|f16|f16|[link](./kernels/hgemm/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [hgemm_mma_m16n8k16...swizzle{smem}*](./kernels/hgemm/mma/swizzle/hgemm_mma_stage_swizzle.cu)|f16|f16|[link](./kernels/hgemm/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [hgemm_mma_m16n8k16...swizzle{tn}{smem}*](./kernels/hgemm/mma/swizzle/hgemm_mma_stage_tn_swizzle_x4.cu)|f16|f16|[link](./kernels/hgemm/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [hgemm_mma_stages_swizzle{smem}...cute*](./kernels/hgemm/cutlass/hgemm_mma_stage_tn_cute.cu)|f16|f16|[link](./kernels/hgemm/)|â­ï¸â­ï¸â­ï¸|  
| âœ”ï¸ [hgemm_mma_cublas*](./kernels/hgemm/cublas/hgemm_cublas.cu)|f16|f16|[link](./kernels/hgemm/)|â­ï¸â­ï¸|   

### ğŸ“š Hard+ â­ï¸â­ï¸â­ï¸â­ï¸ & Hard++ â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸ ([Â©ï¸backğŸ‘†ğŸ»](#cuda-kernel)) 

- ğŸ“š FlashAttention-2 MMA (MMA Acc F32/F16, swizzle, QKV smem share, fine-grained tiling, etc.ğŸ‰)

<div id="cuda-kernel-hard-plus"></div>  

|ğŸ“– CUDA Kernel| ğŸ“– Elem DType| ğŸ“– Acc DType| ğŸ“– Docs | ğŸ“– Level |
|:---|:---|:---|:---|:---|   
| âœ”ï¸ [How to implement MMA smem swizzle*](./kernels/swizzle/mma_simple_swizzle.cu)|f16|f16|[link](./kernels/swizzle)|â­ï¸â­ï¸â­ï¸| 
| âœ”ï¸ [flash_attn_mma_stages_split_kv*](./kernels/flash-attn/mma/basic/flash_attn_mma_split_kv.cu)|f16|f16|[link](./kernels/flash-attn)|â­ï¸â­ï¸â­ï¸â­ï¸| 
| âœ”ï¸ [flash_attn_mma_stages_split_q*](./kernels/flash-attn/mma/basic/flash_attn_mma_split_q.cu)|f16|f16|[link](./kernels/flash-attn)|â­ï¸â­ï¸â­ï¸â­ï¸|   
| âœ”ï¸ [flash_attn_mma_stages...shared_kv*](./kernels/flash-attn/mma/basic/flash_attn_mma_share_kv.cu)|f16|f16|[link](./kernels/flash-attn)|â­ï¸â­ï¸â­ï¸â­ï¸|   
| âœ”ï¸ [flash_attn_mma_stages...shared_qkv*](./kernels/flash-attn/mma/basic/flash_attn_mma_share_qkv.cu)|f16|f16|[link](./kernels/flash-attn)|â­ï¸â­ï¸â­ï¸â­ï¸|   
| âœ”ï¸ [flash_attn_mma_stages...tiling_qk*](./kernels/flash-attn/mma/basic/flash_attn_mma_tiling_qk.cu)|f16|f16|[link](./kernels/flash-attn)|â­ï¸â­ï¸â­ï¸â­ï¸|     
| âœ”ï¸ [flash_attn_mma_stages...tiling_qkv*](./kernels/flash-attn/mma/basic/flash_attn_mma_tiling_qkv.cu)|f16|f16|[link](./kernels/flash-attn)|â­ï¸â­ï¸â­ï¸â­ï¸|   
| âœ”ï¸ [flash_attn_mma_stages...shared_kv{f32}*](./kernels/flash-attn/mma/basic/flash_attn_mma_share_kv_F32F16F16F32.cu)|f16|f32|[link](./kernels/flash-attn)|â­ï¸â­ï¸â­ï¸â­ï¸|   
| âœ”ï¸ [flash_attn_mma_stages...shared_qkv{f32}*](./kernels/flash-attn/mma/basic/flash_attn_mma_share_qkv_F32F16F16F32.cu)|f16|f32|[link](./kernels/flash-attn)|â­ï¸â­ï¸â­ï¸â­ï¸|   
| âœ”ï¸ [flash_attn_mma_stages...tiling_qk{f32}*](./kernels/flash-attn/mma/basic/flash_attn_mma_tiling_qk_F32F16F16F32.cu)|f16|f32|[link](./kernels/flash-attn)|â­ï¸â­ï¸â­ï¸â­ï¸|   
| âœ”ï¸ [flash_attn_mma_stages...tiling_qkv{f32}*](./kernels/flash-attn/mma/basic/flash_attn_mma_tiling_qkv_F32F16F16F32.cu)|f16|f32|[link](./kernels/flash-attn)|â­ï¸â­ï¸â­ï¸â­ï¸| 
| âœ”ï¸ [flash_attn_mma...shared_kv{f32}{rr}*](./kernels/flash-attn/mma/others/flash_attn_mma_share_kv_F32F16F16F32_rr.cu)|f16|f32|[link](./kernels/flash-attn)|â­ï¸â­ï¸â­ï¸â­ï¸|   
| âœ”ï¸ [flash_attn_mma...shared_qkv{f32}{rr}*](./kernels/flash-attn/mma/others/flash_attn_mma_share_qkv_F32F16F16F32_rr.cu)|f16|f32|[link](./kernels/flash-attn)|â­ï¸â­ï¸â­ï¸â­ï¸| 
| âœ”ï¸ [flash_attn_mma...shared_kv_swizzle{q}*](./kernels/flash-attn/mma/swizzle/flash_attn_mma_share_kv_swizzle_q.cu)|f16|f16|[link](./kernels/flash-attn)|â­ï¸â­ï¸â­ï¸â­ï¸|   
| âœ”ï¸ [flash_attn_mma...shared_kv_swizzle{qk}*](./kernels/flash-attn/mma/swizzle/flash_attn_mma_share_kv_swizzle_qk.cu)|f16|f16|[link](./kernels/flash-attn)|â­ï¸â­ï¸â­ï¸â­ï¸|   
| âœ”ï¸ [flash_attn_mma...shared_kv_swizzle{qkv}*](./kernels/flash-attn/mma/swizzle/flash_attn_mma_share_kv_swizzle_qkv.cu)|f16|f16|[link](./kernels/flash-attn)|â­ï¸â­ï¸â­ï¸â­ï¸|   
| âœ”ï¸ [flash_attn_mma...shared_qkv_swizzle{q}*](./kernels/flash-attn/mma/swizzle/flash_attn_mma_share_qkv_swizzle_q.cu)|f16|f16|[link](./kernels/flash-attn)|â­ï¸â­ï¸â­ï¸â­ï¸|   
| âœ”ï¸ [flash_attn_mma...shared_qkv_swizzle{qk}*](./kernels/flash-attn/mma/swizzle/flash_attn_mma_share_qkv_swizzle_qk.cu)|f16|f16|[link](./kernels/flash-attn)|â­ï¸â­ï¸â­ï¸â­ï¸|   
| âœ”ï¸ [flash_attn_mma...shared_qkv_swizzle{qkv}*](./kernels/flash-attn/mma/swizzle/flash_attn_mma_share_qkv_swizzle_qkv.cu)|f16|f16|[link](./kernels/flash-attn)|â­ï¸â­ï¸â­ï¸â­ï¸|
| âœ”ï¸ [flash_attn_mma...tiling_qk_swizzle{q}*](./kernels/flash-attn/mma/swizzle/flash_attn_mma_tiling_qk_swizzle_q.cu)|f16|f16|[link](./kernels/flash-attn)|â­ï¸â­ï¸â­ï¸â­ï¸|   
| âœ”ï¸ [flash_attn_mma...tiling_qk_swizzle{qk}*](./kernels/flash-attn/mma/swizzle/flash_attn_mma_tiling_qk_swizzle_qk.cu)|f16|f16|[link](./kernels/flash-attn)|â­ï¸â­ï¸â­ï¸â­ï¸|   
| âœ”ï¸ [flash_attn_mma...tiling_qk_swizzle{qkv}*](./kernels/flash-attn/mma/swizzle/flash_attn_mma_tiling_qk_swizzle_qkv.cu)|f16|f16|[link](./kernels/flash-attn)|â­ï¸â­ï¸â­ï¸â­ï¸|   
| âœ”ï¸ [flash_attn_mma...tiling_qkv_swizzle{q}*](./kernels/flash-attn/mma/swizzle/flash_attn_mma_tiling_qkv_swizzle_q.cu)|f16|f16|[link](./kernels/flash-attn)|â­ï¸â­ï¸â­ï¸â­ï¸|   
| âœ”ï¸ [flash_attn_mma...tiling_qkv_swizzle{qk}*](./kernels/flash-attn/mma/swizzle/flash_attn_mma_tiling_qkv_swizzle_qk.cu)|f16|f16|[link](./kernels/flash-attn)|â­ï¸â­ï¸â­ï¸â­ï¸|   
| âœ”ï¸ [flash_attn_mma...tiling_qkv_swizzle{qkv}*](./kernels/flash-attn/mma/swizzle/flash_attn_mma_tiling_qkv_swizzle_qkv.cu)|f16|f16|[link](./kernels/flash-attn)|â­ï¸â­ï¸â­ï¸â­ï¸| 
| âœ”ï¸ [flash_attn...tiling_qkv_swizzle{q}{f32}*](./kernels/flash-attn/mma/swizzle/flash_attn_mma_tiling_qkv_swizzle_q_F32F16F16F32.cu)|f16|f32|[link](./kernels/flash-attn)|â­ï¸â­ï¸â­ï¸â­ï¸|   
| âœ”ï¸ [flash_attn...tiling_qkv_swizzle{qk}{f32}*](./kernels/flash-attn/mma/swizzle/flash_attn_mma_tiling_qkv_swizzle_qk_F32F16F16F32.cu)|f16|f32|[link](./kernels/flash-attn)|â­ï¸â­ï¸â­ï¸â­ï¸|   
| âœ”ï¸ [flash_attn...tiling_qkv_swizzle{qkv}{f32}*](./kernels/flash-attn/mma/swizzle/flash_attn_mma_tiling_qkv_swizzle_qkv_F32F16F16F32.cu)|f16|f32|[link](./kernels/flash-attn)|â­ï¸â­ï¸â­ï¸â­ï¸| 

ğŸ’¡NOTE: **rr**: means reduce registers usage (for `d>128`); **f32**: means MMA accumulate with FP32 dtype, otherwise, FP16. softmax Acc dtype is always be FP32 for high precision; **swizzle**: now, only support smem swizzle for MMA.

- ğŸ“š FFPA Attention MMA (**1.8x~3x**ğŸ‰faster vs SDPA EA, D > 256, FA2 not supported)

|ğŸ“– CUDA Kernel| ğŸ“– Elem DType| ğŸ“– Acc DType| ğŸ“– Docs | ğŸ“– Level |
|:---|:---|:---|:---|:---|   
| âœ”ï¸ [ffpa_mma_stages_split_q_L1_F16F16F16](https://github.com/xlite-dev/ffpa-attn-mma/blob/main/csrc/cuffpa/ffpa_attn_F16F16F16_L1.cu)|f16|f16|[link](https://github.com/xlite-dev/ffpa-attn-mma)|â­ï¸â­ï¸â­ï¸â­ï¸| 
| âœ”ï¸ [ffpa_mma_stages_split_q_L1_F16F16F32](https://github.com/xlite-dev/ffpa-attn-mma/blob/main/csrc/cuffpa/ffpa_attn_F16F16F32_L1.cu)|f16|f32|[link](https://github.com/xlite-dev/ffpa-attn-mma)|â­ï¸â­ï¸â­ï¸â­ï¸| 
| âœ”ï¸ [ffpa_mma_stages_split_q_L1_mixed_acc](https://github.com/xlite-dev/ffpa-attn-mma/blob/main/csrc/cuffpa/ffpa_attn_F16F16F32_L1.cu)|f16|QK f32, PV f16|[link](https://github.com/xlite-dev/ffpa-attn-mma)|â­ï¸â­ï¸â­ï¸â­ï¸| 
| âš ï¸ [ffpa_mma_stages_split_q_L2_F16F16F16](https://github.com/xlite-dev/ffpa-attn-mma/blob/main/csrc/cuffpa/ffpa_attn_F16F16F16_L2.cu)|f16|f16|[link](https://github.com/xlite-dev/ffpa-attn-mma)|â­ï¸â­ï¸â­ï¸â­ï¸| 
| âš ï¸ [ffpa_mma_stages_split_q_L2_F16F16F32](https://github.com/xlite-dev/ffpa-attn-mma/blob/main/csrc/cuffpa/ffpa_attn_F16F16F32_L2.cu)|f16|f32|[link](https://github.com/xlite-dev/ffpa-attn-mma)|â­ï¸â­ï¸â­ï¸â­ï¸| 
| âš ï¸ [ffpa_mma_stages_split_q_L2_mixed_acc](https://github.com/xlite-dev/ffpa-attn-mma/blob/main/csrc/cuffpa/ffpa_attn_F16F16F32_L2.cu)|f16|QK f32, PV f16|[link](https://github.com/xlite-dev/ffpa-attn-mma)|â­ï¸â­ï¸â­ï¸â­ï¸| 
| âš ï¸ [ffpa_mma_stages_split_q_L3_F16F16F16](https://github.com/xlite-dev/ffpa-attn-mma/blob/main/csrc/cuffpa/ffpa_attn_F16F16F16_L3.cu)|f16|f16|[link](https://github.com/xlite-dev/ffpa-attn-mma)|â­ï¸â­ï¸â­ï¸â­ï¸| 
| âš ï¸ [ffpa_mma_stages_split_q_L3_F16F16F32](https://github.com/xlite-dev/ffpa-attn-mma/blob/main/csrc/cuffpa/ffpa_attn_F16F16F32_L3.cu)|f16|f32|[link](https://github.com/xlite-dev/ffpa-attn-mma)|â­ï¸â­ï¸â­ï¸â­ï¸| 
| âš ï¸ [ffpa_mma_stages_split_q_L3_mixed_acc](https://github.com/xlite-dev/ffpa-attn-mma/blob/main/csrc/cuffpa/ffpa_attn_F16F16F32_L3.cu)|f16|QK f32, PV f16|[link](https://github.com/xlite-dev/ffpa-attn-mma)|â­ï¸â­ï¸â­ï¸â­ï¸| 

ğŸ’¡NOTE: ğŸ¤–[ffpa-attn-mma](https://github.com/xlite-dev/ffpa-attn-mma): ğŸ“šFFPA - Yet another Faster Flash Prefill Attention with O(1)ğŸ‰SRAM complexity for headdim > 256, **1.8x~3x**ğŸ‰faster than SDPA EA: [ğŸ“ˆL20 ~1.9xâ†‘ğŸ‰](https://github.com/xlite-dev/ffpa-attn-mma?tab=readme-ov-file#L1-bench-l20), [ğŸ“ˆ A30 ~1.8xâ†‘ğŸ‰](https://github.com/xlite-dev/ffpa-attn-mma?tab=readme-ov-file#L1-bench-a30), [ğŸ“ˆ3080 ~2.9xâ†‘ğŸ‰](https://github.com/xlite-dev/ffpa-attn-mma?tab=readme-ov-file#L1-bench-3080), [ğŸ“ˆ4090 ~2.1xâ†‘ğŸ‰](https://github.com/xlite-dev/ffpa-attn-mma?tab=readme-ov-file#L1-bench-4090).  

## ğŸ“– 100+ LLM/VLM/CV/CUDA/CuTe Tech Blogs

<div id="my-blogs-part-1"></div>  

### ğŸ“š å¤§æ¨¡å‹|å¤šæ¨¡æ€|Diffusion|æ¨ç†ä¼˜åŒ– (æœ¬äººä½œè€…) ([Â©ï¸backğŸ‘†ğŸ»](#contents))

|ğŸ“– ç±»å‹-æ ‡é¢˜|ğŸ“– ä½œè€…| ğŸ“– æ¨è |  
|:---|:---|:---|    
|[[vLLMå®è·µ]ğŸ“švLLM + DeepSeek-R1 671B å¤šæœºéƒ¨ç½²åŠä¿®Bugç¬”è®°](https://zhuanlan.zhihu.com/p/29950052712)|@xlite-dev|â­ï¸â­ï¸â­â­ï¸| 
|[[Attentionä¼˜åŒ–]ğŸ“šFFPA(Split-D): FA2æ— é™HeadDimæ‰©å±•ï¼Œ2xâ†‘ğŸ‰ vs SDPA EA](https://zhuanlan.zhihu.com/p/13975660308)|@xlite-dev|â­ï¸â­ï¸â­â­ï¸| 
|[[CUDAåŸºç¡€][å¼€ç¯‡]ğŸ“–CUDA-Learn-Notes: v3.0 å¤§å‡çº§-é¢è¯•åˆ·é¢˜ä¸è¿·è·¯](https://zhuanlan.zhihu.com/p/19862356369)|@xlite-dev|â­ï¸â­ï¸â­â­ï¸| 
|[[åˆ†å¸ƒå¼è®­æ¨][å¼ é‡/åºåˆ—å¹¶è¡Œ]ğŸ“–å›¾è§£DeepSpeed-Ulysses&Megatron-LM TP/SP](https://zhuanlan.zhihu.com/p/5750410146)|@xlite-dev|â­ï¸â­ï¸| 
|[[VLMæ¨ç†ä¼˜åŒ–][InternVLç³»åˆ—]ğŸ“–InternLM2/.../InternVL1.5ç³»åˆ—ç¬”è®°: æ ¸å¿ƒç‚¹è§£æ](https://zhuanlan.zhihu.com/p/702481058)|@xlite-dev|â­ï¸â­ï¸| 
|[[LLMæ¨ç†ä¼˜åŒ–][TensorRT-LLM][5wå­—]ğŸ“–TensorRT-LLMéƒ¨ç½²è°ƒä¼˜-æŒ‡åŒ—](https://zhuanlan.zhihu.com/p/699333691)|@xlite-dev|â­ï¸â­ï¸â­ï¸| 
|[[LLMæ¨ç†ä¼˜åŒ–][KV Cacheä¼˜åŒ–]ğŸ“–GQA/YOCO/CLA/MLKV: å±‚å†…å’Œå±‚é—´KV Cacheå…±äº«](https://zhuanlan.zhihu.com/p/697311739)|@xlite-dev|â­ï¸â­ï¸| 
|[[LLMæ¨ç†ä¼˜åŒ–][Prefillä¼˜åŒ–]ğŸ“–å›¾è§£vLLM Prefix Prefill Triton Kernel](https://zhuanlan.zhihu.com/p/695799736)|@xlite-dev|â­ï¸â­ï¸â­ï¸| 
|[[LLMæ¨ç†ä¼˜åŒ–][Prefillä¼˜åŒ–][ä¸‡å­—]ğŸ“–å›¾è§£vLLM Automatic Prefix Caching: TTFTä¼˜åŒ–](https://zhuanlan.zhihu.com/p/693556044)|@xlite-dev|â­ï¸â­ï¸â­ï¸| 
|[[LLMæ¨ç†ä¼˜åŒ–][Attentionä¼˜åŒ–]ğŸ“–å›¾è§£:ä»Online-Softmaxåˆ°FlashAttention V1/V2/V3](https://zhuanlan.zhihu.com/p/668888063)|@xlite-dev|â­ï¸â­ï¸â­ï¸| 
|[[LLMæ¨ç†ä¼˜åŒ–][Decodingä¼˜åŒ–]ğŸ“–åŸç†&å›¾è§£FlashDecoding/FlashDecoding++](https://zhuanlan.zhihu.com/p/696075602)|@xlite-dev|â­ï¸â­ï¸| 
|[[VLMæ¨ç†ä¼˜åŒ–][LLaVAç³»åˆ—]ğŸ“–CLIP/LLaVA/LLaVA1.5/VILAç¬”è®°: æ ¸å¿ƒç‚¹è§£æ](https://zhuanlan.zhihu.com/p/683137074)|@xlite-dev|â­ï¸â­ï¸| 
|[[LLMæ¨ç†ä¼˜åŒ–][Attentionä¼˜åŒ–][ä¸‡å­—]ğŸ“–TensorRT MHA/Myelin vs FlashAttention-2](https://zhuanlan.zhihu.com/p/678873216)|@xlite-dev|â­ï¸â­ï¸â­ï¸| 
|[[LLMæ¨ç†ä¼˜åŒ–][PTXæ±‡ç¼–]ğŸ“–CUDA 12 PTXæ±‡ç¼–: PRMTæŒ‡ä»¤è¯¦è§£-é€šç”¨æ¨¡å¼](https://zhuanlan.zhihu.com/p/660630414)|@xlite-dev|â­ï¸| 
|[[LLMæ¨ç†ä¼˜åŒ–][PTXæ±‡ç¼–]ğŸ“–CUDA 12 PTXæ±‡ç¼–: LOP3æŒ‡ä»¤è¯¦è§£](https://zhuanlan.zhihu.com/p/659741469)|@xlite-dev|â­ï¸| 
|[[LLMæ¨ç†ä¼˜åŒ–][CUDA][3wå­—]ğŸ“–é«˜é¢‘é¢è¯•é¢˜æ±‡æ€»-å¤§æ¨¡å‹æ‰‹æ’•CUDA](https://zhuanlan.zhihu.com/p/678903537)|@xlite-dev|â­ï¸â­ï¸â­ï¸| 
|[[LLMæ¨ç†ä¼˜åŒ–][Weight Only]ğŸ“–WINT8/4-(00): é€šä¿—æ˜“æ‡‚è®²è§£-å¿«é€Ÿåé‡åŒ–ç®—æ³•](https://zhuanlan.zhihu.com/p/657072856)|@xlite-dev|â­ï¸â­ï¸| 
|[[LLMæ¨ç†ä¼˜åŒ–][Weight Only]ğŸ“–WINT8/4-(01): PRMTæŒ‡ä»¤è¯¦è§£åŠFTæºç è§£æ](https://zhuanlan.zhihu.com/p/657070837)|@xlite-dev|â­ï¸â­ï¸| 
|[[LLMæ¨ç†ä¼˜åŒ–][Weight Only]ğŸ“–WINT8/4-(02): å¿«é€Ÿåé‡åŒ–ä¹‹INT8è½¬BF16](https://zhuanlan.zhihu.com/p/657073159)|@xlite-dev|â­ï¸â­ï¸| 
|[[LLMæ¨ç†ä¼˜åŒ–][Weight Only]ğŸ“–WINT8/4-(03): LOP3æŒ‡ä»¤è¯¦è§£åŠINT4è½¬FP16/BF16](https://zhuanlan.zhihu.com/p/657073857)|@xlite-dev|â­ï¸â­ï¸| 
|[[LLMæ¨ç†ä¼˜åŒ–][LLM Infraæ•´ç†]ğŸ“–100+ç¯‡: å¤§æ¨¡å‹æ¨ç†å„æ–¹å‘æ–°å‘å±•æ•´ç†](https://zhuanlan.zhihu.com/p/693680304)|@xlite-dev|â­ï¸â­ï¸| 
|[[LLMæ¨ç†ä¼˜åŒ–][LLM Infraæ•´ç†]ğŸ“–30+ç¯‡: LLMæ¨ç†è®ºæ–‡é›†-500é¡µPDF](https://zhuanlan.zhihu.com/p/669777159)|@xlite-dev|â­ï¸â­ï¸| 
|[[LLMæ¨ç†ä¼˜åŒ–][LLM Infraæ•´ç†]ğŸ“–FlashDecoding++: æ¯”FlashDecodingè¿˜è¦å¿«ï¼](https://zhuanlan.zhihu.com/p/665022589)|@xlite-dev|â­ï¸| 
|[[LLMæ¨ç†ä¼˜åŒ–][LLM Infraæ•´ç†]ğŸ“–TensorRT-LLMå¼€æºï¼ŒTensorRT 9.1ä¹Ÿæ¥äº†](https://zhuanlan.zhihu.com/p/662361469)|@xlite-dev|â­ï¸| 
|[[LLMæ¨ç†ä¼˜åŒ–][LLM Infraæ•´ç†]ğŸ“–20+ç¯‡: LLMæ¨ç†è®ºæ–‡é›†-300é¡µPDF](https://zhuanlan.zhihu.com/p/658091768)|@xlite-dev|â­ï¸â­ï¸| 
|[[LLMæ¨ç†ä¼˜åŒ–][LLM Infraæ•´ç†]ğŸ“–PagedAttentionè®ºæ–‡æ–°é²œå‡ºç‚‰](https://zhuanlan.zhihu.com/p/617015570)|@xlite-dev|â­ï¸| 


### ğŸ“š CVæ¨ç†éƒ¨ç½²|C++|ç®—æ³•|æŠ€æœ¯éšç¬” (æœ¬äººä½œè€…) ([Â©ï¸backğŸ‘†ğŸ»](#contents))

<div id="my-blogs-part-2"></div>  

|ğŸ“– ç±»å‹-æ ‡é¢˜|ğŸ“– ä½œè€…| ğŸ“– æ¨è |  
|:---|:---|:---|   
| [[æ¨ç†éƒ¨ç½²][CV/NLP]ğŸ“–FastDeployä¸‰è¡Œä»£ç æå®š150+ CVã€NLPæ¨¡å‹éƒ¨ç½²](https://zhuanlan.zhihu.com/p/581326442)|@xlite-dev|â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][CV]ğŸ“–å¦‚ä½•åœ¨lite.ai.toolkit(3.6k+ stars)ä¸­å¢åŠ æ‚¨çš„æ¨¡å‹ï¼Ÿ](https://zhuanlan.zhihu.com/p/523876625)|@xlite-dev|â­ï¸â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][CV]ğŸ“–ç¾å›¢ YOLOv6 ORT/MNN/TNN/NCNN C++æ¨ç†éƒ¨ç½²](https://zhuanlan.zhihu.com/p/533643238)|@xlite-dev|â­ï¸â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][ONNX]ğŸ“–ONNXæ¨ç†åŠ é€ŸæŠ€æœ¯æ–‡æ¡£-æ‚è®°](https://zhuanlan.zhihu.com/p/524023964)|@xlite-dev|â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][TensorFlow]ğŸ“–Macæºç ç¼–è¯‘TensorFlow C++æŒ‡åŒ—](https://zhuanlan.zhihu.com/p/524013615)|@xlite-dev|â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][CV]ğŸ“–1Mb!å¤´éƒ¨å§¿æ€ä¼°è®¡: FSANetï¼Œä¸€ä¸ªå°è€Œç¾çš„æ¨¡å‹(C++)](https://zhuanlan.zhihu.com/p/447364201)|@xlite-dev|â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][CV]ğŸ“–opencv+ffmpegç¼–è¯‘æ‰“åŒ…å…¨è§£æŒ‡å—](https://zhuanlan.zhihu.com/p/472115312)|@xlite-dev|â­ï¸â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][CV]ğŸ“–RobustVideoMattingè§†é¢‘æŠ å›¾é™æ€ONNXæ¨¡å‹è½¬æ¢](https://zhuanlan.zhihu.com/p/459088407)|@xlite-dev|â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][CV]ğŸ“–190Kb!SSRNetå¹´é¾„æ£€æµ‹è¯¦ç»†è§£è¯»ï¼ˆå«C++å·¥ç¨‹ï¼‰](https://zhuanlan.zhihu.com/p/462762797)|@xlite-dev|â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][CV]ğŸ“–MGMatting(CVPR2021)äººåƒæŠ å›¾C++åº”ç”¨è®°å½•](https://zhuanlan.zhihu.com/p/464732042)|@xlite-dev|â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][CV]ğŸ“–è¶…å‡†ç¡®äººè„¸æ£€æµ‹(å¸¦å…³é”®ç‚¹)YOLO5Face C++å·¥ç¨‹è¯¦ç»†è®°å½•](https://zhuanlan.zhihu.com/p/461878005)|@xlite-dev|â­ï¸â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][ORT]ğŸ“–è§£å†³: ONNXRuntime(Python) GPU éƒ¨ç½²é…ç½®è®°å½•](https://zhuanlan.zhihu.com/p/457484536)|@xlite-dev|â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][CV]ğŸ“–è®°å½•SCRFD(CVPR2021)äººè„¸æ£€æµ‹C++å·¥ç¨‹åŒ–(å«dockeré•œåƒ)](https://zhuanlan.zhihu.com/p/455165568)|@xlite-dev|â­ï¸â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][NCNN]ğŸ“–é‡è·¯å­ï¼šè®°å½•ä¸€ä¸ªè§£å†³onnxè½¬ncnnæ—¶opä¸æ”¯æŒçš„trick](https://zhuanlan.zhihu.com/p/451446147)|@xlite-dev|â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][CV]ğŸ“–å‡çº§ç‰ˆè½»é‡çº§NanoDet-Plus MNN/TNN/NCNN/ORT C++å·¥ç¨‹è®°å½•](https://zhuanlan.zhihu.com/p/450586647)|@xlite-dev|â­ï¸â­ï¸|    
| [[æ¨ç†éƒ¨ç½²][CV]ğŸ“–è¶…è½»é‡çº§NanoDet MNN/TNN/NCNN/ORT C++å·¥ç¨‹è®°å½•](https://zhuanlan.zhihu.com/p/443419387)|@xlite-dev|â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][CV]ğŸ“–è¯¦ç»†è®°å½•MGMattingä¹‹MNNã€TNNå’ŒORT C++ç§»æ¤](https://zhuanlan.zhihu.com/p/442949027)|@xlite-dev|â­ï¸â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][CV]ğŸ“–YOLOX NCNN/MNN/TNN/ONNXRuntime C++å·¥ç¨‹ç®€è®°](https://zhuanlan.zhihu.com/p/447364122)|@xlite-dev|â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][TNN]ğŸ“–æ‰‹åŠ¨ä¿®æ”¹YoloXçš„tnnprotoè®°å½•-TNN](https://zhuanlan.zhihu.com/p/425668734)|@xlite-dev|â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][ORT]ğŸ“–å…¨ç½‘æœ€è¯¦ç»† ONNXRuntime C++/Java/Python èµ„æ–™ï¼](https://zhuanlan.zhihu.com/p/414317269)|@xlite-dev|â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][CV]ğŸ“–RobustVideoMatting: C++å·¥ç¨‹åŒ–è®°å½•-å®ç°ç¯‡](https://zhuanlan.zhihu.com/p/413280488)|@xlite-dev|â­ï¸â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][CV]ğŸ“–RobustVideoMatting: C++å·¥ç¨‹åŒ–è®°å½•-åº”ç”¨ç¯‡](https://zhuanlan.zhihu.com/p/412491918)|@xlite-dev|â­ï¸â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][ORT]ğŸ“–ONNXRuntime C++ CMake å·¥ç¨‹åˆ†æåŠç¼–è¯‘](https://zhuanlan.zhihu.com/p/411887386)|@xlite-dev|â­ï¸â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][ORT]ğŸ“–å¦‚ä½•ä½¿ç”¨ORT C++ APIå¤„ç†NCHWå’ŒNHWCè¾“å…¥ï¼Ÿ](https://zhuanlan.zhihu.com/p/524230808)|@xlite-dev|â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][TNN]ğŸ“–tnn-convertæ­å»ºç®€è®°-YOLOPè½¬TNN](https://zhuanlan.zhihu.com/p/431418709)|@xlite-dev|â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][CV]ğŸ“–YOLOP ONNXRuntime C++å·¥ç¨‹åŒ–è®°å½•](https://zhuanlan.zhihu.com/p/411651933)|@xlite-dev|â­ï¸â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][NCNN]ğŸ“–è¶…æœ‰ç”¨NCNNå‚è€ƒèµ„æ–™æ•´ç†](https://zhuanlan.zhihu.com/p/449765328)|@xlite-dev|â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][MNN]ğŸ“–è¶…æœ‰ç”¨MNNå‚è€ƒèµ„æ–™æ•´ç†](https://zhuanlan.zhihu.com/p/449761992)|@xlite-dev|â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][TNN]ğŸ“–è¶…æœ‰ç”¨TNNå‚è€ƒèµ„æ–™æ•´ç†](https://zhuanlan.zhihu.com/p/449769615)|@xlite-dev|â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][ONNX]ğŸ“–è¶…æœ‰ç”¨ONNXå‚è€ƒèµ„æ–™æ•´ç†](https://zhuanlan.zhihu.com/p/449773663)|@xlite-dev|â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][ONNX]ğŸ“–è¶…æœ‰ç”¨ONNXæ¨¡å‹ç»“æ„å‚è€ƒèµ„æ–™æ•´ç†](https://zhuanlan.zhihu.com/p/449775926)|@xlite-dev|â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][OpenCV-DNN]ğŸ“–è¶…æœ‰ç”¨OpenCV-DNNå‚è€ƒèµ„æ–™æ•´ç†](https://zhuanlan.zhihu.com/p/449778377)|@xlite-dev|â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][Tensorflow]ğŸ“–è¶…æœ‰ç”¨Tensorflow C++å·¥ç¨‹åŒ–çŸ¥è¯†ç‚¹](https://zhuanlan.zhihu.com/p/449788027)|@xlite-dev|â­ï¸|   
| [[æ¨ç†éƒ¨ç½²][æ¨¡å‹è½¬æ¢]ğŸ“–æ·±åº¦å­¦ä¹ æ¨¡å‹è½¬æ¢èµ„æ–™æ•´ç†](https://zhuanlan.zhihu.com/p/449759361)|@xlite-dev|â­ï¸|   
| [[æŠ€æœ¯éšç¬”][C++][CMake]ğŸ“–è¶…æœ‰ç”¨CMakeå‚è€ƒèµ„æ–™æ•´ç†](https://zhuanlan.zhihu.com/p/449779892)|@xlite-dev|â­ï¸â­ï¸|   
| [[æŠ€æœ¯éšç¬”][C++][3Wå­—]ğŸ“–é™æ€é“¾æ¥å’Œé™æ€åº“å®è·µæŒ‡åŒ—-åŸç†ç¯‡](https://zhuanlan.zhihu.com/p/595527528)|@xlite-dev|â­ï¸â­ï¸â­ï¸|   
| [[æŠ€æœ¯éšç¬”][C++]ğŸ“–Macä¸‹C++å†…å­˜æ£€æŸ¥æŒ‡åŒ—(Valgrind VS Asan)](https://zhuanlan.zhihu.com/p/508470880)|@xlite-dev|â­ï¸|   
| [[æŠ€æœ¯éšç¬”][CV]ğŸ“–torchlm: äººè„¸å…³é”®ç‚¹æ£€æµ‹åº“](https://zhuanlan.zhihu.com/p/467211561)|@xlite-dev|â­ï¸â­ï¸|   
| [[æŠ€æœ¯éšç¬”][ML]ğŸ“–ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•-æèˆª: ç¬”è®°-ä»åŸç†åˆ°å®ç°-åŸºäºRã€‹](https://zhuanlan.zhihu.com/p/684885595)|@xlite-dev|â­ï¸â­ï¸|   
| [[æŠ€æœ¯éšç¬”][Git]ğŸ“–å¦‚ä½•ä¼˜é›…åœ°git cloneå’Œgit submoduleï¼Ÿ](https://zhuanlan.zhihu.com/p/639136221)|@xlite-dev|â­ï¸|   
| [[æŠ€æœ¯éšç¬”][3D]ğŸ“–äººè„¸é‡å»º3Då‚è€ƒèµ„æ–™æ•´ç†](https://zhuanlan.zhihu.com/p/524034741)|@xlite-dev|â­ï¸|   
| [[æŠ€æœ¯éšç¬”][3D]ğŸ“–BlendShapeså‚è€ƒèµ„æ–™æ•´ç†](https://zhuanlan.zhihu.com/p/524036145)|@xlite-dev|â­ï¸|   
| [[æŠ€æœ¯éšç¬”][3D]ğŸ“–ä»æºç å®‰è£…Pytorch3Dè¯¦ç»†è®°å½•åŠå­¦ä¹ èµ„æ–™](https://zhuanlan.zhihu.com/p/512347464)|@xlite-dev|â­ï¸|   
| [[æŠ€æœ¯éšç¬”][ML]ğŸ“–200é¡µ:ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ï¼šæèˆªã€‹ç¬”è®° -ä»åŸç†åˆ°å®ç°](https://zhuanlan.zhihu.com/p/461520847)|@xlite-dev|â­ï¸â­ï¸|   


### ğŸ“š CUTLASS|CuTe|NCCL|CUDA|æ–‡ç« æ¨è (å…¶ä»–ä½œè€…) ([Â©ï¸backğŸ‘†ğŸ»](#contents))

<div id="other-blogs"></div>  

ğŸ’¡è¯´æ˜: æœ¬å°èŠ‚æ•´ç†ä¸€äº›è‡ªå·±æ¯”è¾ƒå–œæ¬¢çš„æ–‡ç« ã€‚æ¬¢è¿å¤§å®¶æPRæ¨èæ›´å¤šä¼˜ç§€çš„æ–‡ç« ï¼

|ğŸ“– ç±»å‹-æ ‡é¢˜|ğŸ“– ä½œè€…| ğŸ“– æ¨è |    
|:---|:---|:---|  
| [[cuteç³»åˆ—è¯¦è§£][å…¥é—¨]ğŸ“–cutlass cute 101](https://zhuanlan.zhihu.com/p/660379052)|@æœ±å°éœ–|â­ï¸â­ï¸â­ï¸|
| [[cuteç³»åˆ—è¯¦è§£][å…¥é—¨]ğŸ“–CUTLASS 2.x & CUTLASS 3.x Intro å­¦ä¹ ç¬”è®°](https://zhuanlan.zhihu.com/p/710516489)|@BBuf|â­ï¸â­ï¸â­ï¸|
| [[cuteç³»åˆ—è¯¦è§£][Layout]ğŸ“–cute ä¹‹ Layout](https://zhuanlan.zhihu.com/p/661182311)|@reed|â­ï¸â­ï¸â­ï¸|
| [[cuteç³»åˆ—è¯¦è§£][Layout]ğŸ“–cute Layout çš„ä»£æ•°å’Œå‡ ä½•è§£é‡Š](https://zhuanlan.zhihu.com/p/662089556)|@reed|â­ï¸â­ï¸â­ï¸|
| [[cuteç³»åˆ—è¯¦è§£][Tensor]ğŸ“–cute ä¹‹ Tensor](https://zhuanlan.zhihu.com/p/663093816)|@reed|â­ï¸â­ï¸â­ï¸|
| [[cuteç³»åˆ—è¯¦è§£][MMA]ğŸ“–cute ä¹‹ MMAæŠ½è±¡](https://zhuanlan.zhihu.com/p/663092747)|@reed|â­ï¸â­ï¸â­ï¸|
| [[cuteç³»åˆ—è¯¦è§£][Copy]ğŸ“–cute ä¹‹ CopyæŠ½è±¡](https://zhuanlan.zhihu.com/p/666232173)|@reed|â­ï¸â­ï¸â­ï¸|
| [[cuteç³»åˆ—è¯¦è§£][Swizzle]ğŸ“–cute ä¹‹ Swizzle](https://zhuanlan.zhihu.com/p/671419093)|@reed|â­ï¸â­ï¸â­ï¸|
| [[cuteç³»åˆ—è¯¦è§£][Swizzle]ğŸ“–cute Swizzleç»†è°ˆ](https://zhuanlan.zhihu.com/p/684250988)|@è¿›å‡»çš„Killua|â­ï¸â­ï¸â­ï¸|
| [[cuteç³»åˆ—è¯¦è§£][Swizzle]ğŸ“–cutlass swizzleæœºåˆ¶è§£æï¼ˆä¸€ï¼‰](https://zhuanlan.zhihu.com/p/710337546)|@Titus|â­ï¸â­ï¸â­ï¸|
| [[cuteç³»åˆ—è¯¦è§£][Swizzle]ğŸ“–cutlass swizzleæœºåˆ¶è§£æï¼ˆäºŒï¼‰](https://zhuanlan.zhihu.com/p/711398930)|@Titus|â­ï¸â­ï¸â­ï¸|
| [[cuteç³»åˆ—è¯¦è§£][Swizzle]ğŸ“–CUDAé¿å…smem bank conflictçš„swizzleæœºåˆ¶è§£æ](https://zhuanlan.zhihu.com/p/4746910252)|@frankshi|â­ï¸â­ï¸â­ï¸|
| [[cuteç³»åˆ—è¯¦è§£][GEMM]ğŸ“–cute ä¹‹ ç®€å•GEMMå®ç°](https://zhuanlan.zhihu.com/p/667521327)|@reed|â­ï¸â­ï¸â­ï¸|
| [[cuteç³»åˆ—è¯¦è§£][GEMM]ğŸ“–cute ä¹‹ GEMMæµæ°´çº¿](https://zhuanlan.zhihu.com/p/665082713)|@reed|â­ï¸â­ï¸â­ï¸|
| [[cuteç³»åˆ—è¯¦è§£][GEMM]ğŸ“–cute ä¹‹ é«˜æ•ˆGEMMå®ç°](https://zhuanlan.zhihu.com/p/675308830)|@reed|â­ï¸â­ï¸â­ï¸|
| [[cuteç³»åˆ—è¯¦è§£][GEMM]ğŸ“–GEMMæµæ°´çº¿: single/multi-stageã€pipeline](https://zhuanlan.zhihu.com/p/712451053)|@Titus|â­ï¸â­ï¸â­ï¸|
| [[cuteç³»åˆ—è¯¦è§£][GEMM]ğŸ“–GEMMç»†èŠ‚åˆ†æ(ä¸€): ldmatrixçš„é€‰æ‹©](https://zhuanlan.zhihu.com/p/702818267)|@Anonymous|â­ï¸â­ï¸â­ï¸|
| [[cuteç³»åˆ—è¯¦è§£][GEMM]ğŸ“–GEMMç»†èŠ‚åˆ†æ(äºŒ): TiledCopyä¸cp.async](https://zhuanlan.zhihu.com/p/703560147)|@Anonymous|â­ï¸â­ï¸â­ï¸|
| [[cuteç³»åˆ—è¯¦è§£][GEMM]ğŸ“–GEMMç»†èŠ‚åˆ†æ(ä¸‰): Swizzle<B,M,S>å‚æ•°å–å€¼](https://zhuanlan.zhihu.com/p/713713957)|@Anonymous|â­ï¸â­ï¸â­ï¸|
| [[cuteç³»åˆ—è¯¦è§£][å®è·µ]ğŸ“–Hopper Mixed GEMMçš„CUTLASSå®ç°ç¬”è®°](https://zhuanlan.zhihu.com/p/714378343)|@BBuf|â­ï¸â­ï¸â­ï¸|
| [[cuteç³»åˆ—è¯¦è§£][å®è·µ]ğŸ“–CUTLASS CuTeå®æˆ˜(ä¸€): åŸºç¡€](https://zhuanlan.zhihu.com/p/690703999)|@è¿›å‡»çš„Killua|â­ï¸â­ï¸â­ï¸|
| [[cuteç³»åˆ—è¯¦è§£][å®è·µ]ğŸ“–CUTLASS CuTeå®æˆ˜(äºŒ): åº”ç”¨](https://zhuanlan.zhihu.com/p/692078624)|@è¿›å‡»çš„Killua|â­ï¸â­ï¸â­ï¸|
| [[cuteç³»åˆ—è¯¦è§£][å®è·µ]ğŸ“–FlashAttention fp8å®ç°ï¼ˆadaæ¶æ„)](https://zhuanlan.zhihu.com/p/712314257)|@shengying.wei|â­ï¸â­ï¸â­ï¸|
| [[cuteç³»åˆ—è¯¦è§£][å®è·µ]ğŸ“–FlashAttention ç¬”è®°: tiny-flash-attentionè§£è¯»](https://zhuanlan.zhihu.com/p/708867810)|@shengying.wei|â­ï¸â­ï¸â­ï¸|
| [[cuteç³»åˆ—è¯¦è§£][å®è·µ]ğŸ“–ä½¿ç”¨cutlass cuteå¤ç°flash attention](https://zhuanlan.zhihu.com/p/696323042)|@66RING|â­ï¸â­ï¸â­ï¸|
| [[cutlassæ•™ç¨‹][å…¥é—¨]ğŸ“–cutlass åŸºæœ¬è®¤çŸ¥](https://zhuanlan.zhihu.com/p/677616101)|@JoeNomad|â­ï¸â­ï¸â­ï¸|
| [[cutlassæ•™ç¨‹][å…¥é—¨]ğŸ“–cutlass è½¯ä»¶æ¶æ„](https://zhuanlan.zhihu.com/p/678915618)|@JoeNomad|â­ï¸â­ï¸â­ï¸|
| [[cutlassæ•™ç¨‹][å…¥é—¨]ğŸ“–CUTLASS åŸºç¡€ä»‹ç»](https://zhuanlan.zhihu.com/p/671324125)|@è¿›å‡»çš„Killua|â­ï¸â­ï¸â­ï¸|
| [[cutlassæ•™ç¨‹][å…¥é—¨]ğŸ“–ä¹±è°ˆCUTLASS GTC2020 SLIDES](https://zhuanlan.zhihu.com/p/674693873)|@zzk again|â­ï¸â­ï¸â­ï¸|
| [[cutlassæ•™ç¨‹][æ·±å…¥]ğŸ“–cutlass block swizzle å’Œ tile iterator](https://zhuanlan.zhihu.com/p/679929705)|@JoeNomad|â­ï¸â­ï¸â­ï¸|
| [[cutlassæ•™ç¨‹][æ·±å…¥]ğŸ“–cutlass bank conflict freeçš„smem layout](https://zhuanlan.zhihu.com/p/681966685)|@JoeNomad|â­ï¸â­ï¸â­ï¸|
| [[cutlassæ•™ç¨‹][æ·±å…¥]ğŸ“–cutlass å¤šçº§æµæ°´çº¿](https://zhuanlan.zhihu.com/p/687397095)|@JoeNomad|â­ï¸â­ï¸â­ï¸|
| [[GPUæŒ‡ä»¤é›†æ¶æ„][ç²¾è§£]ğŸ“–NVidia GPUæŒ‡ä»¤é›†æ¶æ„-å‰è¨€](https://zhuanlan.zhihu.com/p/686198447)|@reed|â­ï¸â­ï¸â­ï¸|
| [[GPUæŒ‡ä»¤é›†æ¶æ„][ç²¾è§£]ğŸ“–NVidia GPUæŒ‡ä»¤é›†æ¶æ„-å¯„å­˜å™¨](https://zhuanlan.zhihu.com/p/688616037)|@reed|â­ï¸â­ï¸â­ï¸|
| [[GPUæŒ‡ä»¤é›†æ¶æ„][ç²¾è§£]ğŸ“–NVidia GPUæŒ‡ä»¤é›†æ¶æ„-Loadå’ŒCache](https://zhuanlan.zhihu.com/p/692445145)|@reed|â­ï¸â­ï¸â­ï¸|
| [[GPUæŒ‡ä»¤é›†æ¶æ„][ç²¾è§£]ğŸ“–NVidia GPUæŒ‡ä»¤é›†æ¶æ„-æµ®ç‚¹è¿ç®—](https://zhuanlan.zhihu.com/p/695667044)|@reed|â­ï¸â­ï¸â­ï¸|
| [[GPUæŒ‡ä»¤é›†æ¶æ„][ç²¾è§£]ğŸ“–NVidia GPUæŒ‡ä»¤é›†æ¶æ„-æ•´æ•°è¿ç®—](https://zhuanlan.zhihu.com/p/700921948)|@reed|â­ï¸â­ï¸â­ï¸|
| [[GPUæŒ‡ä»¤é›†æ¶æ„][ç²¾è§£]ğŸ“–NVidia GPUæŒ‡ä»¤é›†æ¶æ„-æ¯”ç‰¹å’Œé€»è¾‘æ“ä½œ](https://zhuanlan.zhihu.com/p/712356884)|@reed|â­ï¸â­ï¸â­ï¸|
| [[GPUæŒ‡ä»¤é›†æ¶æ„][ç²¾è§£]ğŸ“–NVidia GPUæŒ‡ä»¤é›†æ¶æ„-Warpçº§å’ŒUniformæ“ä½œ](https://zhuanlan.zhihu.com/p/712357647)|@reed|â­ï¸â­ï¸â­ï¸|
| [[CUDAä¼˜åŒ–][å…¥é—¨]ğŸ“–CUDAï¼ˆä¸€ï¼‰ï¼šCUDA ç¼–ç¨‹åŸºç¡€](https://zhuanlan.zhihu.com/p/645330027)|@ç´«æ°”ä¸œæ¥|â­ï¸â­ï¸â­ï¸|
| [[CUDAä¼˜åŒ–][å…¥é—¨]ğŸ“–CUDAï¼ˆäºŒï¼‰ï¼šGPUçš„å†…å­˜ä½“ç³»åŠå…¶ä¼˜åŒ–æŒ‡å—](https://zhuanlan.zhihu.com/p/654027980)|@ç´«æ°”ä¸œæ¥|â­ï¸â­ï¸â­ï¸|
| [[CUDAä¼˜åŒ–][å®è·µ]ğŸ“–CUDAï¼ˆä¸‰ï¼‰ï¼šé€šç”¨çŸ©é˜µä¹˜æ³•ï¼šä»å…¥é—¨åˆ°ç†Ÿç»ƒ](https://zhuanlan.zhihu.com/p/657632577)|@ç´«æ°”ä¸œæ¥|â­ï¸â­ï¸â­ï¸|
| [[CUDAä¼˜åŒ–][å®è·µ]ğŸ“–ops(1)ï¼šLayerNorm ç®—å­çš„ CUDA å®ç°ä¸ä¼˜åŒ–](https://zhuanlan.zhihu.com/p/694974164)|@ç´«æ°”ä¸œæ¥|â­ï¸â­ï¸â­ï¸|
| [[CUDAä¼˜åŒ–][å®è·µ]ğŸ“–ops(2)ï¼šSoftMaxç®—å­çš„ CUDA å®ç°](https://zhuanlan.zhihu.com/p/695307283)|@ç´«æ°”ä¸œæ¥|â­ï¸â­ï¸â­ï¸|
| [[CUDAä¼˜åŒ–][å®è·µ]ğŸ“–ops(3)ï¼šCross Entropy çš„ CUDA å®ç°](https://zhuanlan.zhihu.com/p/695594396)|@ç´«æ°”ä¸œæ¥|â­ï¸â­ï¸â­ï¸|
| [[CUDAä¼˜åŒ–][å®è·µ]ğŸ“–ops(4)ï¼šAdamW ä¼˜åŒ–å™¨çš„ CUDA å®ç°](https://zhuanlan.zhihu.com/p/695611950)|@ç´«æ°”ä¸œæ¥|â­ï¸â­ï¸â­ï¸|
| [[CUDAä¼˜åŒ–][å®è·µ]ğŸ“–ops(5)ï¼šæ¿€æ´»å‡½æ•°ä¸æ®‹å·®è¿æ¥çš„ CUDA å®ç°](https://zhuanlan.zhihu.com/p/695703671)|@ç´«æ°”ä¸œæ¥|â­ï¸â­ï¸â­ï¸|
| [[CUDAä¼˜åŒ–][å®è·µ]ğŸ“–ops(6)ï¼šembedding å±‚ä¸ LM head å±‚çš„ CUDA å®ç°](https://zhuanlan.zhihu.com/p/695785781)|@ç´«æ°”ä¸œæ¥|â­ï¸â­ï¸â­ï¸|
| [[CUDAä¼˜åŒ–][å®è·µ]ğŸ“–ops(7)ï¼šself-attention çš„ CUDA å®ç°åŠä¼˜åŒ– (ä¸Š)](https://zhuanlan.zhihu.com/p/695898274)|@ç´«æ°”ä¸œæ¥|â­ï¸â­ï¸â­ï¸|
| [[CUDAä¼˜åŒ–][å®è·µ]ğŸ“–ops(8)ï¼šself-attention çš„ CUDA å®ç°åŠä¼˜åŒ– (ä¸‹)](https://zhuanlan.zhihu.com/p/696197013)|@ç´«æ°”ä¸œæ¥|â­ï¸â­ï¸â­ï¸|
| [[CUDAä¼˜åŒ–][å®è·µ]ğŸ“–CUDAï¼ˆå››ï¼‰ï¼šä½¿ç”¨ CUDA å®ç° Transformer ç»“æ„](https://zhuanlan.zhihu.com/p/694416583)|@ç´«æ°”ä¸œæ¥|â­ï¸â­ï¸â­ï¸|
| [[CUDAä¼˜åŒ–][Copy]ğŸ“–Async CopyåŠMemory BarrieræŒ‡ä»¤çš„åŠŸèƒ½ä¸å®ç°](https://zhuanlan.zhihu.com/p/685168850)|@Frank Wang|â­ï¸â­ï¸â­ï¸|
| [[CUDAä¼˜åŒ–][GEMV]ğŸ“–æ·±å…¥æµ…å‡ºGPUä¼˜åŒ–ç³»åˆ—ï¼šgemvä¼˜åŒ–](https://zhuanlan.zhihu.com/p/494144694)|@æœ‰äº†ç¦ç¦çš„æ£å­|â­ï¸â­ï¸â­ï¸|
| [[Tensor Cores]ğŸ“–Nvidia Tensor Coreåˆæ¢](https://zhuanlan.zhihu.com/p/620185229)|@æœ¨å­çŸ¥|â­ï¸â­ï¸â­ï¸|
| [[Tensor Cores]ğŸ“–Nvidia Tensor Core-WMMA APIç¼–ç¨‹å…¥é—¨](https://zhuanlan.zhihu.com/p/620766588)|@æœ¨å­çŸ¥|â­ï¸â­ï¸â­ï¸|
| [[Tensor Cores]ğŸ“–Nvidia Tensor Core-MMA PTXç¼–ç¨‹å…¥é—¨](https://zhuanlan.zhihu.com/p/621855199)|@æœ¨å­çŸ¥|â­ï¸â­ï¸â­ï¸|
| [[Tensor Cores]ğŸ“–CUDA Ampere Tensor Core HGEMM çŸ©é˜µä¹˜æ³•ä¼˜åŒ–](https://zhuanlan.zhihu.com/p/555339335)|@nicholaswilde|â­ï¸â­ï¸â­ï¸|
| [[GPUé€šä¿¡æ¶æ„][ç²¾è§£]ğŸ“–NVIDIA GPGPUï¼ˆå››ï¼‰- é€šä¿¡æ¶æ„](https://zhuanlan.zhihu.com/p/680262016)|@Bruce|â­ï¸â­ï¸â­ï¸|


## Â©ï¸License ([Â©ï¸backğŸ‘†ğŸ»](#contents))

<div id="License"></div>  

GNU General Public License v3.0

## ğŸ‰Contribute ([Â©ï¸backğŸ‘†ğŸ»](#contents))

<div id="contribute"></div>  

How to contribute? Star this repo or check [ğŸŒ¤ğŸŒ¤CONTRIBUTEğŸ‰ğŸ‰](https://github.com/xlite-dev/CUDA-Learn-Notes/issues/50). 

<div align='center'>
<a href="https://star-history.com/#xlite-dev/CUDA-Learn-Notes&Date">
 <picture>
   <source media="(prefers-color-scheme: dark)" srcset="https://api.star-history.com/svg?repos=xlite-dev/CUDA-Learn-Notes&type=Date&theme=dark" />
   <source media="(prefers-color-scheme: light)" srcset="https://api.star-history.com/svg?repos=xlite-dev/CUDA-Learn-Notes&type=Date" />
   <img width=400 height=300 alt="Star History Chart" src="https://api.star-history.com/svg?repos=xlite-dev/CUDA-Learn-Notes&type=Date" />
 </picture>
</a>
</div>

## ğŸ“– References ([Â©ï¸backğŸ‘†ğŸ»](#contents)) 
<div id="ref"></div>  

- [flash-attention-minimal](https://github.com/tspeterkim/flash-attention-minimal)
- [tiny-flash-attention](https://github.com/66RING/tiny-flash-attention)
- [cute-gemm](https://github.com/reed-lau/cute-gemm)
- [cutlass_flash_atten_fp8](https://github.com/weishengying/cutlass_flash_atten_fp8)
- [cuda_learning](https://github.com/ifromeast/cuda_learning)
- [cuda_hgemm](https://github.com/Bruce-Lee-LY/cuda_hgemm)
- [cuda-tensorcore-hgemm](https://github.com/nicolaswilde/cuda-tensorcore-hgemm)
- [How_to_optimize_in_GPU](https://github.com/Liu-xiandong/How_to_optimize_in_GPU/tree/master/sgemv)
- [how-to-optim-algorithm-in-cuda](https://github.com/BBuf/how-to-optim-algorithm-in-cuda) 
- [cute_gemm](https://github.com/weishengying/cute_gemm)
- [cutlass](https://github.com/NVIDIA/cutlass)
-->
